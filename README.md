# Generated Tuning Hints with Large Language Modles by Exploiting Natural Language Processing
Two machine learning models were developed in an effort to reduce the cognitive load on Database Administrators when fine-tuning a PostgreSQL database system. The first model answers simple questions by utilizing embeddings on a custom-tailored knowledge-base consisting of user manuals, YouTube transcripts, and Reddit/Quora posts. The second model uses a Generative Pre-trained Transformer 2 (GPT-2) model with 124M parameters to answer more complex queries that require understanding of semantic relationships and dependencies between parameter non-independence. The training data to fine-tune the model was generated by ChatGPT-4 and the author by leveraging the first models input-output pairs.
## Technical Details
1. Model #1:
   - **HayStacks** was used to configure the question-answer pipeline that leveraged sentence-similarity tasks and the data preparation/pre-processing from a user-defined knowledge base.
2. Model #2:
   - **SFTTrainer** was used to fine-tune the GPT2 model on relevant Question=Answer pairs.
3. Web Scraping:
   - **PlayWright, BeautifulSoup4** were used to scrape tuning hints from Reddit/Quora and save them into the user-defined knowledge base as text files.
   - **youtube-transcript-api** was used to obtain text files that contained transcripts from conferences that discussed tuning-related hints.
4. External Tools:
   - **ChatGPT4** was used to help generate training data to finetune the LLM as a proof-of-concept. 
## Dependencies
- Due to copyright concerns, I will not be uploading my knowledge base.
- Paper / Proposal is available upon request.
