{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "7tILRng55dIF",
        "R3NJTic88eJ0",
        "ZItOn08X8lg6",
        "T5GxrmRo89Nl",
        "DUhLhCsBl8e8",
        "x_qZENw4mBfr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "260f645505644fbc9c751721b12a9bc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bd7fb640a5334379abb1655a71cc8f79",
              "IPY_MODEL_06a2b5b4cd5049f3988dc0c3bdea0f93",
              "IPY_MODEL_50e4c5e5f13140889e79c70a0862fded"
            ],
            "layout": "IPY_MODEL_afcfbc6f9961446cb2a131addb021dcf"
          }
        },
        "bd7fb640a5334379abb1655a71cc8f79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b7d502358fe046a0873ef7959b36f758",
            "placeholder": "​",
            "style": "IPY_MODEL_06e648be14d9467196eac4a1f176169e",
            "value": "(…)851d5dd1af673670cdb299753/.gitattributes: 100%"
          }
        },
        "06a2b5b4cd5049f3988dc0c3bdea0f93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8c5a1525a3841e7ad0e63d3d12f5445",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_090c521e6dc042819438be61869a6fb2",
            "value": 1175
          }
        },
        "50e4c5e5f13140889e79c70a0862fded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa18fd18c912466c8caf2a79476dc8d2",
            "placeholder": "​",
            "style": "IPY_MODEL_6670a6903648436b92b32edc9c861b9d",
            "value": " 1.18k/1.18k [00:00&lt;00:00, 64.1kB/s]"
          }
        },
        "afcfbc6f9961446cb2a131addb021dcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7d502358fe046a0873ef7959b36f758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06e648be14d9467196eac4a1f176169e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8c5a1525a3841e7ad0e63d3d12f5445": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "090c521e6dc042819438be61869a6fb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa18fd18c912466c8caf2a79476dc8d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6670a6903648436b92b32edc9c861b9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6804fcf58abc4d22a23c105f4c2a5e0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_870a39c21da543299b41fa05f102ef21",
              "IPY_MODEL_29275baa096743debb908de5895fd860",
              "IPY_MODEL_5a8d1eec033644a9aa7a1a19c7d0548f"
            ],
            "layout": "IPY_MODEL_8453d5ffdef94342b08104b8c862e76d"
          }
        },
        "870a39c21da543299b41fa05f102ef21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be5e2ffff9854aedbeea7b3bec7ca2eb",
            "placeholder": "​",
            "style": "IPY_MODEL_fa07d459c75a47a9b4d056aadcf334d9",
            "value": "(…)1af673670cdb299753/1_Pooling/config.json: 100%"
          }
        },
        "29275baa096743debb908de5895fd860": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f642282974d47b88742b93df76a4914",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6d419c396a44e6fb709713a81084f63",
            "value": 190
          }
        },
        "5a8d1eec033644a9aa7a1a19c7d0548f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6290ff5c4c444ec8c3475518ecc1d64",
            "placeholder": "​",
            "style": "IPY_MODEL_5bec9d6fb07c481f81cbd97cdaeb276a",
            "value": " 190/190 [00:00&lt;00:00, 12.3kB/s]"
          }
        },
        "8453d5ffdef94342b08104b8c862e76d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be5e2ffff9854aedbeea7b3bec7ca2eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa07d459c75a47a9b4d056aadcf334d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f642282974d47b88742b93df76a4914": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6d419c396a44e6fb709713a81084f63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6290ff5c4c444ec8c3475518ecc1d64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bec9d6fb07c481f81cbd97cdaeb276a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3552290c87cb4908bb2af2ba6e1ecf80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cf0fa1c2af81429c981417bd17c9c1ca",
              "IPY_MODEL_b0312f0aeace4f47be9d9302e469e1d9",
              "IPY_MODEL_03c397dd509646e590ae8437134181e6"
            ],
            "layout": "IPY_MODEL_011abc2499c140a3a29a5e5db9c2b64a"
          }
        },
        "cf0fa1c2af81429c981417bd17c9c1ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70bd86abee1344c89ce89437e16e4fe5",
            "placeholder": "​",
            "style": "IPY_MODEL_07b9568c2a7747ff96188cd1e82be8e6",
            "value": "(…)6e48e851d5dd1af673670cdb299753/README.md: 100%"
          }
        },
        "b0312f0aeace4f47be9d9302e469e1d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6292e7aba1d843cdb56f154057a31b44",
            "max": 10571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14d587e0e20c4099b18f1882b1a3f16f",
            "value": 10571
          }
        },
        "03c397dd509646e590ae8437134181e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ee147bce8854edd81b1726f6968eaa5",
            "placeholder": "​",
            "style": "IPY_MODEL_0c340865bf1d4478addb083fff638887",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 785kB/s]"
          }
        },
        "011abc2499c140a3a29a5e5db9c2b64a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70bd86abee1344c89ce89437e16e4fe5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07b9568c2a7747ff96188cd1e82be8e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6292e7aba1d843cdb56f154057a31b44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14d587e0e20c4099b18f1882b1a3f16f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ee147bce8854edd81b1726f6968eaa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c340865bf1d4478addb083fff638887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad5401b795ff4d3395c98d9f9672cd23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_116affaf0e5143f4894c0500518101e5",
              "IPY_MODEL_bba4addf6a18495999fcee1232086fc4",
              "IPY_MODEL_59906401f3bc4d28a0373a0928024235"
            ],
            "layout": "IPY_MODEL_b7eac8045a74424d95090db5fa54c605"
          }
        },
        "116affaf0e5143f4894c0500518101e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdf8f7305c0f4f4090a420f6515c34b4",
            "placeholder": "​",
            "style": "IPY_MODEL_2c81471aa2ff4cf0bbf88dc758fa0df3",
            "value": "(…)48e851d5dd1af673670cdb299753/config.json: 100%"
          }
        },
        "bba4addf6a18495999fcee1232086fc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e60ef57d40a4a2398399679b81b07a8",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a3a3c04e19654eb6980e9a14c612d3b7",
            "value": 571
          }
        },
        "59906401f3bc4d28a0373a0928024235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ff3278236b4491dbd86b38250c57c55",
            "placeholder": "​",
            "style": "IPY_MODEL_db629ec83dc74fdbbf5fc852354f646e",
            "value": " 571/571 [00:00&lt;00:00, 45.0kB/s]"
          }
        },
        "b7eac8045a74424d95090db5fa54c605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdf8f7305c0f4f4090a420f6515c34b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c81471aa2ff4cf0bbf88dc758fa0df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e60ef57d40a4a2398399679b81b07a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3a3c04e19654eb6980e9a14c612d3b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ff3278236b4491dbd86b38250c57c55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db629ec83dc74fdbbf5fc852354f646e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e50eecf00e1b4d49af0eb50f0a5049e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_238aec16c8c64f1496d6fb3935439c5e",
              "IPY_MODEL_b128985c188649d394c48b60e5343ed5",
              "IPY_MODEL_ff878f0306c2412a8bd4aacc35f0df4d"
            ],
            "layout": "IPY_MODEL_89fd92303e264c26884981bf5ab71906"
          }
        },
        "238aec16c8c64f1496d6fb3935439c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adc536dda7ca4a689422ae7d512b7e4c",
            "placeholder": "​",
            "style": "IPY_MODEL_065352fcf5d248fab6424590e24bccd1",
            "value": "(…)299753/config_sentence_transformers.json: 100%"
          }
        },
        "b128985c188649d394c48b60e5343ed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8f808c611904be19d3911da3f3713b2",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3de8a201b18f43db9f6088d97e23eabd",
            "value": 116
          }
        },
        "ff878f0306c2412a8bd4aacc35f0df4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_035ac00c9e274b57bc3a51f1b16481ed",
            "placeholder": "​",
            "style": "IPY_MODEL_61db9d74814c47b191708b304c4f1395",
            "value": " 116/116 [00:00&lt;00:00, 8.68kB/s]"
          }
        },
        "89fd92303e264c26884981bf5ab71906": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adc536dda7ca4a689422ae7d512b7e4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "065352fcf5d248fab6424590e24bccd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8f808c611904be19d3911da3f3713b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3de8a201b18f43db9f6088d97e23eabd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "035ac00c9e274b57bc3a51f1b16481ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61db9d74814c47b191708b304c4f1395": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ede5265596034cb6bd7b60596217038f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6f0379ab9aab4b3cad09895e47378c84",
              "IPY_MODEL_b5b614f2ba5d4fa4af5d628164fb5c73",
              "IPY_MODEL_65b56cf729224282a3039dec44704f56"
            ],
            "layout": "IPY_MODEL_298ed699495c426d8e292d563d4141ff"
          }
        },
        "6f0379ab9aab4b3cad09895e47378c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a5f3755f7fd4c5caf301d959632de30",
            "placeholder": "​",
            "style": "IPY_MODEL_4100cd7199c543d694aa53a7dd5609ac",
            "value": "(…)1d5dd1af673670cdb299753/data_config.json: 100%"
          }
        },
        "b5b614f2ba5d4fa4af5d628164fb5c73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4820a0c2ea74ec3ac898b8320b5270b",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b8c0c2814b24206b615f8243019c044",
            "value": 39265
          }
        },
        "65b56cf729224282a3039dec44704f56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_601ed2200a1949488a9832b36867596b",
            "placeholder": "​",
            "style": "IPY_MODEL_3b8f23e163e24263a2e79521741d09e3",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 2.74MB/s]"
          }
        },
        "298ed699495c426d8e292d563d4141ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a5f3755f7fd4c5caf301d959632de30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4100cd7199c543d694aa53a7dd5609ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4820a0c2ea74ec3ac898b8320b5270b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b8c0c2814b24206b615f8243019c044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "601ed2200a1949488a9832b36867596b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b8f23e163e24263a2e79521741d09e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e464378ab31a425c9e66e04bbcb98e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b19c05d221ac423d8509d03c795b1cb6",
              "IPY_MODEL_af4af82bb16f4d5595a2ce558b542425",
              "IPY_MODEL_524d416a2dc84dc1b0cd6bc865a5a723"
            ],
            "layout": "IPY_MODEL_a1e582cba8a045a4bd4d962dd2d001bd"
          }
        },
        "b19c05d221ac423d8509d03c795b1cb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67f031087073444f8770beea58b59d3c",
            "placeholder": "​",
            "style": "IPY_MODEL_a90a4344944d4a6a87b0e99442a7f33a",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "af4af82bb16f4d5595a2ce558b542425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9442994ba2d74821bb0660c9c37b96cf",
            "max": 438011953,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e06f7c94cc134db2a1eff09515caf0e6",
            "value": 438011953
          }
        },
        "524d416a2dc84dc1b0cd6bc865a5a723": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc3f6698e72449c0bcf3c0b8c0d72f44",
            "placeholder": "​",
            "style": "IPY_MODEL_a90a091545a54cbbbd5934f89ff7c005",
            "value": " 438M/438M [00:03&lt;00:00, 117MB/s]"
          }
        },
        "a1e582cba8a045a4bd4d962dd2d001bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67f031087073444f8770beea58b59d3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a90a4344944d4a6a87b0e99442a7f33a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9442994ba2d74821bb0660c9c37b96cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e06f7c94cc134db2a1eff09515caf0e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc3f6698e72449c0bcf3c0b8c0d72f44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a90a091545a54cbbbd5934f89ff7c005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d9b31d59d2e4ef9878e985d25db9e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d6095b64f3b740aba22cf02030eb2f5f",
              "IPY_MODEL_84e9e8cfe4da480dba40a3ffb05eca18",
              "IPY_MODEL_7d3bac7487874f7988d32035896fc48e"
            ],
            "layout": "IPY_MODEL_e84e73db3ee646628ce1af0c2ab69bb6"
          }
        },
        "d6095b64f3b740aba22cf02030eb2f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc6b0fbb3bab4663b7e2bf8944b2e5ed",
            "placeholder": "​",
            "style": "IPY_MODEL_0dd6fba41d7640bcae6e6fefd0c35a19",
            "value": "(…)73670cdb299753/sentence_bert_config.json: 100%"
          }
        },
        "84e9e8cfe4da480dba40a3ffb05eca18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a5b6f1bd1674097b0fb7c87404557f3",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17da1bd25ad5494c85bae1ebef5ade07",
            "value": 53
          }
        },
        "7d3bac7487874f7988d32035896fc48e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_825a217b08ed4f96942f49d3d32fc7b2",
            "placeholder": "​",
            "style": "IPY_MODEL_286d12903afe4f96b5c119d41ea828e4",
            "value": " 53.0/53.0 [00:00&lt;00:00, 1.65kB/s]"
          }
        },
        "e84e73db3ee646628ce1af0c2ab69bb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc6b0fbb3bab4663b7e2bf8944b2e5ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dd6fba41d7640bcae6e6fefd0c35a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a5b6f1bd1674097b0fb7c87404557f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17da1bd25ad5494c85bae1ebef5ade07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "825a217b08ed4f96942f49d3d32fc7b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "286d12903afe4f96b5c119d41ea828e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21fd10c86efd468294d568404b483400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3c11db6a9cc04bc0a5ad069b70213619",
              "IPY_MODEL_c0f57ccfd3a346e3828deb331868f127",
              "IPY_MODEL_0c443e9409ae42dc973cc183b758a3f8"
            ],
            "layout": "IPY_MODEL_f0d5343347cb4d7f98ac1719248d2759"
          }
        },
        "3c11db6a9cc04bc0a5ad069b70213619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67e7d82cbdc44b0a8dafd0607479d9fb",
            "placeholder": "​",
            "style": "IPY_MODEL_ac6a2ca60a9e484b8de2cda90c6ae86d",
            "value": "(…)f673670cdb299753/special_tokens_map.json: 100%"
          }
        },
        "c0f57ccfd3a346e3828deb331868f127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02a95c866a94484188a4f9a65383a633",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_134cff7fa79044e082609eafcb3fceb1",
            "value": 239
          }
        },
        "0c443e9409ae42dc973cc183b758a3f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_960ca7cd50544a429741d8687600cd2a",
            "placeholder": "​",
            "style": "IPY_MODEL_e82f532a253a4cc9af278efb1f0aafcc",
            "value": " 239/239 [00:00&lt;00:00, 8.28kB/s]"
          }
        },
        "f0d5343347cb4d7f98ac1719248d2759": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67e7d82cbdc44b0a8dafd0607479d9fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac6a2ca60a9e484b8de2cda90c6ae86d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02a95c866a94484188a4f9a65383a633": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "134cff7fa79044e082609eafcb3fceb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "960ca7cd50544a429741d8687600cd2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e82f532a253a4cc9af278efb1f0aafcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36233e0e77804f628c69c22ee310de0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e4046e4eba84f55be08986c3a2bc64b",
              "IPY_MODEL_a104a130fccf4a35bb1a3f4190fe4638",
              "IPY_MODEL_8dd0b44f605040e3aebe56fd4133bfd7"
            ],
            "layout": "IPY_MODEL_f0667d6dec904ec0bc2397802788f472"
          }
        },
        "1e4046e4eba84f55be08986c3a2bc64b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8289e193d31449cad79a3cb25990dd6",
            "placeholder": "​",
            "style": "IPY_MODEL_0caf7890b3fd498c8279f38ac0544704",
            "value": "(…)851d5dd1af673670cdb299753/tokenizer.json: 100%"
          }
        },
        "a104a130fccf4a35bb1a3f4190fe4638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08ad12e98c314f038468551403d33c4b",
            "max": 466021,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c16c375d720844c8afe37b9692cc19db",
            "value": 466021
          }
        },
        "8dd0b44f605040e3aebe56fd4133bfd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84722bbcdd6e44899551f6e89757849e",
            "placeholder": "​",
            "style": "IPY_MODEL_cd363f0794774403b1d1c20c2b603d9f",
            "value": " 466k/466k [00:00&lt;00:00, 7.91MB/s]"
          }
        },
        "f0667d6dec904ec0bc2397802788f472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8289e193d31449cad79a3cb25990dd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0caf7890b3fd498c8279f38ac0544704": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08ad12e98c314f038468551403d33c4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c16c375d720844c8afe37b9692cc19db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84722bbcdd6e44899551f6e89757849e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd363f0794774403b1d1c20c2b603d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5411e838b20148a4bddca41c1b198db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2723713fcc0c469dac5874c1f83f1dd4",
              "IPY_MODEL_d3b01260f5894fcfaa16df99ec3a2ef7",
              "IPY_MODEL_6ec2c81d45cb40de945677049b3b7c0e"
            ],
            "layout": "IPY_MODEL_361fe6fc8ef44c6b8affa859f1868647"
          }
        },
        "2723713fcc0c469dac5874c1f83f1dd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f62ed308b3d9494f8084fb8442ca39be",
            "placeholder": "​",
            "style": "IPY_MODEL_73627ff745024773b8d089e979d4c3c3",
            "value": "(…)1af673670cdb299753/tokenizer_config.json: 100%"
          }
        },
        "d3b01260f5894fcfaa16df99ec3a2ef7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d610a85fd64f4e95a14cf6f0a4d49c36",
            "max": 363,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c0d3b9347d9440a969f75fb62eb2207",
            "value": 363
          }
        },
        "6ec2c81d45cb40de945677049b3b7c0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_deb76ba622894c8cac9e76012f554882",
            "placeholder": "​",
            "style": "IPY_MODEL_b4770f2a40bc4483b757c7f151eab911",
            "value": " 363/363 [00:00&lt;00:00, 20.4kB/s]"
          }
        },
        "361fe6fc8ef44c6b8affa859f1868647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f62ed308b3d9494f8084fb8442ca39be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73627ff745024773b8d089e979d4c3c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d610a85fd64f4e95a14cf6f0a4d49c36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c0d3b9347d9440a969f75fb62eb2207": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "deb76ba622894c8cac9e76012f554882": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4770f2a40bc4483b757c7f151eab911": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f181d001e8444e879ce5ac0eb70b6515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b3f13981ee154bd7a70df46cda1a4b50",
              "IPY_MODEL_1f71c21069af40b68023e2219e65ecb1",
              "IPY_MODEL_64eb2c1edfbc40b9a7d359df8b772734"
            ],
            "layout": "IPY_MODEL_b3c2b727b67b463fa26b13bbbe747e21"
          }
        },
        "b3f13981ee154bd7a70df46cda1a4b50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb350f3c817942748d8b7c048fa9c3cd",
            "placeholder": "​",
            "style": "IPY_MODEL_20f85bad69ed4e8686f449aef942b9f6",
            "value": "(…)51d5dd1af673670cdb299753/train_script.py: 100%"
          }
        },
        "1f71c21069af40b68023e2219e65ecb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7d0c7eaf1ea4786bffc652752cc8ef3",
            "max": 13123,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62f34ef5fbf74130a44467aa4e737b4b",
            "value": 13123
          }
        },
        "64eb2c1edfbc40b9a7d359df8b772734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3081c21f73d42b1b7a872c1e60ed1e8",
            "placeholder": "​",
            "style": "IPY_MODEL_a5eb031864524e7a9e459f727cfe2d3e",
            "value": " 13.1k/13.1k [00:00&lt;00:00, 491kB/s]"
          }
        },
        "b3c2b727b67b463fa26b13bbbe747e21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb350f3c817942748d8b7c048fa9c3cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20f85bad69ed4e8686f449aef942b9f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7d0c7eaf1ea4786bffc652752cc8ef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62f34ef5fbf74130a44467aa4e737b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3081c21f73d42b1b7a872c1e60ed1e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5eb031864524e7a9e459f727cfe2d3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd618a5473a74492b5dfe6384f9acf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_845e95a975324652be76fa81ee55d2cd",
              "IPY_MODEL_25f9565d0fd34761a2480f47c1c3be91",
              "IPY_MODEL_31e6d5e81ee24c9280d3a487f4cdab92"
            ],
            "layout": "IPY_MODEL_0f0fd71d54b14227b9b9c3937451616a"
          }
        },
        "845e95a975324652be76fa81ee55d2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e4ac32c6794433a96ae5c1f32edfc73",
            "placeholder": "​",
            "style": "IPY_MODEL_a101b182623a41f882e3ca85cf60ebf7",
            "value": "(…)6e48e851d5dd1af673670cdb299753/vocab.txt: 100%"
          }
        },
        "25f9565d0fd34761a2480f47c1c3be91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28aabb89466b4bf5868e038720f4af65",
            "max": 231536,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_341715aa866e4c51a05bb6d1e3dd3014",
            "value": 231536
          }
        },
        "31e6d5e81ee24c9280d3a487f4cdab92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0441fa99aef24e6980566f135b749fb9",
            "placeholder": "​",
            "style": "IPY_MODEL_d40307e7104a43fc9e07cae107c4dd0f",
            "value": " 232k/232k [00:00&lt;00:00, 11.9MB/s]"
          }
        },
        "0f0fd71d54b14227b9b9c3937451616a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e4ac32c6794433a96ae5c1f32edfc73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a101b182623a41f882e3ca85cf60ebf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28aabb89466b4bf5868e038720f4af65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "341715aa866e4c51a05bb6d1e3dd3014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0441fa99aef24e6980566f135b749fb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d40307e7104a43fc9e07cae107c4dd0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b747c755efe4e61b2b339675e73e142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5afef5985139445cadbb24654a40c3e1",
              "IPY_MODEL_4b66ebaf49cf489fa83c1eaeee38e8b9",
              "IPY_MODEL_8626054759934e12a4bfeef64f2fdd95"
            ],
            "layout": "IPY_MODEL_514975224c044cb7b001ca3f1d3b13f0"
          }
        },
        "5afef5985139445cadbb24654a40c3e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66a46818d18f468c9a3f47498a998812",
            "placeholder": "​",
            "style": "IPY_MODEL_3281de727bb842cba94b60d7cee20dc6",
            "value": "(…)8e851d5dd1af673670cdb299753/modules.json: 100%"
          }
        },
        "4b66ebaf49cf489fa83c1eaeee38e8b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce055c7cde384fd290b03e41783e3ec6",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bb5b63186e5e42769abb64a9f6a5ae16",
            "value": 349
          }
        },
        "8626054759934e12a4bfeef64f2fdd95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41922f21a79e4b9c811ab856c5e186aa",
            "placeholder": "​",
            "style": "IPY_MODEL_c309c973321547b19e546d644ea81e62",
            "value": " 349/349 [00:00&lt;00:00, 17.6kB/s]"
          }
        },
        "514975224c044cb7b001ca3f1d3b13f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66a46818d18f468c9a3f47498a998812": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3281de727bb842cba94b60d7cee20dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce055c7cde384fd290b03e41783e3ec6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb5b63186e5e42769abb64a9f6a5ae16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41922f21a79e4b9c811ab856c5e186aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c309c973321547b19e546d644ea81e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35ae1702551049029a73b9394ac11a67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49b88ae647024f43845db70d4a2c99a2",
              "IPY_MODEL_64d0254f4aaf4af592fe1fec817c2dd6",
              "IPY_MODEL_8e110c68be9243b6aec796ee7175586e"
            ],
            "layout": "IPY_MODEL_51771cc22059425aaba75dfa1c3909f4"
          }
        },
        "49b88ae647024f43845db70d4a2c99a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97ba3b7f02704c3ea9d42f07966540e7",
            "placeholder": "​",
            "style": "IPY_MODEL_6e2f936ab50c42ba94d68646ba129c2a",
            "value": "Batches: 100%"
          }
        },
        "64d0254f4aaf4af592fe1fec817c2dd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7261578bed914a6db061fc081aafa2fe",
            "max": 313,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd6b8f8c597d4ad1898d6f845d18e17e",
            "value": 313
          }
        },
        "8e110c68be9243b6aec796ee7175586e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6ef984925b543118a0c3071df10546e",
            "placeholder": "​",
            "style": "IPY_MODEL_a20b9653146244e69587f5f31348e512",
            "value": " 313/313 [02:14&lt;00:00,  5.40it/s]"
          }
        },
        "51771cc22059425aaba75dfa1c3909f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97ba3b7f02704c3ea9d42f07966540e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e2f936ab50c42ba94d68646ba129c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7261578bed914a6db061fc081aafa2fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd6b8f8c597d4ad1898d6f845d18e17e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6ef984925b543118a0c3071df10546e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a20b9653146244e69587f5f31348e512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33c14c018f4a437b979245249435223b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42cea9bb677b4961abb325a021f2407b",
              "IPY_MODEL_9737a9fba40d45bb8738e2f95976042f",
              "IPY_MODEL_ab10c7f148ba40c5a0c7c1e0d7f21ae4"
            ],
            "layout": "IPY_MODEL_3089b97ee29c49d3b838e98a1582530a"
          }
        },
        "42cea9bb677b4961abb325a021f2407b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97e8901d80bd4e6e84b330237d42bbd7",
            "placeholder": "​",
            "style": "IPY_MODEL_ed50d6767e654dee9d1149037a9c9e4a",
            "value": "Batches: 100%"
          }
        },
        "9737a9fba40d45bb8738e2f95976042f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a11ee7fa11c468bb1ae73fadcee8ffe",
            "max": 160,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_33680885e0ff44afa0f00fb7158ded6f",
            "value": 160
          }
        },
        "ab10c7f148ba40c5a0c7c1e0d7f21ae4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d0ec7fd95e74167bdc1ec0bf9af1753",
            "placeholder": "​",
            "style": "IPY_MODEL_78cf344c71cc424faff0cd92235de980",
            "value": " 160/160 [01:06&lt;00:00,  5.90it/s]"
          }
        },
        "3089b97ee29c49d3b838e98a1582530a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97e8901d80bd4e6e84b330237d42bbd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed50d6767e654dee9d1149037a9c9e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a11ee7fa11c468bb1ae73fadcee8ffe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33680885e0ff44afa0f00fb7158ded6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d0ec7fd95e74167bdc1ec0bf9af1753": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78cf344c71cc424faff0cd92235de980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e6d8c2c296c4d0c9d262528462ecb19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_848c1bd8c6cc45b496132d39a830bc0b",
              "IPY_MODEL_74edb306327c46dea68caca5c9b67205",
              "IPY_MODEL_388a00834f564969b0488696c7a5de85"
            ],
            "layout": "IPY_MODEL_3bcb18347da1475f9360ffc9c89949c1"
          }
        },
        "848c1bd8c6cc45b496132d39a830bc0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ca34969b09045ae961a1e765d9e10a7",
            "placeholder": "​",
            "style": "IPY_MODEL_efad89b6ab824feaaee563e34e48cd85",
            "value": "(…)rta-base-squad2/resolve/main/config.json: 100%"
          }
        },
        "74edb306327c46dea68caca5c9b67205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74ec0b0ae60c4d4c830705f2c9d308d7",
            "max": 571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad53ba2c3a374b8ab1af61e5317a66aa",
            "value": 571
          }
        },
        "388a00834f564969b0488696c7a5de85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d66eca3b37e4614830e42a68800f780",
            "placeholder": "​",
            "style": "IPY_MODEL_81e811bfdfc941f19a1108a525e1b689",
            "value": " 571/571 [00:00&lt;00:00, 30.5kB/s]"
          }
        },
        "3bcb18347da1475f9360ffc9c89949c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ca34969b09045ae961a1e765d9e10a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efad89b6ab824feaaee563e34e48cd85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74ec0b0ae60c4d4c830705f2c9d308d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad53ba2c3a374b8ab1af61e5317a66aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d66eca3b37e4614830e42a68800f780": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81e811bfdfc941f19a1108a525e1b689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5af50e1ff21c4cc1be8f54effdeddad5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_583ced1be4494111b0c107260d20669d",
              "IPY_MODEL_8515eb63a83e479dafd6af73ad68c2bf",
              "IPY_MODEL_6707cd1a018c43149371ae77f4bfd2d1"
            ],
            "layout": "IPY_MODEL_e06620106e35460a84c313f2247a6d5c"
          }
        },
        "583ced1be4494111b0c107260d20669d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f03204999275446694e85becbe4289d3",
            "placeholder": "​",
            "style": "IPY_MODEL_41526f88c79149d78b261a410c406991",
            "value": "model.safetensors: 100%"
          }
        },
        "8515eb63a83e479dafd6af73ad68c2bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fceb30274a09465c800cbce9f0f11f13",
            "max": 496254442,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a65555768e446b69b773f126d202c83",
            "value": 496254442
          }
        },
        "6707cd1a018c43149371ae77f4bfd2d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cd95ceaeaf04f9b9e90b215e6713792",
            "placeholder": "​",
            "style": "IPY_MODEL_025975b5d0be43d9b84ce83bbb09c8fc",
            "value": " 496M/496M [00:05&lt;00:00, 67.7MB/s]"
          }
        },
        "e06620106e35460a84c313f2247a6d5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f03204999275446694e85becbe4289d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41526f88c79149d78b261a410c406991": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fceb30274a09465c800cbce9f0f11f13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a65555768e446b69b773f126d202c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cd95ceaeaf04f9b9e90b215e6713792": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "025975b5d0be43d9b84ce83bbb09c8fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "861510e3c49646289d73647fef52038d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae855120120a40e19293dffca251b23f",
              "IPY_MODEL_d58ac7623a3241429908485ebc54269a",
              "IPY_MODEL_cf1c9f66464249edb9ff1ea58c045b35"
            ],
            "layout": "IPY_MODEL_416ef9fb4c8641e7aa78ffbafc46a81c"
          }
        },
        "ae855120120a40e19293dffca251b23f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05396522bb61467798082c1651a4fe5e",
            "placeholder": "​",
            "style": "IPY_MODEL_ae4e8089a4bf4800a4f97c6842963ff1",
            "value": "(…)quad2/resolve/main/tokenizer_config.json: 100%"
          }
        },
        "d58ac7623a3241429908485ebc54269a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2f6277f9a434c4496efe58a8c34b390",
            "max": 79,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d85c511a30d42668dc45b7c67e2193d",
            "value": 79
          }
        },
        "cf1c9f66464249edb9ff1ea58c045b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f97a1695cd5494cbdefa22496ea0fc5",
            "placeholder": "​",
            "style": "IPY_MODEL_4992c3bea6ee4c6f879e740cac5722b6",
            "value": " 79.0/79.0 [00:00&lt;00:00, 3.81kB/s]"
          }
        },
        "416ef9fb4c8641e7aa78ffbafc46a81c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05396522bb61467798082c1651a4fe5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae4e8089a4bf4800a4f97c6842963ff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2f6277f9a434c4496efe58a8c34b390": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d85c511a30d42668dc45b7c67e2193d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f97a1695cd5494cbdefa22496ea0fc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4992c3bea6ee4c6f879e740cac5722b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7c1967746b04b46bf21d1d9ab13d576": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_473945e2eae44eb1af6ac42313b6d843",
              "IPY_MODEL_af185998c31c479abb3d8cd4268bde9b",
              "IPY_MODEL_010aba6c097d427485691d0b6677e2f2"
            ],
            "layout": "IPY_MODEL_71e755bdaf394b51903aced8e3b629c0"
          }
        },
        "473945e2eae44eb1af6ac42313b6d843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35854491863641c5b16094e036b47408",
            "placeholder": "​",
            "style": "IPY_MODEL_0b2d91788b4f4078b6d9a0b7600a7718",
            "value": "(…)erta-base-squad2/resolve/main/vocab.json: 100%"
          }
        },
        "af185998c31c479abb3d8cd4268bde9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00bf5ad9aebe47fd9c4f9c48bc7c1174",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3a2f373973c4b688a04b54d6143c5d5",
            "value": 898822
          }
        },
        "010aba6c097d427485691d0b6677e2f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d5474d5e502494ea15e82f9425ff907",
            "placeholder": "​",
            "style": "IPY_MODEL_4cb5b70f85ad425b9d548a40b3672c05",
            "value": " 899k/899k [00:00&lt;00:00, 7.28MB/s]"
          }
        },
        "71e755bdaf394b51903aced8e3b629c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35854491863641c5b16094e036b47408": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b2d91788b4f4078b6d9a0b7600a7718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00bf5ad9aebe47fd9c4f9c48bc7c1174": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3a2f373973c4b688a04b54d6143c5d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8d5474d5e502494ea15e82f9425ff907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cb5b70f85ad425b9d548a40b3672c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "159aeae01f85490fb8fea2a288fd4eeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7038a2f2af6e4e19ad2128541f23f712",
              "IPY_MODEL_823c9e56493c429c8fb20a21c546927e",
              "IPY_MODEL_f24525528c514051ad0f54160aeace51"
            ],
            "layout": "IPY_MODEL_8a96cbadc4df43259ef8cae8ab4e55e3"
          }
        },
        "7038a2f2af6e4e19ad2128541f23f712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f257a4399b548f489dbd5c043680e9a",
            "placeholder": "​",
            "style": "IPY_MODEL_817e725058184deeb98f5814d17d9e73",
            "value": "(…)erta-base-squad2/resolve/main/merges.txt: 100%"
          }
        },
        "823c9e56493c429c8fb20a21c546927e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c67528430ff7477eaea8777bf798fe38",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b3742e015aae4ebbb353e292a78b79fc",
            "value": 456318
          }
        },
        "f24525528c514051ad0f54160aeace51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_129cc1d802844a95aead3af09846970b",
            "placeholder": "​",
            "style": "IPY_MODEL_864c93fbc72443f6ac13aeec4b73e684",
            "value": " 456k/456k [00:00&lt;00:00, 15.8MB/s]"
          }
        },
        "8a96cbadc4df43259ef8cae8ab4e55e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f257a4399b548f489dbd5c043680e9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "817e725058184deeb98f5814d17d9e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c67528430ff7477eaea8777bf798fe38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3742e015aae4ebbb353e292a78b79fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "129cc1d802844a95aead3af09846970b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "864c93fbc72443f6ac13aeec4b73e684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b15658490562436e87d0b3544483473d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6bef671f205f4a2fa6359f6e9cff4775",
              "IPY_MODEL_f00ef8acfb6049949891fd179435a2a5",
              "IPY_MODEL_217b494a20bf42dc97ba7d43761a6adb"
            ],
            "layout": "IPY_MODEL_a56640091ffa4ca286ea8f396df11511"
          }
        },
        "6bef671f205f4a2fa6359f6e9cff4775": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9333740368ac46c5947a04865fc52d5b",
            "placeholder": "​",
            "style": "IPY_MODEL_2ab92deb9e8042b0b6cbc44ed8bc9509",
            "value": "(…)ad2/resolve/main/special_tokens_map.json: 100%"
          }
        },
        "f00ef8acfb6049949891fd179435a2a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6319312dec6a429fa02b81be7504f048",
            "max": 772,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3eea7f32fe164100a7f61897da28b430",
            "value": 772
          }
        },
        "217b494a20bf42dc97ba7d43761a6adb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82d604a0d5e546e6a9f557d85740861b",
            "placeholder": "​",
            "style": "IPY_MODEL_894379684e6e45b292a059aed5bb0aa7",
            "value": " 772/772 [00:00&lt;00:00, 31.6kB/s]"
          }
        },
        "a56640091ffa4ca286ea8f396df11511": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9333740368ac46c5947a04865fc52d5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ab92deb9e8042b0b6cbc44ed8bc9509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6319312dec6a429fa02b81be7504f048": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3eea7f32fe164100a7f61897da28b430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82d604a0d5e546e6a9f557d85740861b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "894379684e6e45b292a059aed5bb0aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd1753f0bbc944f38eff0931f89f3c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48aa5e0154db47beb7f8ee878fd2f670",
              "IPY_MODEL_ef4ff9a5a1014e919f42e05437a7e322",
              "IPY_MODEL_866c55b898f64a588ca141ffa7dc5181"
            ],
            "layout": "IPY_MODEL_6b396b461ed44e49bd90ed64c612178b"
          }
        },
        "48aa5e0154db47beb7f8ee878fd2f670": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_486dd1bda8844b779b8e7b593eac174a",
            "placeholder": "​",
            "style": "IPY_MODEL_48dd101822b54966b87005cce1658997",
            "value": "Batches: 100%"
          }
        },
        "ef4ff9a5a1014e919f42e05437a7e322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8743d4142e5e45f5bcd7e94fef1f83df",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5db3eb62bdea467d8a8897bb7e155917",
            "value": 1
          }
        },
        "866c55b898f64a588ca141ffa7dc5181": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f089d7dc363a47dcabd7170cb8d2db94",
            "placeholder": "​",
            "style": "IPY_MODEL_886f3b092ca040558ae08593eca4ebd4",
            "value": " 1/1 [00:00&lt;00:00, 15.49it/s]"
          }
        },
        "6b396b461ed44e49bd90ed64c612178b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "486dd1bda8844b779b8e7b593eac174a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48dd101822b54966b87005cce1658997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8743d4142e5e45f5bcd7e94fef1f83df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5db3eb62bdea467d8a8897bb7e155917": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f089d7dc363a47dcabd7170cb8d2db94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "886f3b092ca040558ae08593eca4ebd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6debbb4aacea45119818d1f8fc3f8a31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2abade3efd504de9b028188f8d308a53",
              "IPY_MODEL_d3a9f1fc834e469abfdae39421031520",
              "IPY_MODEL_3a7f876f1d354fe2b7889e912ce9641e"
            ],
            "layout": "IPY_MODEL_991cb0afe1924a60bb0fac26237bb9cf"
          }
        },
        "2abade3efd504de9b028188f8d308a53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f73799f73864ffe91c303a29b400bfd",
            "placeholder": "​",
            "style": "IPY_MODEL_108398cdfb6e4d258892f4107370ba0e",
            "value": "Downloading data files: 100%"
          }
        },
        "d3a9f1fc834e469abfdae39421031520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e182cebc092496cae4a42e5b91cef8f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55fd86e9a111462681686de1b117a0fc",
            "value": 1
          }
        },
        "3a7f876f1d354fe2b7889e912ce9641e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3e1d62e0a674dcd9d95ba6f6ff0d509",
            "placeholder": "​",
            "style": "IPY_MODEL_3d4d74b2da5243ad93a0e2a379822a32",
            "value": " 1/1 [00:00&lt;00:00, 47.98it/s]"
          }
        },
        "991cb0afe1924a60bb0fac26237bb9cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f73799f73864ffe91c303a29b400bfd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "108398cdfb6e4d258892f4107370ba0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e182cebc092496cae4a42e5b91cef8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55fd86e9a111462681686de1b117a0fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3e1d62e0a674dcd9d95ba6f6ff0d509": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d4d74b2da5243ad93a0e2a379822a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57a8b33ed00f4e0ebcb0be65e7ddfca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_908284fde66c479b8e05fc3bfb11cc05",
              "IPY_MODEL_65587ab1f6aa4d5bb85a2ac979fff241",
              "IPY_MODEL_54f92e410bcf4981be39abb0ab749c39"
            ],
            "layout": "IPY_MODEL_18a678b89b7648c98f6c304a95acf1a5"
          }
        },
        "908284fde66c479b8e05fc3bfb11cc05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4644b519e3741e48c4a5486fd28ea24",
            "placeholder": "​",
            "style": "IPY_MODEL_541ac43e4c1540e7a489491758c84ba8",
            "value": "Extracting data files: 100%"
          }
        },
        "65587ab1f6aa4d5bb85a2ac979fff241": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc9a7215caf94535804906dc25f1bfa2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ed8cf8c1e32467892bee21966450f41",
            "value": 1
          }
        },
        "54f92e410bcf4981be39abb0ab749c39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6197738ead1e4d6bbcc7f0a95f8c82e0",
            "placeholder": "​",
            "style": "IPY_MODEL_77411c7b048443dca52d89400a7033cd",
            "value": " 1/1 [00:00&lt;00:00,  2.06it/s]"
          }
        },
        "18a678b89b7648c98f6c304a95acf1a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4644b519e3741e48c4a5486fd28ea24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "541ac43e4c1540e7a489491758c84ba8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc9a7215caf94535804906dc25f1bfa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ed8cf8c1e32467892bee21966450f41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6197738ead1e4d6bbcc7f0a95f8c82e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77411c7b048443dca52d89400a7033cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dcfec40520141069773ae0abc5f4b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f04ff8f37754cc69a861f72c3f2d2bb",
              "IPY_MODEL_cbd548e06fd4403d913d451c5b796728",
              "IPY_MODEL_ca5d926c84a2430d9491765370872a48"
            ],
            "layout": "IPY_MODEL_fe25dfce69484e7c8d59ad29ee5ab041"
          }
        },
        "4f04ff8f37754cc69a861f72c3f2d2bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f095ca0ee1744820898b77b13eda17cb",
            "placeholder": "​",
            "style": "IPY_MODEL_8df199d7d61b43fca7b7ef58940e7992",
            "value": "Generating train split: "
          }
        },
        "cbd548e06fd4403d913d451c5b796728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41d6658555a2424bb5f03654573bc3c7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef25d501bcd143ddbda0f461aa64f961",
            "value": 1
          }
        },
        "ca5d926c84a2430d9491765370872a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fb6c48486424e1d8fdb94405164985c",
            "placeholder": "​",
            "style": "IPY_MODEL_7f7637d959944fdaab6663710380c7f8",
            "value": " 186/0 [00:00&lt;00:00, 3017.52 examples/s]"
          }
        },
        "fe25dfce69484e7c8d59ad29ee5ab041": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f095ca0ee1744820898b77b13eda17cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8df199d7d61b43fca7b7ef58940e7992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41d6658555a2424bb5f03654573bc3c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ef25d501bcd143ddbda0f461aa64f961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fb6c48486424e1d8fdb94405164985c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f7637d959944fdaab6663710380c7f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fed40efe61fd42d8b817601a634525d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_597420bbbeca4ea2b95ca274fc9bf2b5",
              "IPY_MODEL_d371edeb48d941938a44da1ef5f25aea",
              "IPY_MODEL_92b5385689cb46cc9ac3907695b5f237"
            ],
            "layout": "IPY_MODEL_ac98661c88fa4d9a88ba7e18b0b71fc1"
          }
        },
        "597420bbbeca4ea2b95ca274fc9bf2b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_550bad64c378414fb831b202bd6fe0df",
            "placeholder": "​",
            "style": "IPY_MODEL_e316b6d07a1e470bb37e6df3db947144",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "d371edeb48d941938a44da1ef5f25aea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cebc973f9db47879701753976b48616",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38d9c5cef31c4ba48c1d608083d0cded",
            "value": 665
          }
        },
        "92b5385689cb46cc9ac3907695b5f237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4b9c5f66dfb4f3087b79f6db772c2dd",
            "placeholder": "​",
            "style": "IPY_MODEL_95388bcea0bf41c986e10d661e2551dd",
            "value": " 665/665 [00:00&lt;00:00, 24.7kB/s]"
          }
        },
        "ac98661c88fa4d9a88ba7e18b0b71fc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "550bad64c378414fb831b202bd6fe0df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e316b6d07a1e470bb37e6df3db947144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cebc973f9db47879701753976b48616": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38d9c5cef31c4ba48c1d608083d0cded": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c4b9c5f66dfb4f3087b79f6db772c2dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95388bcea0bf41c986e10d661e2551dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be3a28b2e4a1455bb639a6770eaacb81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b12b07d8e5c24ed181f7f3cabec23285",
              "IPY_MODEL_8b55cdd5b7a241bc9be11dbd9fda3202",
              "IPY_MODEL_eaef3cbdd75e4785b307525ea56db35b"
            ],
            "layout": "IPY_MODEL_a70fe5cbcc994078abc54b7eba9efcab"
          }
        },
        "b12b07d8e5c24ed181f7f3cabec23285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94faa6c6d38a41b88a447163fdef9345",
            "placeholder": "​",
            "style": "IPY_MODEL_7d658739ed6242a3b7370055415841c3",
            "value": "Downloading model.safetensors: 100%"
          }
        },
        "8b55cdd5b7a241bc9be11dbd9fda3202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a76dfed99d44a23aa065a89ad361014",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a383fe8c920e42b7b0a8e96188a5621c",
            "value": 548105171
          }
        },
        "eaef3cbdd75e4785b307525ea56db35b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0803d8e230a74226956089d6b957b46c",
            "placeholder": "​",
            "style": "IPY_MODEL_9786f103e4604cc29b67c6186a224558",
            "value": " 548M/548M [00:04&lt;00:00, 161MB/s]"
          }
        },
        "a70fe5cbcc994078abc54b7eba9efcab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94faa6c6d38a41b88a447163fdef9345": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d658739ed6242a3b7370055415841c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a76dfed99d44a23aa065a89ad361014": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a383fe8c920e42b7b0a8e96188a5621c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0803d8e230a74226956089d6b957b46c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9786f103e4604cc29b67c6186a224558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dd888022bf0e4caf8e32df48b5b23771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0348c2bcb7ae48bfbdbae235341b8ece",
              "IPY_MODEL_177f790316f142329fbbbca1670a7377",
              "IPY_MODEL_1176f5c93a704f8187ff6100fe0ec472"
            ],
            "layout": "IPY_MODEL_62614f4f7d2048b38a1d2944ef5868be"
          }
        },
        "0348c2bcb7ae48bfbdbae235341b8ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beab94bc521f46d89f2b04aa0a73912a",
            "placeholder": "​",
            "style": "IPY_MODEL_5f9f0c8b2cd241588c9d047ce0992e37",
            "value": "Downloading (…)neration_config.json: 100%"
          }
        },
        "177f790316f142329fbbbca1670a7377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bdd4f9d7fdf4b658e3db81a060818b1",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36d28bff2433435f8158862025110664",
            "value": 124
          }
        },
        "1176f5c93a704f8187ff6100fe0ec472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_793deb07f6a14e759bc2e4cddbf94ab0",
            "placeholder": "​",
            "style": "IPY_MODEL_32c047e180e3441d9de132a9d6c0f79c",
            "value": " 124/124 [00:00&lt;00:00, 8.93kB/s]"
          }
        },
        "62614f4f7d2048b38a1d2944ef5868be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "beab94bc521f46d89f2b04aa0a73912a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f9f0c8b2cd241588c9d047ce0992e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2bdd4f9d7fdf4b658e3db81a060818b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36d28bff2433435f8158862025110664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "793deb07f6a14e759bc2e4cddbf94ab0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32c047e180e3441d9de132a9d6c0f79c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abb3b563336a42f9b5b015a59c30ba4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b767ac70b23480e908c9f96ee88de02",
              "IPY_MODEL_67a0b1e7c9e841a799f8eeac5b10fbcb",
              "IPY_MODEL_0be0a83ede9a4b26ae1370edd1106558"
            ],
            "layout": "IPY_MODEL_99f7693790a1411dad4b64e7f86173a8"
          }
        },
        "5b767ac70b23480e908c9f96ee88de02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c5f960fff214769b0683d5d335889bc",
            "placeholder": "​",
            "style": "IPY_MODEL_29d0fa3ed339428e9a2df8ebcb3af8da",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "67a0b1e7c9e841a799f8eeac5b10fbcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b443f83d4324918b6541253317c9385",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44f25f6cd62d485ebac385a7ab2e37dc",
            "value": 1042301
          }
        },
        "0be0a83ede9a4b26ae1370edd1106558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f94185a5b516456c9d33565e0ee82add",
            "placeholder": "​",
            "style": "IPY_MODEL_4ebe190598464c7ab725dad9f62c2d11",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 9.46MB/s]"
          }
        },
        "99f7693790a1411dad4b64e7f86173a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c5f960fff214769b0683d5d335889bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29d0fa3ed339428e9a2df8ebcb3af8da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b443f83d4324918b6541253317c9385": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44f25f6cd62d485ebac385a7ab2e37dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f94185a5b516456c9d33565e0ee82add": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ebe190598464c7ab725dad9f62c2d11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "876aab1dd6164ff1bf28d46e0087c28a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_237e0db6dc104bd197792348b0df55e8",
              "IPY_MODEL_20b68ffce5ef449f8e0d2f5000b67432",
              "IPY_MODEL_5d3befb7a30d4a4fa4b619fb5fcb8a24"
            ],
            "layout": "IPY_MODEL_238f0d1205854aafaf2fbe12e029bd6c"
          }
        },
        "237e0db6dc104bd197792348b0df55e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_802dc597fe224c469a94942c175edc6f",
            "placeholder": "​",
            "style": "IPY_MODEL_a3caa01a433c423697cb8aad6ac753da",
            "value": "Downloading (…)olve/main/merges.txt: 100%"
          }
        },
        "20b68ffce5ef449f8e0d2f5000b67432": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26a12064c99f4f8593b7673bd4011e1b",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18edd0e16b424d28adc662355d05ef8e",
            "value": 456318
          }
        },
        "5d3befb7a30d4a4fa4b619fb5fcb8a24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb0c88b2e51d446cafdc749d610403fb",
            "placeholder": "​",
            "style": "IPY_MODEL_9e492c61c2034407b61572e5f69fac8e",
            "value": " 456k/456k [00:00&lt;00:00, 14.6MB/s]"
          }
        },
        "238f0d1205854aafaf2fbe12e029bd6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "802dc597fe224c469a94942c175edc6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3caa01a433c423697cb8aad6ac753da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26a12064c99f4f8593b7673bd4011e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18edd0e16b424d28adc662355d05ef8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb0c88b2e51d446cafdc749d610403fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e492c61c2034407b61572e5f69fac8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7267a11584014fe2a701020d32d6ca9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_996b1db325564b01bfdfbd29f6ed4b85",
              "IPY_MODEL_a83f42c72a1c4b70a2f6255102fb9466",
              "IPY_MODEL_34292a4190f54323b024637edf02a4a1"
            ],
            "layout": "IPY_MODEL_fd814e7c3a3e487c9613f644bd5cea35"
          }
        },
        "996b1db325564b01bfdfbd29f6ed4b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb27052611e84cfcab970d9dd51df093",
            "placeholder": "​",
            "style": "IPY_MODEL_2b9706e7dfa24700abac8036ad0878d9",
            "value": "Downloading (…)/main/tokenizer.json: 100%"
          }
        },
        "a83f42c72a1c4b70a2f6255102fb9466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eb528925850454db9ac06a96feeab5d",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_89474a1af8c541108cb50ca56716e505",
            "value": 1355256
          }
        },
        "34292a4190f54323b024637edf02a4a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07adf6cd556f41d48e64b27614ee8b09",
            "placeholder": "​",
            "style": "IPY_MODEL_80a5da168acf455297230216b17d579f",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 4.00MB/s]"
          }
        },
        "fd814e7c3a3e487c9613f644bd5cea35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb27052611e84cfcab970d9dd51df093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b9706e7dfa24700abac8036ad0878d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eb528925850454db9ac06a96feeab5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89474a1af8c541108cb50ca56716e505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07adf6cd556f41d48e64b27614ee8b09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80a5da168acf455297230216b17d579f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc98937a138444479e97f0b216b704ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7beb0b054aa64d6badbd9e0ad9ba3d9f",
              "IPY_MODEL_4374359398c04fc2ba687d2fb8b6b82f",
              "IPY_MODEL_de9d72c5e4604c2792709ef5b0003fcb"
            ],
            "layout": "IPY_MODEL_5757dc9f6b18430e896f60e3e83553bd"
          }
        },
        "7beb0b054aa64d6badbd9e0ad9ba3d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_214b69019400402caa86472191075267",
            "placeholder": "​",
            "style": "IPY_MODEL_c93178eceb5b429f918a4e55dae4f5a5",
            "value": "Map: 100%"
          }
        },
        "4374359398c04fc2ba687d2fb8b6b82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_237bd2bdefd8461bab2261b56dbe04b1",
            "max": 186,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6e54462ff274ae1beaae28c4543ad76",
            "value": 186
          }
        },
        "de9d72c5e4604c2792709ef5b0003fcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77fcc4fac3b7446e8a19251e4df24be6",
            "placeholder": "​",
            "style": "IPY_MODEL_db76710e8d834e63892208d395473310",
            "value": " 186/186 [00:00&lt;00:00, 1551.16 examples/s]"
          }
        },
        "5757dc9f6b18430e896f60e3e83553bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "214b69019400402caa86472191075267": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c93178eceb5b429f918a4e55dae4f5a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "237bd2bdefd8461bab2261b56dbe04b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6e54462ff274ae1beaae28c4543ad76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "77fcc4fac3b7446e8a19251e4df24be6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db76710e8d834e63892208d395473310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Packages"
      ],
      "metadata": {
        "id": "ulMnfIrO5Klm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iM4zfZrDOBA_",
        "outputId": "8a5e65a4-44ee-44ac-ef30-02ba70c0a586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]\n",
            "  Downloading farm_haystack-1.22.1-py3-none-any.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.0/856.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting boilerpy3 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading boilerpy3-1.0.7-py3-none-any.whl (22 kB)\n",
            "Collecting events (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading Events-0.5-py3-none-any.whl (6.8 kB)\n",
            "Collecting httpx (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (4.19.2)\n",
            "Collecting lazy-imports==0.3.1 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (3.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (9.4.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (3.11.0)\n",
            "Collecting posthog (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Collecting prompthub-py==4.0.0 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading prompthub_py-4.0.0-py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: pydantic<2 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (1.10.13)\n",
            "Collecting quantulum3 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading quantulum3-0.9.0-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rank-bm25 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (2.31.0)\n",
            "Collecting requests-cache<1.0.0 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading requests_cache-0.9.8-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn>=1.3.0 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sseclient-py (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (8.2.3)\n",
            "Collecting tiktoken>=0.5.1 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (4.66.1)\n",
            "Collecting transformers==4.34.1 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdf2image>1.14 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading pdf2image-1.16.3-py3-none-any.whl (11 kB)\n",
            "Collecting pytesseract>0.3.7 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading pytesseract-0.3.10-py3-none-any.whl (14 kB)\n",
            "Collecting pillow (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading Pillow-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-ai-formrecognizer>=3.2.0b2 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading azure_ai_formrecognizer-3.3.2-py3-none-any.whl (300 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.1/300.1 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (4.11.2)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (3.5.1)\n",
            "Collecting python-docx (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-frontmatter (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading python_frontmatter-1.0.1-py3-none-any.whl (9.2 kB)\n",
            "Collecting python-magic (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Collecting tika (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading tika-2.6.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymupdf>=1.18.16 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading PyMuPDF-1.23.6-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (3.8.1)\n",
            "Collecting huggingface-hub>=0.5.0 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading huggingface_hub-0.19.3-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers>=2.2.0 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from prompthub-py==4.0.0->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (23.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (2.1.0+cu118)\n",
            "Collecting accelerate>=0.20.3 (from transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92,>=0.1.91 (from transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (3.20.3)\n",
            "Collecting azure-core<2.0.0,>=1.23.0 (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading azure_core-1.29.5-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.4/192.4 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msrest>=0.6.21 (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-common~=1.1 (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (4.5.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.5.0->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (2023.6.0)\n",
            "Collecting PyMuPDFb==1.23.6 (from pymupdf>=1.18.16->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading PyMuPDFb-1.23.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (2023.7.22)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (1.4.4)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (23.1.0)\n",
            "Collecting cattrs>=22.2 (from requests-cache<1.0.0->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading cattrs-23.1.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting url-normalize>=1.4 (from requests-cache<1.0.0->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (1.11.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (3.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.2.0->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (0.16.0+cu118)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (2.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (3.7.1)\n",
            "Collecting httpcore (from httpx->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (1.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (2023.7.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (0.30.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (0.12.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (8.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (2023.3.post1)\n",
            "Collecting monotonic>=1.5 (from posthog->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (4.9.3)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from quantulum3->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (7.0.0)\n",
            "Collecting num2words (from quantulum3->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tika->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (67.7.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (5.9.5)\n",
            "Collecting typing-extensions>=4.0.1 (from azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.2->requests-cache<1.0.0->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (1.1.3)\n",
            "Collecting isodate>=0.6.0 (from msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (1.3.1)\n",
            "Collecting huggingface-hub>=0.5.0 (from farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.10->transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (2.1.0)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore->httpx->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docopt>=0.6.2 (from num2words->quantulum3->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing])\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-formrecognizer>=3.2.0b2->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.10->transformers==4.34.1->farm-haystack[colab,file-conversion,inference,ocr,pdf,preprocessing]) (1.3.0)\n",
            "Building wheels for collected packages: sentence-transformers, langdetect, tika, docopt\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=9e450d02d55f42de6df77d82521cf2b8241b1f61c4630a15c149b3fb6112d8b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=8f1e805e14084a44283ef405189cc5fbeb48c2d69450f8b952258a0e6e4eecee\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-2.6.0-py3-none-any.whl size=32621 sha256=0828ffeef85df7deff6d06b402f8331f7d849c9816e7e9aa6216bba3c653e064\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/71/c7/b757709531121b1700cffda5b6b0d4aad095fb507ec84316d0\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=619927678c8c6c5ede5605e7f4b385333576760758a6232841a29d0cd1c308df\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built sentence-transformers langdetect tika docopt\n",
            "Installing collected packages: sseclient-py, sentencepiece, monotonic, events, docopt, azure-common, url-normalize, typing-extensions, safetensors, rank-bm25, python-magic, python-frontmatter, PyMuPDFb, pillow, num2words, lazy-imports, langdetect, isodate, h11, boilerpy3, backoff, tiktoken, tika, scikit-learn, python-docx, pytesseract, pymupdf, prompthub-py, posthog, pdf2image, huggingface-hub, httpcore, cattrs, azure-core, tokenizers, requests-cache, msrest, httpx, accelerate, transformers, quantulum3, azure-ai-formrecognizer, sentence-transformers, farm-haystack\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMuPDFb-1.23.6 accelerate-0.24.1 azure-ai-formrecognizer-3.3.2 azure-common-1.1.28 azure-core-1.29.5 backoff-2.2.1 boilerpy3-1.0.7 cattrs-23.1.2 docopt-0.6.2 events-0.5 farm-haystack-1.22.1 h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 huggingface-hub-0.17.3 isodate-0.6.1 langdetect-1.0.9 lazy-imports-0.3.1 monotonic-1.6 msrest-0.7.1 num2words-0.5.13 pdf2image-1.16.3 pillow-9.0.0 posthog-3.0.2 prompthub-py-4.0.0 pymupdf-1.23.6 pytesseract-0.3.10 python-docx-1.1.0 python-frontmatter-1.0.1 python-magic-0.4.27 quantulum3-0.9.0 rank-bm25-0.2.2 requests-cache-0.9.8 safetensors-0.4.0 scikit-learn-1.3.2 sentence-transformers-2.2.2 sentencepiece-0.1.99 sseclient-py-1.8.0 tika-2.6.0 tiktoken-0.5.1 tokenizers-0.14.1 transformers-4.34.1 typing-extensions-4.8.0 url-normalize-1.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.34.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2023.7.22)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.1\n",
            "Collecting playwright\n",
            "  Downloading playwright-1.39.0-py3-none-manylinux1_x86_64.whl (35.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.5/35.5 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting greenlet==3.0.0 (from playwright)\n",
            "  Downloading greenlet-3.0.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.9/612.9 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyee==11.0.1 (from playwright)\n",
            "  Downloading pyee-11.0.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee==11.0.1->playwright) (4.8.0)\n",
            "Installing collected packages: pyee, greenlet, playwright\n",
            "  Attempting uninstall: greenlet\n",
            "    Found existing installation: greenlet 3.0.1\n",
            "    Uninstalling greenlet-3.0.1:\n",
            "      Successfully uninstalled greenlet-3.0.1\n",
            "Successfully installed greenlet-3.0.0 playwright-1.39.0 pyee-11.0.1\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.5.8)\n",
            "Downloading Chromium 119.0.6045.9 (playwright build v1084)\u001b[2m from https://playwright.azureedge.net/builds/chromium/1084/chromium-linux.zip\u001b[22m\n",
            "\u001b[1G155.8 Mb [] 0% 0.0s\u001b[0K\u001b[1G155.8 Mb [] 0% 14.5s\u001b[0K\u001b[1G155.8 Mb [] 0% 6.5s\u001b[0K\u001b[1G155.8 Mb [] 0% 6.4s\u001b[0K\u001b[1G155.8 Mb [] 1% 6.0s\u001b[0K\u001b[1G155.8 Mb [] 1% 5.6s\u001b[0K\u001b[1G155.8 Mb [] 1% 5.1s\u001b[0K\u001b[1G155.8 Mb [] 2% 5.1s\u001b[0K\u001b[1G155.8 Mb [] 2% 4.9s\u001b[0K\u001b[1G155.8 Mb [] 3% 4.9s\u001b[0K\u001b[1G155.8 Mb [] 3% 4.7s\u001b[0K\u001b[1G155.8 Mb [] 3% 4.8s\u001b[0K\u001b[1G155.8 Mb [] 4% 4.8s\u001b[0K\u001b[1G155.8 Mb [] 4% 5.0s\u001b[0K\u001b[1G155.8 Mb [] 4% 4.6s\u001b[0K\u001b[1G155.8 Mb [] 5% 4.7s\u001b[0K\u001b[1G155.8 Mb [] 5% 4.6s\u001b[0K\u001b[1G155.8 Mb [] 6% 4.4s\u001b[0K\u001b[1G155.8 Mb [] 7% 4.2s\u001b[0K\u001b[1G155.8 Mb [] 8% 4.1s\u001b[0K\u001b[1G155.8 Mb [] 8% 4.0s\u001b[0K\u001b[1G155.8 Mb [] 9% 3.9s\u001b[0K\u001b[1G155.8 Mb [] 10% 3.8s\u001b[0K\u001b[1G155.8 Mb [] 11% 3.8s\u001b[0K\u001b[1G155.8 Mb [] 11% 3.7s\u001b[0K\u001b[1G155.8 Mb [] 12% 3.6s\u001b[0K\u001b[1G155.8 Mb [] 12% 3.8s\u001b[0K\u001b[1G155.8 Mb [] 13% 3.8s\u001b[0K\u001b[1G155.8 Mb [] 14% 3.8s\u001b[0K\u001b[1G155.8 Mb [] 14% 3.7s\u001b[0K\u001b[1G155.8 Mb [] 15% 3.6s\u001b[0K\u001b[1G155.8 Mb [] 16% 3.6s\u001b[0K\u001b[1G155.8 Mb [] 16% 3.5s\u001b[0K\u001b[1G155.8 Mb [] 17% 3.5s\u001b[0K\u001b[1G155.8 Mb [] 18% 3.4s\u001b[0K\u001b[1G155.8 Mb [] 18% 3.3s\u001b[0K\u001b[1G155.8 Mb [] 19% 3.3s\u001b[0K\u001b[1G155.8 Mb [] 20% 3.2s\u001b[0K\u001b[1G155.8 Mb [] 20% 3.3s\u001b[0K\u001b[1G155.8 Mb [] 21% 3.2s\u001b[0K\u001b[1G155.8 Mb [] 22% 3.2s\u001b[0K\u001b[1G155.8 Mb [] 22% 3.1s\u001b[0K\u001b[1G155.8 Mb [] 23% 3.1s\u001b[0K\u001b[1G155.8 Mb [] 23% 3.0s\u001b[0K\u001b[1G155.8 Mb [] 24% 3.0s\u001b[0K\u001b[1G155.8 Mb [] 25% 3.0s\u001b[0K\u001b[1G155.8 Mb [] 25% 2.9s\u001b[0K\u001b[1G155.8 Mb [] 26% 2.9s\u001b[0K\u001b[1G155.8 Mb [] 27% 2.8s\u001b[0K\u001b[1G155.8 Mb [] 28% 2.8s\u001b[0K\u001b[1G155.8 Mb [] 28% 2.7s\u001b[0K\u001b[1G155.8 Mb [] 29% 2.7s\u001b[0K\u001b[1G155.8 Mb [] 30% 2.7s\u001b[0K\u001b[1G155.8 Mb [] 30% 2.6s\u001b[0K\u001b[1G155.8 Mb [] 31% 2.6s\u001b[0K\u001b[1G155.8 Mb [] 32% 2.5s\u001b[0K\u001b[1G155.8 Mb [] 33% 2.5s\u001b[0K\u001b[1G155.8 Mb [] 33% 2.4s\u001b[0K\u001b[1G155.8 Mb [] 34% 2.4s\u001b[0K\u001b[1G155.8 Mb [] 35% 2.4s\u001b[0K\u001b[1G155.8 Mb [] 35% 2.3s\u001b[0K\u001b[1G155.8 Mb [] 36% 2.3s\u001b[0K\u001b[1G155.8 Mb [] 37% 2.2s\u001b[0K\u001b[1G155.8 Mb [] 38% 2.2s\u001b[0K\u001b[1G155.8 Mb [] 38% 2.1s\u001b[0K\u001b[1G155.8 Mb [] 39% 2.2s\u001b[0K\u001b[1G155.8 Mb [] 40% 2.1s\u001b[0K\u001b[1G155.8 Mb [] 41% 2.1s\u001b[0K\u001b[1G155.8 Mb [] 42% 2.0s\u001b[0K\u001b[1G155.8 Mb [] 43% 1.9s\u001b[0K\u001b[1G155.8 Mb [] 44% 1.9s\u001b[0K\u001b[1G155.8 Mb [] 45% 1.8s\u001b[0K\u001b[1G155.8 Mb [] 46% 1.8s\u001b[0K\u001b[1G155.8 Mb [] 47% 1.8s\u001b[0K\u001b[1G155.8 Mb [] 47% 1.7s\u001b[0K\u001b[1G155.8 Mb [] 48% 1.7s\u001b[0K\u001b[1G155.8 Mb [] 49% 1.7s\u001b[0K\u001b[1G155.8 Mb [] 50% 1.7s\u001b[0K\u001b[1G155.8 Mb [] 51% 1.6s\u001b[0K\u001b[1G155.8 Mb [] 52% 1.6s\u001b[0K\u001b[1G155.8 Mb [] 53% 1.5s\u001b[0K\u001b[1G155.8 Mb [] 54% 1.5s\u001b[0K\u001b[1G155.8 Mb [] 55% 1.4s\u001b[0K\u001b[1G155.8 Mb [] 56% 1.4s\u001b[0K\u001b[1G155.8 Mb [] 57% 1.4s\u001b[0K\u001b[1G155.8 Mb [] 58% 1.3s\u001b[0K\u001b[1G155.8 Mb [] 59% 1.3s\u001b[0K\u001b[1G155.8 Mb [] 60% 1.2s\u001b[0K\u001b[1G155.8 Mb [] 61% 1.2s\u001b[0K\u001b[1G155.8 Mb [] 62% 1.2s\u001b[0K\u001b[1G155.8 Mb [] 63% 1.2s\u001b[0K\u001b[1G155.8 Mb [] 64% 1.1s\u001b[0K\u001b[1G155.8 Mb [] 65% 1.1s\u001b[0K\u001b[1G155.8 Mb [] 66% 1.1s\u001b[0K\u001b[1G155.8 Mb [] 66% 1.0s\u001b[0K\u001b[1G155.8 Mb [] 67% 1.0s\u001b[0K\u001b[1G155.8 Mb [] 68% 1.0s\u001b[0K\u001b[1G155.8 Mb [] 69% 1.0s\u001b[0K\u001b[1G155.8 Mb [] 70% 0.9s\u001b[0K\u001b[1G155.8 Mb [] 71% 0.9s\u001b[0K\u001b[1G155.8 Mb [] 72% 0.9s\u001b[0K\u001b[1G155.8 Mb [] 73% 0.8s\u001b[0K\u001b[1G155.8 Mb [] 74% 0.8s\u001b[0K\u001b[1G155.8 Mb [] 75% 0.8s\u001b[0K\u001b[1G155.8 Mb [] 76% 0.8s\u001b[0K\u001b[1G155.8 Mb [] 76% 0.7s\u001b[0K\u001b[1G155.8 Mb [] 77% 0.7s\u001b[0K\u001b[1G155.8 Mb [] 78% 0.7s\u001b[0K\u001b[1G155.8 Mb [] 79% 0.6s\u001b[0K\u001b[1G155.8 Mb [] 80% 0.6s\u001b[0K\u001b[1G155.8 Mb [] 81% 0.6s\u001b[0K\u001b[1G155.8 Mb [] 82% 0.5s\u001b[0K\u001b[1G155.8 Mb [] 84% 0.5s\u001b[0K\u001b[1G155.8 Mb [] 85% 0.4s\u001b[0K\u001b[1G155.8 Mb [] 86% 0.4s\u001b[0K\u001b[1G155.8 Mb [] 87% 0.4s\u001b[0K\u001b[1G155.8 Mb [] 88% 0.4s\u001b[0K\u001b[1G155.8 Mb [] 88% 0.3s\u001b[0K\u001b[1G155.8 Mb [] 89% 0.3s\u001b[0K\u001b[1G155.8 Mb [] 90% 0.3s\u001b[0K\u001b[1G155.8 Mb [] 91% 0.3s\u001b[0K\u001b[1G155.8 Mb [] 92% 0.2s\u001b[0K\u001b[1G155.8 Mb [] 93% 0.2s\u001b[0K\u001b[1G155.8 Mb [] 94% 0.2s\u001b[0K\u001b[1G155.8 Mb [] 95% 0.2s\u001b[0K\u001b[1G155.8 Mb [] 95% 0.1s\u001b[0K\u001b[1G155.8 Mb [] 96% 0.1s\u001b[0K\u001b[1G155.8 Mb [] 97% 0.1s\u001b[0K\u001b[1G155.8 Mb [] 98% 0.1s\u001b[0K\u001b[1G155.8 Mb [] 98% 0.0s\u001b[0K\u001b[1G155.8 Mb [] 99% 0.0s\u001b[0K\u001b[1G155.8 Mb [] 100% 0.0s\u001b[0K\n",
            "Chromium 119.0.6045.9 (playwright build v1084) downloaded to /root/.cache/ms-playwright/chromium-1084\n",
            "Downloading FFMPEG playwright build v1009\u001b[2m from https://playwright.azureedge.net/builds/ffmpeg/1009/ffmpeg-linux.zip\u001b[22m\n",
            "\u001b[1G2.6 Mb [] 0% 0.0s\u001b[0K\u001b[1G2.6 Mb [] 5% 0.3s\u001b[0K\u001b[1G2.6 Mb [] 22% 0.1s\u001b[0K\u001b[1G2.6 Mb [] 35% 0.1s\u001b[0K\u001b[1G2.6 Mb [] 48% 0.1s\u001b[0K\u001b[1G2.6 Mb [] 64% 0.0s\u001b[0K\u001b[1G2.6 Mb [] 74% 0.0s\u001b[0K\u001b[1G2.6 Mb [] 87% 0.0s\u001b[0K\u001b[1G2.6 Mb [] 100% 0.0s\u001b[0K\n",
            "FFMPEG playwright build v1009 downloaded to /root/.cache/ms-playwright/ffmpeg-1009\n",
            "Downloading Firefox 118.0.1 (playwright build v1425)\u001b[2m from https://playwright.azureedge.net/builds/firefox/1425/firefox-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G80.8 Mb [] 0% 0.0s\u001b[0K\u001b[1G80.8 Mb [] 0% 11.7s\u001b[0K\u001b[1G80.8 Mb [] 0% 5.0s\u001b[0K\u001b[1G80.8 Mb [] 1% 3.6s\u001b[0K\u001b[1G80.8 Mb [] 2% 2.4s\u001b[0K\u001b[1G80.8 Mb [] 3% 2.0s\u001b[0K\u001b[1G80.8 Mb [] 5% 1.7s\u001b[0K\u001b[1G80.8 Mb [] 5% 1.9s\u001b[0K\u001b[1G80.8 Mb [] 6% 1.9s\u001b[0K\u001b[1G80.8 Mb [] 7% 1.9s\u001b[0K\u001b[1G80.8 Mb [] 8% 2.0s\u001b[0K\u001b[1G80.8 Mb [] 9% 1.7s\u001b[0K\u001b[1G80.8 Mb [] 11% 1.6s\u001b[0K\u001b[1G80.8 Mb [] 12% 1.6s\u001b[0K\u001b[1G80.8 Mb [] 13% 1.6s\u001b[0K\u001b[1G80.8 Mb [] 14% 1.5s\u001b[0K\u001b[1G80.8 Mb [] 16% 1.4s\u001b[0K\u001b[1G80.8 Mb [] 17% 1.3s\u001b[0K\u001b[1G80.8 Mb [] 19% 1.3s\u001b[0K\u001b[1G80.8 Mb [] 20% 1.2s\u001b[0K\u001b[1G80.8 Mb [] 20% 1.3s\u001b[0K\u001b[1G80.8 Mb [] 20% 1.4s\u001b[0K\u001b[1G80.8 Mb [] 21% 1.3s\u001b[0K\u001b[1G80.8 Mb [] 23% 1.3s\u001b[0K\u001b[1G80.8 Mb [] 24% 1.2s\u001b[0K\u001b[1G80.8 Mb [] 25% 1.3s\u001b[0K\u001b[1G80.8 Mb [] 26% 1.2s\u001b[0K\u001b[1G80.8 Mb [] 28% 1.2s\u001b[0K\u001b[1G80.8 Mb [] 29% 1.1s\u001b[0K\u001b[1G80.8 Mb [] 30% 1.1s\u001b[0K\u001b[1G80.8 Mb [] 31% 1.1s\u001b[0K\u001b[1G80.8 Mb [] 32% 1.1s\u001b[0K\u001b[1G80.8 Mb [] 33% 1.1s\u001b[0K\u001b[1G80.8 Mb [] 34% 1.1s\u001b[0K\u001b[1G80.8 Mb [] 35% 1.0s\u001b[0K\u001b[1G80.8 Mb [] 36% 1.0s\u001b[0K\u001b[1G80.8 Mb [] 37% 1.0s\u001b[0K\u001b[1G80.8 Mb [] 39% 1.0s\u001b[0K\u001b[1G80.8 Mb [] 40% 1.0s\u001b[0K\u001b[1G80.8 Mb [] 42% 0.9s\u001b[0K\u001b[1G80.8 Mb [] 43% 0.9s\u001b[0K\u001b[1G80.8 Mb [] 45% 0.9s\u001b[0K\u001b[1G80.8 Mb [] 46% 0.8s\u001b[0K\u001b[1G80.8 Mb [] 47% 0.8s\u001b[0K\u001b[1G80.8 Mb [] 49% 0.8s\u001b[0K\u001b[1G80.8 Mb [] 52% 0.7s\u001b[0K\u001b[1G80.8 Mb [] 53% 0.7s\u001b[0K\u001b[1G80.8 Mb [] 56% 0.6s\u001b[0K\u001b[1G80.8 Mb [] 57% 0.6s\u001b[0K\u001b[1G80.8 Mb [] 58% 0.6s\u001b[0K\u001b[1G80.8 Mb [] 59% 0.6s\u001b[0K\u001b[1G80.8 Mb [] 61% 0.5s\u001b[0K\u001b[1G80.8 Mb [] 62% 0.5s\u001b[0K\u001b[1G80.8 Mb [] 64% 0.5s\u001b[0K\u001b[1G80.8 Mb [] 65% 0.5s\u001b[0K\u001b[1G80.8 Mb [] 68% 0.4s\u001b[0K\u001b[1G80.8 Mb [] 70% 0.4s\u001b[0K\u001b[1G80.8 Mb [] 72% 0.4s\u001b[0K\u001b[1G80.8 Mb [] 74% 0.4s\u001b[0K\u001b[1G80.8 Mb [] 75% 0.3s\u001b[0K\u001b[1G80.8 Mb [] 76% 0.3s\u001b[0K\u001b[1G80.8 Mb [] 79% 0.3s\u001b[0K\u001b[1G80.8 Mb [] 80% 0.3s\u001b[0K\u001b[1G80.8 Mb [] 81% 0.3s\u001b[0K\u001b[1G80.8 Mb [] 82% 0.2s\u001b[0K\u001b[1G80.8 Mb [] 84% 0.2s\u001b[0K\u001b[1G80.8 Mb [] 85% 0.2s\u001b[0K\u001b[1G80.8 Mb [] 88% 0.2s\u001b[0K\u001b[1G80.8 Mb [] 89% 0.1s\u001b[0K\u001b[1G80.8 Mb [] 90% 0.1s\u001b[0K\u001b[1G80.8 Mb [] 91% 0.1s\u001b[0K\u001b[1G80.8 Mb [] 92% 0.1s\u001b[0K\u001b[1G80.8 Mb [] 94% 0.1s\u001b[0K\u001b[1G80.8 Mb [] 97% 0.0s\u001b[0K\u001b[1G80.8 Mb [] 99% 0.0s\u001b[0K\u001b[1G80.8 Mb [] 100% 0.0s\u001b[0K\n",
            "Firefox 118.0.1 (playwright build v1425) downloaded to /root/.cache/ms-playwright/firefox-1425\n",
            "Downloading Webkit 17.4 (playwright build v1921)\u001b[2m from https://playwright.azureedge.net/builds/webkit/1921/webkit-ubuntu-22.04.zip\u001b[22m\n",
            "\u001b[1G82.9 Mb [] 0% 0.0s\u001b[0K\u001b[1G82.9 Mb [] 0% 14.4s\u001b[0K\u001b[1G82.9 Mb [] 0% 8.1s\u001b[0K\u001b[1G82.9 Mb [] 1% 4.1s\u001b[0K\u001b[1G82.9 Mb [] 1% 3.7s\u001b[0K\u001b[1G82.9 Mb [] 2% 3.4s\u001b[0K\u001b[1G82.9 Mb [] 3% 2.9s\u001b[0K\u001b[1G82.9 Mb [] 4% 2.7s\u001b[0K\u001b[1G82.9 Mb [] 5% 2.5s\u001b[0K\u001b[1G82.9 Mb [] 6% 2.3s\u001b[0K\u001b[1G82.9 Mb [] 7% 2.3s\u001b[0K\u001b[1G82.9 Mb [] 8% 2.3s\u001b[0K\u001b[1G82.9 Mb [] 9% 2.1s\u001b[0K\u001b[1G82.9 Mb [] 10% 2.2s\u001b[0K\u001b[1G82.9 Mb [] 10% 2.3s\u001b[0K\u001b[1G82.9 Mb [] 11% 2.2s\u001b[0K\u001b[1G82.9 Mb [] 12% 2.1s\u001b[0K\u001b[1G82.9 Mb [] 13% 2.0s\u001b[0K\u001b[1G82.9 Mb [] 13% 2.1s\u001b[0K\u001b[1G82.9 Mb [] 14% 2.1s\u001b[0K\u001b[1G82.9 Mb [] 15% 2.0s\u001b[0K\u001b[1G82.9 Mb [] 16% 2.0s\u001b[0K\u001b[1G82.9 Mb [] 17% 2.0s\u001b[0K\u001b[1G82.9 Mb [] 18% 2.1s\u001b[0K\u001b[1G82.9 Mb [] 19% 2.0s\u001b[0K\u001b[1G82.9 Mb [] 20% 2.0s\u001b[0K\u001b[1G82.9 Mb [] 24% 1.7s\u001b[0K\u001b[1G82.9 Mb [] 26% 1.7s\u001b[0K\u001b[1G82.9 Mb [] 27% 1.6s\u001b[0K\u001b[1G82.9 Mb [] 29% 1.5s\u001b[0K\u001b[1G82.9 Mb [] 29% 1.6s\u001b[0K\u001b[1G82.9 Mb [] 29% 1.8s\u001b[0K\u001b[1G82.9 Mb [] 29% 1.9s\u001b[0K\u001b[1G82.9 Mb [] 29% 2.0s\u001b[0K\u001b[1G82.9 Mb [] 29% 2.3s\u001b[0K\u001b[1G82.9 Mb [] 29% 2.4s\u001b[0K\u001b[1G82.9 Mb [] 30% 2.4s\u001b[0K\u001b[1G82.9 Mb [] 31% 2.3s\u001b[0K\u001b[1G82.9 Mb [] 32% 2.2s\u001b[0K\u001b[1G82.9 Mb [] 33% 2.1s\u001b[0K\u001b[1G82.9 Mb [] 34% 2.1s\u001b[0K\u001b[1G82.9 Mb [] 36% 2.0s\u001b[0K\u001b[1G82.9 Mb [] 37% 2.0s\u001b[0K\u001b[1G82.9 Mb [] 37% 1.9s\u001b[0K\u001b[1G82.9 Mb [] 38% 1.9s\u001b[0K\u001b[1G82.9 Mb [] 39% 1.9s\u001b[0K\u001b[1G82.9 Mb [] 40% 1.8s\u001b[0K\u001b[1G82.9 Mb [] 41% 1.8s\u001b[0K\u001b[1G82.9 Mb [] 42% 1.7s\u001b[0K\u001b[1G82.9 Mb [] 43% 1.7s\u001b[0K\u001b[1G82.9 Mb [] 45% 1.6s\u001b[0K\u001b[1G82.9 Mb [] 47% 1.6s\u001b[0K\u001b[1G82.9 Mb [] 48% 1.5s\u001b[0K\u001b[1G82.9 Mb [] 50% 1.4s\u001b[0K\u001b[1G82.9 Mb [] 52% 1.3s\u001b[0K\u001b[1G82.9 Mb [] 54% 1.2s\u001b[0K\u001b[1G82.9 Mb [] 56% 1.1s\u001b[0K\u001b[1G82.9 Mb [] 57% 1.1s\u001b[0K\u001b[1G82.9 Mb [] 58% 1.1s\u001b[0K\u001b[1G82.9 Mb [] 59% 1.0s\u001b[0K\u001b[1G82.9 Mb [] 61% 1.0s\u001b[0K\u001b[1G82.9 Mb [] 62% 0.9s\u001b[0K\u001b[1G82.9 Mb [] 64% 0.9s\u001b[0K\u001b[1G82.9 Mb [] 66% 0.8s\u001b[0K\u001b[1G82.9 Mb [] 68% 0.7s\u001b[0K\u001b[1G82.9 Mb [] 69% 0.7s\u001b[0K\u001b[1G82.9 Mb [] 71% 0.7s\u001b[0K\u001b[1G82.9 Mb [] 72% 0.6s\u001b[0K\u001b[1G82.9 Mb [] 73% 0.6s\u001b[0K\u001b[1G82.9 Mb [] 75% 0.5s\u001b[0K\u001b[1G82.9 Mb [] 77% 0.5s\u001b[0K\u001b[1G82.9 Mb [] 79% 0.4s\u001b[0K\u001b[1G82.9 Mb [] 80% 0.4s\u001b[0K\u001b[1G82.9 Mb [] 81% 0.4s\u001b[0K\u001b[1G82.9 Mb [] 82% 0.4s\u001b[0K\u001b[1G82.9 Mb [] 84% 0.3s\u001b[0K\u001b[1G82.9 Mb [] 85% 0.3s\u001b[0K\u001b[1G82.9 Mb [] 86% 0.3s\u001b[0K\u001b[1G82.9 Mb [] 87% 0.3s\u001b[0K\u001b[1G82.9 Mb [] 89% 0.2s\u001b[0K\u001b[1G82.9 Mb [] 91% 0.2s\u001b[0K\u001b[1G82.9 Mb [] 92% 0.2s\u001b[0K\u001b[1G82.9 Mb [] 92% 0.1s\u001b[0K\u001b[1G82.9 Mb [] 94% 0.1s\u001b[0K\u001b[1G82.9 Mb [] 96% 0.1s\u001b[0K\u001b[1G82.9 Mb [] 97% 0.0s\u001b[0K\u001b[1G82.9 Mb [] 99% 0.0s\u001b[0K\u001b[1G82.9 Mb [] 100% 0.0s\u001b[0K\n",
            "Webkit 17.4 (playwright build v1921) downloaded to /root/.cache/ms-playwright/webkit-1921\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Collecting trl\n",
            "  Downloading trl-0.7.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers>=4.18.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.34.1)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl) (1.23.5)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl) (0.24.1)\n",
            "Collecting datasets (from trl)\n",
            "  Downloading datasets-2.14.7-py3-none-any.whl (520 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m520.4/520.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tyro>=0.5.11 (from trl)\n",
            "  Downloading tyro-0.5.14-py3-none-any.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.0/100.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (0.17.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.18.0->trl) (4.66.1)\n",
            "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl)\n",
            "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.6.0)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n",
            "  Downloading shtab-1.6.4-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets->trl)\n",
            "  Downloading pyarrow_hotfix-0.5-py3-none-any.whl (7.8 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->trl)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.4.1)\n",
            "Collecting multiprocess (from datasets->trl)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.8.6)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.18.0->trl) (2023.7.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->trl) (1.16.0)\n",
            "Installing collected packages: shtab, pyarrow-hotfix, docstring-parser, dill, multiprocess, tyro, datasets, trl\n",
            "Successfully installed datasets-2.14.7 dill-0.3.7 docstring-parser-0.15 multiprocess-0.70.15 pyarrow-hotfix-0.5 shtab-1.6.4 trl-0.7.4 tyro-0.5.14\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install farm-haystack[colab,ocr,preprocessing,file-conversion,pdf,inference]\n",
        "!pip install -U sentence-transformers\n",
        "# Web-Scraping Utilities\n",
        "!pip install youtube-transcript-api\n",
        "!pip install playwright\n",
        "!pip install beautifulsoup4\n",
        "!pip install nest-asyncio\n",
        "!playwright install\n",
        "# LLM Finetuning\n",
        "!pip install transformers\n",
        "!pip install trl\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies"
      ],
      "metadata": {
        "id": "sFHcpmDx5TOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from haystack.utils import convert_files_to_docs\n",
        "from haystack.nodes import PreProcessor\n",
        "from haystack.document_stores import InMemoryDocumentStore\n",
        "from haystack.nodes import EmbeddingRetriever\n",
        "from haystack.nodes import FARMReader\n",
        "from haystack.pipelines import ExtractiveQAPipeline\n",
        "from haystack.utils import print_answers\n",
        "from haystack.nodes import EmbeddingRetriever\n",
        "# YouTube Transcript\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "# Web Scraping\n",
        "import requests\n",
        "from collections import Counter\n",
        "import statistics\n",
        "from playwright.async_api import async_playwright, Playwright\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "nest_asyncio.apply()\n",
        "# Finetuning LLM\n",
        "from transformers import pipeline\n",
        "from trl import SFTTrainer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
        "from transformers import pipeline, set_seed\n",
        "from transformers import TrainingArguments"
      ],
      "metadata": {
        "id": "ITbk7JtZOllK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fdf334-7f7f-44f3-c8a9-74556f1e819c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Handler"
      ],
      "metadata": {
        "id": "7tILRng55dIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all files in pdf folder to readable documents for HayStack..\n",
        "files = convert_files_to_docs(\n",
        "    dir_path='drive/MyDrive/training_data'\n",
        ")"
      ],
      "metadata": {
        "id": "gO_8E8M7RDcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess files\n",
        "preprocessor = PreProcessor(\n",
        "    # Experimenting...\n",
        "    clean_empty_lines=True,\n",
        "    clean_whitespace=True,\n",
        "    clean_header_footer=True,\n",
        "    split_by='word',\n",
        "    # Maximum Split Recommendation: 500 words\n",
        "    # We will use the recommendation\n",
        "    split_length=100,\n",
        "    split_respect_sentence_boundary=True,\n",
        "    # Positive Value --> Sliding Window Approach\n",
        "    split_overlap=3,\n",
        "    max_chars_check=100000,\n",
        "    # Additional Parameters\n",
        "    progress_bar=True\n",
        ")\n",
        "preprocessed_docs = preprocessor.process(files)"
      ],
      "metadata": {
        "id": "EJUDD3DqOtjk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea62ac64-4883-456d-f6bd-7fdf4e74e778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Preprocessing:   0%|          | 0/19 [00:00<?, ?docs/s]WARNING:haystack.nodes.preprocessor.preprocessor:We found one or more sentences whose word count is higher than the split length.\n",
            "Preprocessing: 100%|██████████| 19/19 [00:13<00:00,  1.38docs/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DocumentStore\n",
        "document_store = InMemoryDocumentStore()\n",
        "document_store.write_documents(preprocessed_docs)"
      ],
      "metadata": {
        "id": "FbXB4z9jPAO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retriever and Writer Preparation"
      ],
      "metadata": {
        "id": "R3NJTic88eJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retriever\n",
        "# retriever = BM25Retriever(document_store=document_store)\n",
        "retriever = EmbeddingRetriever(\n",
        "    document_store = document_store,\n",
        "    embedding_model='sentence-transformers/all-mpnet-base-v2',\n",
        "    model_format='sentence_transformers'\n",
        ")\n",
        "\n",
        "document_store.update_embeddings(retriever)"
      ],
      "metadata": {
        "id": "qO5c_E1SV94G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636,
          "referenced_widgets": [
            "260f645505644fbc9c751721b12a9bc7",
            "bd7fb640a5334379abb1655a71cc8f79",
            "06a2b5b4cd5049f3988dc0c3bdea0f93",
            "50e4c5e5f13140889e79c70a0862fded",
            "afcfbc6f9961446cb2a131addb021dcf",
            "b7d502358fe046a0873ef7959b36f758",
            "06e648be14d9467196eac4a1f176169e",
            "a8c5a1525a3841e7ad0e63d3d12f5445",
            "090c521e6dc042819438be61869a6fb2",
            "aa18fd18c912466c8caf2a79476dc8d2",
            "6670a6903648436b92b32edc9c861b9d",
            "6804fcf58abc4d22a23c105f4c2a5e0f",
            "870a39c21da543299b41fa05f102ef21",
            "29275baa096743debb908de5895fd860",
            "5a8d1eec033644a9aa7a1a19c7d0548f",
            "8453d5ffdef94342b08104b8c862e76d",
            "be5e2ffff9854aedbeea7b3bec7ca2eb",
            "fa07d459c75a47a9b4d056aadcf334d9",
            "9f642282974d47b88742b93df76a4914",
            "d6d419c396a44e6fb709713a81084f63",
            "c6290ff5c4c444ec8c3475518ecc1d64",
            "5bec9d6fb07c481f81cbd97cdaeb276a",
            "3552290c87cb4908bb2af2ba6e1ecf80",
            "cf0fa1c2af81429c981417bd17c9c1ca",
            "b0312f0aeace4f47be9d9302e469e1d9",
            "03c397dd509646e590ae8437134181e6",
            "011abc2499c140a3a29a5e5db9c2b64a",
            "70bd86abee1344c89ce89437e16e4fe5",
            "07b9568c2a7747ff96188cd1e82be8e6",
            "6292e7aba1d843cdb56f154057a31b44",
            "14d587e0e20c4099b18f1882b1a3f16f",
            "7ee147bce8854edd81b1726f6968eaa5",
            "0c340865bf1d4478addb083fff638887",
            "ad5401b795ff4d3395c98d9f9672cd23",
            "116affaf0e5143f4894c0500518101e5",
            "bba4addf6a18495999fcee1232086fc4",
            "59906401f3bc4d28a0373a0928024235",
            "b7eac8045a74424d95090db5fa54c605",
            "cdf8f7305c0f4f4090a420f6515c34b4",
            "2c81471aa2ff4cf0bbf88dc758fa0df3",
            "5e60ef57d40a4a2398399679b81b07a8",
            "a3a3c04e19654eb6980e9a14c612d3b7",
            "7ff3278236b4491dbd86b38250c57c55",
            "db629ec83dc74fdbbf5fc852354f646e",
            "e50eecf00e1b4d49af0eb50f0a5049e4",
            "238aec16c8c64f1496d6fb3935439c5e",
            "b128985c188649d394c48b60e5343ed5",
            "ff878f0306c2412a8bd4aacc35f0df4d",
            "89fd92303e264c26884981bf5ab71906",
            "adc536dda7ca4a689422ae7d512b7e4c",
            "065352fcf5d248fab6424590e24bccd1",
            "d8f808c611904be19d3911da3f3713b2",
            "3de8a201b18f43db9f6088d97e23eabd",
            "035ac00c9e274b57bc3a51f1b16481ed",
            "61db9d74814c47b191708b304c4f1395",
            "ede5265596034cb6bd7b60596217038f",
            "6f0379ab9aab4b3cad09895e47378c84",
            "b5b614f2ba5d4fa4af5d628164fb5c73",
            "65b56cf729224282a3039dec44704f56",
            "298ed699495c426d8e292d563d4141ff",
            "1a5f3755f7fd4c5caf301d959632de30",
            "4100cd7199c543d694aa53a7dd5609ac",
            "e4820a0c2ea74ec3ac898b8320b5270b",
            "9b8c0c2814b24206b615f8243019c044",
            "601ed2200a1949488a9832b36867596b",
            "3b8f23e163e24263a2e79521741d09e3",
            "e464378ab31a425c9e66e04bbcb98e9f",
            "b19c05d221ac423d8509d03c795b1cb6",
            "af4af82bb16f4d5595a2ce558b542425",
            "524d416a2dc84dc1b0cd6bc865a5a723",
            "a1e582cba8a045a4bd4d962dd2d001bd",
            "67f031087073444f8770beea58b59d3c",
            "a90a4344944d4a6a87b0e99442a7f33a",
            "9442994ba2d74821bb0660c9c37b96cf",
            "e06f7c94cc134db2a1eff09515caf0e6",
            "dc3f6698e72449c0bcf3c0b8c0d72f44",
            "a90a091545a54cbbbd5934f89ff7c005",
            "4d9b31d59d2e4ef9878e985d25db9e42",
            "d6095b64f3b740aba22cf02030eb2f5f",
            "84e9e8cfe4da480dba40a3ffb05eca18",
            "7d3bac7487874f7988d32035896fc48e",
            "e84e73db3ee646628ce1af0c2ab69bb6",
            "cc6b0fbb3bab4663b7e2bf8944b2e5ed",
            "0dd6fba41d7640bcae6e6fefd0c35a19",
            "6a5b6f1bd1674097b0fb7c87404557f3",
            "17da1bd25ad5494c85bae1ebef5ade07",
            "825a217b08ed4f96942f49d3d32fc7b2",
            "286d12903afe4f96b5c119d41ea828e4",
            "21fd10c86efd468294d568404b483400",
            "3c11db6a9cc04bc0a5ad069b70213619",
            "c0f57ccfd3a346e3828deb331868f127",
            "0c443e9409ae42dc973cc183b758a3f8",
            "f0d5343347cb4d7f98ac1719248d2759",
            "67e7d82cbdc44b0a8dafd0607479d9fb",
            "ac6a2ca60a9e484b8de2cda90c6ae86d",
            "02a95c866a94484188a4f9a65383a633",
            "134cff7fa79044e082609eafcb3fceb1",
            "960ca7cd50544a429741d8687600cd2a",
            "e82f532a253a4cc9af278efb1f0aafcc",
            "36233e0e77804f628c69c22ee310de0d",
            "1e4046e4eba84f55be08986c3a2bc64b",
            "a104a130fccf4a35bb1a3f4190fe4638",
            "8dd0b44f605040e3aebe56fd4133bfd7",
            "f0667d6dec904ec0bc2397802788f472",
            "b8289e193d31449cad79a3cb25990dd6",
            "0caf7890b3fd498c8279f38ac0544704",
            "08ad12e98c314f038468551403d33c4b",
            "c16c375d720844c8afe37b9692cc19db",
            "84722bbcdd6e44899551f6e89757849e",
            "cd363f0794774403b1d1c20c2b603d9f",
            "5411e838b20148a4bddca41c1b198db4",
            "2723713fcc0c469dac5874c1f83f1dd4",
            "d3b01260f5894fcfaa16df99ec3a2ef7",
            "6ec2c81d45cb40de945677049b3b7c0e",
            "361fe6fc8ef44c6b8affa859f1868647",
            "f62ed308b3d9494f8084fb8442ca39be",
            "73627ff745024773b8d089e979d4c3c3",
            "d610a85fd64f4e95a14cf6f0a4d49c36",
            "5c0d3b9347d9440a969f75fb62eb2207",
            "deb76ba622894c8cac9e76012f554882",
            "b4770f2a40bc4483b757c7f151eab911",
            "f181d001e8444e879ce5ac0eb70b6515",
            "b3f13981ee154bd7a70df46cda1a4b50",
            "1f71c21069af40b68023e2219e65ecb1",
            "64eb2c1edfbc40b9a7d359df8b772734",
            "b3c2b727b67b463fa26b13bbbe747e21",
            "fb350f3c817942748d8b7c048fa9c3cd",
            "20f85bad69ed4e8686f449aef942b9f6",
            "c7d0c7eaf1ea4786bffc652752cc8ef3",
            "62f34ef5fbf74130a44467aa4e737b4b",
            "e3081c21f73d42b1b7a872c1e60ed1e8",
            "a5eb031864524e7a9e459f727cfe2d3e",
            "fd618a5473a74492b5dfe6384f9acf78",
            "845e95a975324652be76fa81ee55d2cd",
            "25f9565d0fd34761a2480f47c1c3be91",
            "31e6d5e81ee24c9280d3a487f4cdab92",
            "0f0fd71d54b14227b9b9c3937451616a",
            "9e4ac32c6794433a96ae5c1f32edfc73",
            "a101b182623a41f882e3ca85cf60ebf7",
            "28aabb89466b4bf5868e038720f4af65",
            "341715aa866e4c51a05bb6d1e3dd3014",
            "0441fa99aef24e6980566f135b749fb9",
            "d40307e7104a43fc9e07cae107c4dd0f",
            "4b747c755efe4e61b2b339675e73e142",
            "5afef5985139445cadbb24654a40c3e1",
            "4b66ebaf49cf489fa83c1eaeee38e8b9",
            "8626054759934e12a4bfeef64f2fdd95",
            "514975224c044cb7b001ca3f1d3b13f0",
            "66a46818d18f468c9a3f47498a998812",
            "3281de727bb842cba94b60d7cee20dc6",
            "ce055c7cde384fd290b03e41783e3ec6",
            "bb5b63186e5e42769abb64a9f6a5ae16",
            "41922f21a79e4b9c811ab856c5e186aa",
            "c309c973321547b19e546d644ea81e62",
            "35ae1702551049029a73b9394ac11a67",
            "49b88ae647024f43845db70d4a2c99a2",
            "64d0254f4aaf4af592fe1fec817c2dd6",
            "8e110c68be9243b6aec796ee7175586e",
            "51771cc22059425aaba75dfa1c3909f4",
            "97ba3b7f02704c3ea9d42f07966540e7",
            "6e2f936ab50c42ba94d68646ba129c2a",
            "7261578bed914a6db061fc081aafa2fe",
            "dd6b8f8c597d4ad1898d6f845d18e17e",
            "b6ef984925b543118a0c3071df10546e",
            "a20b9653146244e69587f5f31348e512",
            "33c14c018f4a437b979245249435223b",
            "42cea9bb677b4961abb325a021f2407b",
            "9737a9fba40d45bb8738e2f95976042f",
            "ab10c7f148ba40c5a0c7c1e0d7f21ae4",
            "3089b97ee29c49d3b838e98a1582530a",
            "97e8901d80bd4e6e84b330237d42bbd7",
            "ed50d6767e654dee9d1149037a9c9e4a",
            "9a11ee7fa11c468bb1ae73fadcee8ffe",
            "33680885e0ff44afa0f00fb7158ded6f",
            "8d0ec7fd95e74167bdc1ec0bf9af1753",
            "78cf344c71cc424faff0cd92235de980"
          ]
        },
        "outputId": "a5de733e-b38f-4e17-b309-2045452ccb52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)851d5dd1af673670cdb299753/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "260f645505644fbc9c751721b12a9bc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)1af673670cdb299753/1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6804fcf58abc4d22a23c105f4c2a5e0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)6e48e851d5dd1af673670cdb299753/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3552290c87cb4908bb2af2ba6e1ecf80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)48e851d5dd1af673670cdb299753/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad5401b795ff4d3395c98d9f9672cd23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)299753/config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e50eecf00e1b4d49af0eb50f0a5049e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)1d5dd1af673670cdb299753/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ede5265596034cb6bd7b60596217038f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e464378ab31a425c9e66e04bbcb98e9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)73670cdb299753/sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d9b31d59d2e4ef9878e985d25db9e42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)f673670cdb299753/special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21fd10c86efd468294d568404b483400"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)851d5dd1af673670cdb299753/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36233e0e77804f628c69c22ee310de0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)1af673670cdb299753/tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5411e838b20148a4bddca41c1b198db4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)51d5dd1af673670cdb299753/train_script.py:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f181d001e8444e879ce5ac0eb70b6515"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)6e48e851d5dd1af673670cdb299753/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd618a5473a74492b5dfe6384f9acf78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)8e851d5dd1af673670cdb299753/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b747c755efe4e61b2b339675e73e142"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "Updating Embedding:   0%|          | 0/15108 [00:00<?, ? docs/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35ae1702551049029a73b9394ac11a67"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Documents Processed:  66%|██████▌   | 10000/15108 [02:16<01:09, 73.50 docs/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/160 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33c14c018f4a437b979245249435223b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Documents Processed: 20000 docs [03:23, 98.44 docs/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reader\n",
        "# reader = FARMReader('ahotrod/albert_xxlargev1_squad2_512', use_gpu=True)\n",
        "reader = FARMReader('deepset/roberta-base-squad2', use_gpu=True)"
      ],
      "metadata": {
        "id": "1hFqpkuaX_qg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209,
          "referenced_widgets": [
            "9e6d8c2c296c4d0c9d262528462ecb19",
            "848c1bd8c6cc45b496132d39a830bc0b",
            "74edb306327c46dea68caca5c9b67205",
            "388a00834f564969b0488696c7a5de85",
            "3bcb18347da1475f9360ffc9c89949c1",
            "8ca34969b09045ae961a1e765d9e10a7",
            "efad89b6ab824feaaee563e34e48cd85",
            "74ec0b0ae60c4d4c830705f2c9d308d7",
            "ad53ba2c3a374b8ab1af61e5317a66aa",
            "0d66eca3b37e4614830e42a68800f780",
            "81e811bfdfc941f19a1108a525e1b689",
            "5af50e1ff21c4cc1be8f54effdeddad5",
            "583ced1be4494111b0c107260d20669d",
            "8515eb63a83e479dafd6af73ad68c2bf",
            "6707cd1a018c43149371ae77f4bfd2d1",
            "e06620106e35460a84c313f2247a6d5c",
            "f03204999275446694e85becbe4289d3",
            "41526f88c79149d78b261a410c406991",
            "fceb30274a09465c800cbce9f0f11f13",
            "8a65555768e446b69b773f126d202c83",
            "6cd95ceaeaf04f9b9e90b215e6713792",
            "025975b5d0be43d9b84ce83bbb09c8fc",
            "861510e3c49646289d73647fef52038d",
            "ae855120120a40e19293dffca251b23f",
            "d58ac7623a3241429908485ebc54269a",
            "cf1c9f66464249edb9ff1ea58c045b35",
            "416ef9fb4c8641e7aa78ffbafc46a81c",
            "05396522bb61467798082c1651a4fe5e",
            "ae4e8089a4bf4800a4f97c6842963ff1",
            "c2f6277f9a434c4496efe58a8c34b390",
            "1d85c511a30d42668dc45b7c67e2193d",
            "4f97a1695cd5494cbdefa22496ea0fc5",
            "4992c3bea6ee4c6f879e740cac5722b6",
            "e7c1967746b04b46bf21d1d9ab13d576",
            "473945e2eae44eb1af6ac42313b6d843",
            "af185998c31c479abb3d8cd4268bde9b",
            "010aba6c097d427485691d0b6677e2f2",
            "71e755bdaf394b51903aced8e3b629c0",
            "35854491863641c5b16094e036b47408",
            "0b2d91788b4f4078b6d9a0b7600a7718",
            "00bf5ad9aebe47fd9c4f9c48bc7c1174",
            "f3a2f373973c4b688a04b54d6143c5d5",
            "8d5474d5e502494ea15e82f9425ff907",
            "4cb5b70f85ad425b9d548a40b3672c05",
            "159aeae01f85490fb8fea2a288fd4eeb",
            "7038a2f2af6e4e19ad2128541f23f712",
            "823c9e56493c429c8fb20a21c546927e",
            "f24525528c514051ad0f54160aeace51",
            "8a96cbadc4df43259ef8cae8ab4e55e3",
            "5f257a4399b548f489dbd5c043680e9a",
            "817e725058184deeb98f5814d17d9e73",
            "c67528430ff7477eaea8777bf798fe38",
            "b3742e015aae4ebbb353e292a78b79fc",
            "129cc1d802844a95aead3af09846970b",
            "864c93fbc72443f6ac13aeec4b73e684",
            "b15658490562436e87d0b3544483473d",
            "6bef671f205f4a2fa6359f6e9cff4775",
            "f00ef8acfb6049949891fd179435a2a5",
            "217b494a20bf42dc97ba7d43761a6adb",
            "a56640091ffa4ca286ea8f396df11511",
            "9333740368ac46c5947a04865fc52d5b",
            "2ab92deb9e8042b0b6cbc44ed8bc9509",
            "6319312dec6a429fa02b81be7504f048",
            "3eea7f32fe164100a7f61897da28b430",
            "82d604a0d5e546e6a9f557d85740861b",
            "894379684e6e45b292a059aed5bb0aa7"
          ]
        },
        "outputId": "a60dc7d1-ba1e-4dae-b865-b061c95a6379"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)rta-base-squad2/resolve/main/config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e6d8c2c296c4d0c9d262528462ecb19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5af50e1ff21c4cc1be8f54effdeddad5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)quad2/resolve/main/tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "861510e3c49646289d73647fef52038d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)erta-base-squad2/resolve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7c1967746b04b46bf21d1d9ab13d576"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)erta-base-squad2/resolve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "159aeae01f85490fb8fea2a288fd4eeb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)ad2/resolve/main/special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b15658490562436e87d0b3544483473d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction Generation"
      ],
      "metadata": {
        "id": "ZItOn08X8lg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rr_pipeline = ExtractiveQAPipeline(reader, retriever)"
      ],
      "metadata": {
        "id": "nCcXjHHDaPj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feed_in = 50 # @param {type:\"integer\"}\n",
        "feed_out = 10 # @param {type:\"integer\"}\n",
        "prediction = rr_pipeline.run(\n",
        "    query=\"What parameters should I tune to increase writing speed performance on a PostgreSQL database system?\" # @param {type:\"string\"}\n",
        "    ,\n",
        "    params={\n",
        "        # 10 is the optimal number per the documentation\n",
        "        'Retriever': {'top_k': feed_in\n",
        "\n",
        "                      },\n",
        "        'Reader': {'top_k': feed_out\n",
        "\n",
        "                   }\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "fd1753f0bbc944f38eff0931f89f3c64",
            "48aa5e0154db47beb7f8ee878fd2f670",
            "ef4ff9a5a1014e919f42e05437a7e322",
            "866c55b898f64a588ca141ffa7dc5181",
            "6b396b461ed44e49bd90ed64c612178b",
            "486dd1bda8844b779b8e7b593eac174a",
            "48dd101822b54966b87005cce1658997",
            "8743d4142e5e45f5bcd7e94fef1f83df",
            "5db3eb62bdea467d8a8897bb7e155917",
            "f089d7dc363a47dcabd7170cb8d2db94",
            "886f3b092ca040558ae08593eca4ebd4"
          ]
        },
        "id": "rHwCH3q5bOnv",
        "outputId": "5eb05488-8444-4f63-9e53-a888bc7f9ddf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd1753f0bbc944f38eff0931f89f3c64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Inferencing Samples: 100%|██████████| 5/5 [00:03<00:00,  1.65 Batches/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_answers(prediction, details='maximum')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRrlZ2Jje5hf",
        "outputId": "d7722537-03b1-4682-dbbb-8863508a5b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:print_answers received details='maximum', which was not understood. \n",
            "WARNING:root:Valid values are minimum, medium and 'all'. Using 'all'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Query: What parameters should I tune to increase writing speed performance '\n",
            " 'on a PostgreSQL database system?')\n",
            "'Answers:'\n",
            "[   <Answer {'answer': 'wal_buffers', 'type': 'extractive', 'score': 0.8283660411834717, 'context': 'ducing connection overhead.\"},\\n{\\n\"Question\": \"What is the role of the wal_buffers parameter in PostgreSQL, and how can it be configured to optimize wr', 'offsets_in_document': [{'start': 340, 'end': 351}], 'offsets_in_context': [{'start': 70, 'end': 81}], 'document_ids': ['2652c99e832a8b2b4a601f14472c7eef'], 'meta': {'name': 'qa_data.txt', '_split_id': 93, '_split_overlap': [{'doc_id': '2d2d44cb7e0c648a7cfa7e1e24d74db3', 'range': (0, 297)}, {'doc_id': '72a7cde91716744884265d08b3fb7ba8', 'range': (556, 620)}]}}>,\n",
            "    <Answer {'answer': 'synchronous_commit', 'type': 'extractive', 'score': 0.7727646827697754, 'context': 'scale_factor = 0.05).\"},\\n{\\n\"Question\": \"How can you configure the synchronous_commit parameter to optimize write performance in PostgreSQL\",\\n\"Answer\":', 'offsets_in_document': [{'start': 87, 'end': 105}], 'offsets_in_context': [{'start': 66, 'end': 84}], 'document_ids': ['66eff9347cdc750248b845998d09356d'], 'meta': {'name': 'qa_data.txt', '_split_id': 23, '_split_overlap': [{'doc_id': '2144ffb09e464be7af176c79a35dff73', 'range': (0, 42)}, {'doc_id': 'fdfc1f6647e185ba2acb5099fdcfb0d', 'range': (541, 618)}]}}>,\n",
            "    <Answer {'answer': 'fsync=off', 'type': 'extractive', 'score': 0.7476253509521484, 'context': ' versions of PostgreSQL, you may find people recommending that you set fsync=off to speed up\\nwrites on busy systems. This is dangerous--a power loss c', 'offsets_in_document': [{'start': 83, 'end': 92}], 'offsets_in_context': [{'start': 71, 'end': 80}], 'document_ids': ['e22d1d17241c9179c97288e544a7ab59'], 'meta': {'name': 'wiki_tuning.pdf', '_split_id': 50, '_split_overlap': [{'doc_id': 'e3a8fe7132a8828a0f273f6fd01a07e6', 'range': (0, 128)}, {'doc_id': '94680b0d2755b7cdb9913f0b954f2edb', 'range': (349, 636)}]}}>,\n",
            "    <Answer {'answer': 'commit_delay', 'type': 'extractive', 'score': 0.6910375356674194, 'context': 'a single 8kB write operation is often the most effective setting for commit_delay, so this\\nvalue is recommended as the starting point to use when opti', 'offsets_in_document': [{'start': 272, 'end': 284}], 'offsets_in_context': [{'start': 69, 'end': 81}], 'document_ids': ['da4665f4b866a18f6d76a0ead07acac1'], 'meta': {'name': 'postgresql-16-A4.pdf', '_split_id': 5425, '_split_overlap': [{'doc_id': 'd4b28ca75cdef25bce08ff59304c955', 'range': (0, 122)}, {'doc_id': 'e993b30e92b48a7fe9e15d92802828fe', 'range': (123, 386)}]}}>,\n",
            "    <Answer {'answer': 'enable_indexonlyscan', 'type': 'extractive', 'score': 0.6289160251617432, 'context': 'ck_functions = \\'read\\').\"},\\n{\\n\"Question\": \"How can you adjust the enable_indexonlyscan parameter to optimize read performance through index-only scans ', 'offsets_in_document': [{'start': 70, 'end': 90}], 'offsets_in_context': [{'start': 65, 'end': 85}], 'document_ids': ['c0a9bccc3d9bdbb8064ab14178151d9f'], 'meta': {'name': 'qa_data.txt', '_split_id': 135, '_split_overlap': [{'doc_id': '2c4f2b2f7900949292350ed1973e240c', 'range': (0, 28)}, {'doc_id': 'abfab71131137244f563c2f55691f50c', 'range': (420, 609)}]}}>,\n",
            "    <Answer {'answer': 'synchronous_commit', 'type': 'extractive', 'score': 0.5959617495536804, 'context': ' 4GB for 4GB of RAM.\"},\\n{\\n\"Question\": \"Explain the purpose of the synchronous_commit parameter, and how can it be configured to improve write performa', 'offsets_in_document': [{'start': 84, 'end': 102}], 'offsets_in_context': [{'start': 66, 'end': 84}], 'document_ids': ['ebe69a7c6d435f596bb3760c4007bc0c'], 'meta': {'name': 'qa_data.txt', '_split_id': 89, '_split_overlap': [{'doc_id': 'f8e7075d2c4f976bae54d30f4829a15c', 'range': (0, 38)}, {'doc_id': '41cd013958b23ad16a2893edc626378c', 'range': (353, 589)}]}}>,\n",
            "    <Answer {'answer': 'work_mem and random_page_cost', 'type': 'extractive', 'score': 0.594178318977356, 'context': 'rameters related to query planning and optimization, such as work_mem and random_page_cost, based on your workload. Partitioning: If you have large ta', 'offsets_in_document': [{'start': 504, 'end': 533}], 'offsets_in_context': [{'start': 61, 'end': 90}], 'document_ids': ['f2eacdc89a69c9de884776da9c20faaa'], 'meta': {'name': 'test1-quora-posts.txt', '_split_id': 1, '_split_overlap': [{'doc_id': '8915819dcc833d2bb29ea8a9c3b200d2', 'range': (0, 75)}, {'doc_id': '2a4d264649c4e3a1e7009ab0fae53d0e', 'range': (657, 674)}]}}>,\n",
            "    <Answer {'answer': 'how much RAM', 'type': 'extractive', 'score': 0.5616673231124878, 'context': 'ytical database so you want to set up a higher work map depending on how much RAM you have so that your performance is better overall random page cost', 'offsets_in_document': [{'start': 20704, 'end': 20716}], 'offsets_in_context': [{'start': 69, 'end': 81}], 'document_ids': ['c18a72004f420f1e38412e338f37b800'], 'meta': {'name': 'svaB2h8LNww-postgres-youtube.txt', '_split_id': 0, '_split_overlap': []}}>,\n",
            "    <Answer {'answer': 'shared_buffers', 'type': 'extractive', 'score': 0.54957515001297, 'context': 'ng specific parameters and settings:\"\\n},\\n{\\n\"Question\": \"What is the shared_buffers parameter, and how can it be configured to increase write performan', 'offsets_in_document': [{'start': 332, 'end': 346}], 'offsets_in_context': [{'start': 68, 'end': 82}], 'document_ids': ['f8e7075d2c4f976bae54d30f4829a15c'], 'meta': {'name': 'qa_data.txt', '_split_id': 88, '_split_overlap': [{'doc_id': 'bacb2f0359e04c7104ce0b25182bfa92', 'range': (0, 116)}, {'doc_id': 'ebe69a7c6d435f596bb3760c4007bc0c', 'range': (592, 630)}]}}>,\n",
            "    <Answer {'answer': 'shared buffers', 'type': 'extractive', 'score': 0.5482164621353149, 'context': \"at's closest to it we go in then I can think of yeah so going on to shared buffers probably one of the most important things and one of the main thing\", 'offsets_in_document': [{'start': 6549, 'end': 6563}], 'offsets_in_context': [{'start': 68, 'end': 82}], 'document_ids': ['c18a72004f420f1e38412e338f37b800'], 'meta': {'name': 'svaB2h8LNww-postgres-youtube.txt', '_split_id': 0, '_split_overlap': []}}>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# YouTube Transcript Extraction for Training"
      ],
      "metadata": {
        "id": "T5GxrmRo89Nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve Transcript from Video URL\n",
        "\n",
        "# youtube_links = [\n",
        "#     'https://www.youtube.com/watch?v=dl98eDu_Ssg',\n",
        "#     'https://www.youtube.com/watch?v=YON9PliOYFk',\n",
        "#     'https://www.youtube.com/watch?v=HE0KCSgIYeY',\n",
        "#     'https://www.youtube.com/watch?v=sJN5UwC60bM',\n",
        "#     'https://www.youtube.com/watch?v=2OJazrXmKXA',\n",
        "#     'https://www.youtube.com/watch?v=xrMbzHdPLKM',\n",
        "#     'https://www.youtube.com/watch?v=xg9CruvOZ1A',\n",
        "#     'https://www.youtube.com/watch?v=clrtT_4WBAw',\n",
        "#     'https://www.youtube.com/watch?v=5M2FFbVeLSs',\n",
        "#     'https://www.youtube.com/watch?v=GA8SaXDLdsY'\n",
        "#     ]\n",
        "\n",
        "youtube_links=[\n",
        "    'https://www.youtube.com/watch?v=dl98eDu_Ssg' # Webinar: Tuning Tips to Maximize Postgres Performance\n",
        "    ,\n",
        "    'https://www.youtube.com/watch?v=xrMbzHdPLKM' # Tuning PostgreSQL for High Write Workloads\n",
        "    ,\n",
        "    'https://www.youtube.com/watch?v=xg9CruvOZ1A&pp=ygUsdHVuaW5nIGEgcG9zdGdyZXNxbCBkYXRhYmFzZSBmb3IgcGVyZm9ybWFuY2U%3D' # PostgreSQL performance tips you have never seen before | Citus Con: An Event for Postgres 2023\n",
        "    ,\n",
        "    'https://www.youtube.com/watch?v=SnrkMYbrS3E&list=PLfflPaWRUdGW20CFW4pVUrdnK4TYL0eRK' # Tips & Tricks on Tuning Slow running SQLs in PostgreSQL\n",
        "    ,\n",
        "    'https://www.youtube.com/watch?v=kWAPxVg9g3A' # Mostly Mistaken and Ignored PostgreSQL Parameters while Optimizing a PostgreSQL Database\n",
        "    ,\n",
        "    'https://www.youtube.com/watch?v=svaB2h8LNww' # NYCPUG - Essential PostgreSQL Performance Tuning - Payal Singh\n",
        "    ,\n",
        "    'https://www.youtube.com/watch?v=wk3dnP7FKq0' # How to Scale Postgres: Automation, Tuning & Sharding\n",
        "    ,\n",
        "    'https://www.youtube.com/watch?v=5M2FFbVeLSs' # PostgreSQL performance in 5 minutes\n",
        "    ,\n",
        "    'https://www.youtube.com/watch?v=hqbTTCKSAx0' # 5mins of Postgres E41: Tuning shared_buffers for OLTP and data warehouse workloads\n",
        "    ,\n",
        "    'https://www.youtube.com/watch?v=uWjDx7kacPk' # 5mins of Postgres E57: The new pg_stat_io view in Postgres 16 and the everyday DBA perspective\n",
        "    ,\n",
        "    'https://www.youtube.com/watch?v=v-KzKa0g7js' # Optimizing bulk loads in Postgres, and how COPY helps with cache performance\n",
        "]\n",
        "\n",
        "for video_id in youtube_links:\n",
        "  transcript = ''\n",
        "  for entry in YouTubeTranscriptApi.get_transcript(video_id[32:]):\n",
        "    transcript += entry['text'] + ' '\n",
        "  print(transcript)\n",
        "\n",
        "  dir_path='drive/MyDrive/training_data'\n",
        "  # Write Transcript to Text File\n",
        "  file = open(f'{dir_path}/{video_id[32:]}-postgres-youtube.txt', 'w+')\n",
        "  file.write(transcript)\n",
        "  file.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsIxaWIl9XW9",
        "outputId": "03165f50-7fc2-4ded-88ba-05fbe8880d08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "good morning good afternoon and good evening my name is ina parisio i am a marketing manager here at edb and i will be your host today for a webinar tuning tips to maximize postgres performance i am joined today by dave page who is the vp and chief architect database infrastructure and devrem gundus who is a consultant at edb before i turn it over to dave and devrim i am going to go over a few housekeeping items um number one this presentation is being recorded so slides and recording will be shared after the webinar and then the lines are currently muted so if you have any questions just drop it in the question and answer panel and uh we'll make sure to go over it if we don't have time at the end of the presentation we'll be following up with you offline and with that said i will hand it over to dave and devrim to kick us off thanks eena uh good afternoon good morning good evening everyone um i hope everyone is doing well my name's dave page um [Music] we have the next slide please so uh in the uh the other window there you'll you'll see my colleague deborah windows with the the headset on um so we'll be presenting today on some high-level tuning tips for postgres looking from the hardware level right through to queries a little bit about schema um specifically we'll have a quick slide just to explain who edb are uh we'll look at designing the hardware um [Music] we'll look at tuning the operating system and tuning postgres and then some tips on tuning queries and partitioning um please note as i said this is this is pretty high level we'll give a link at the end to a paper that one of our colleagues has written vic fearing along with help from devram and myself which goes into this in in a lot more detail so the aim of the session today is just to give you some some some pointers on where to start looking and what to start thinking about and then with the aim of the the the paper is linked to the end you can dive right into your own systems and start looking in more depth uh next slide please don't so who is who are edb i'm i'm never quite sure which is right um so edb's been around since um 2004 um came out of stealth mode in 2006 i joined on the 2nd of january 2007 so i've been here quite a long time now i think devram joined two or three years after i did so he's he's also been here quite a while now we're the largest dedicated postgres company in the world our development team are responsible for various contributions to postgres you can see some of those on the timeline at the bottom there 2007 second quadrant was uh launched and they they became part of edb in 2021 and that leaves us now with i think more major postgres contributors than any other company in the world um so that that's edb in a nutshell um next slide please debra uh thanks dave so um we will first start with designing the hardware um and i'll start with the bare metal option so in by while designing the machines um selecting the right cpu is quite important because the right cpu probably is going to have you give you much better performance so um i want to start with the level three caches first because um the level three cache is quite maybe quite important for postgres um so there are at least two and quite often three caches in the cpu cpu cores um level one and level two are specific i mean tight uh to the cpu but level three cache unlike the others uh are are shared between the cpus so when you have more level three cache you can have more data that can be shared between the cpus and the more level three cache may help you it's gonna not may help you that will help you actually process more data in a given time so um these three caches having a larger three cache is maybe quite important for postgres and of course the next item are disks the most important one of course cp and then the next one is uh disks um so which disk should i choose this is the should i just this is one of one of the most asked questions frequently asked questions to us it actually depends on the application because some ssc's for example are read intensive some of them are much better with writes or some of them are mixed loads so based on your load if you're writing heavily make sure that the drives is optimized for for writing so you should just make make some you should just search and find the best best disk for you and of course you should always use rates and we suggest you to use raid one at least for the transaction logs but usually you can se you can use rate 10 if you need more your much more disk space and of course rate 10 for data it's both for the backup purposes and consistency purposes and of course for the performance reasons and from the on the disks we have the third option the third thing that we should consider is using table spaces um you know from time to time uh you may you are going your drives are going to get slow or you are going to get some new disks and the old one will be slower than new ones so you may want to use those table spaces to store the archive data so when you are when you are doing uh data aging along with partitioning which you are going to describe towards the end of the um webinar with as they mentioned so you may want to push those data to the slower drives and use the new drives for the actual data and the third item is ram um when someone asks us uh about which component should they buy more uh the answer is always ram and ram and ram then you can go go with others because that's the cheapest component uh in the in in all of these three items i mean in terms of uh cpu disk and ram the tram is the cheapest one so go as big as you can the reason of this and in the beginning if you can purchase more ram you will probably you will probably won't pay more in the future in the near future because when you want to add more ram it will be it will be probably expensive uh later on so go as big as you can the more ram means more cash more cash means less io means you you you don't have to rely on this that much of course you will have to eventually the data are going the data is going to be written on the drives but still if you catch more uh we will end up with having more maybe handing more connections and doing more transactions and etc uh the another option could be using hotspot for a hotswap ram which avoids downtime for upgrades and replacement so you don't have to put the server down for to use hot swept ram but the only downside is it's expensive because you have to double the ram that you have that you have on the machine and the fourth and the final part in the hardware design for the bare metal ones is network it may sound irrelevant but a faster network means faster data transfer also faster replication so um just make sure that you have enough network speed between the databases and of course between the application servers and the database yeah that one's always interesting um people sometimes forget to allow for things like backups and then and then realize that when you're running a backup the the network speed becomes a bottleneck um so if you can run a 10 gig network interface for example and you're going to be doing large backups that's definitely well worth thinking about um so most people these days are probably actually deploying on virtual machines not on bare metal so whilst it's it's interesting to talk about um choosing the exact right cpu and choosing different types of disks and so on probably more relevant nowadays is looking at virtualization now many of the same principles will apply you know faster disk is going to make it easier to do backups although in virtualized environment there are other ways of doing those but there are other considerations that we need to make as well with virtual machines first of those is power virtualization versus full virtualization if you're running in the cloud you're not likely to come into this at all and there's a reasonable chance that you won't see it if you're running in a pure vmware type environment either but if you are using unusual virtualization software um qemu for example um make sure you're using parallel para virtualization what this does is basically offloads things like disk and network to the host machine so you're you're using special drivers that essentially bypass virtualizing those devices and connect you directly with the physical devices on the host machine what you're more likely to see in in real life is a need for dedicated hardware one of the problems you'll we may well run into in the cloud or on a vmware system for example is what we call noisy neighbors and that is where your database instance is hosted on a machine that has other applications in other virtual machines on the same physical box you don't always know what those are doing particularly when you're in a cloud environment a lot of the cloud environments however offer you the option to use dedicated hardware so you know that only your stuff is running on that physical machine you might still run into the noisy neighbour problem with disk access if you're using ebs volumes for block devices for example and also for network access but at least as far as cpu and ram is concerned you you know what else is running on that machine because that whole physical machine is allocated to you so you can plan accordingly when you're choosing a vm or setting a vm up look very carefully at the instance types that are available to you the number of cause and the amount of ram are are the obvious ones of the obvious things that people look at um but don't forget to look at things like the network throughput um some of the different machine types on amazon for example actually have relatively slow network compared to others so it's worth just browsing through what's available and making sure you've got the best balance for the number of cores the amount of ram and the network throughput now finally our on here if you're running on certain types of system uh for example ibm power springs to mind you might want to consider pneuma pinning so this is where you're you're running virtual machines on on one box uh and that box has multiple what are known as numa nodes so consider them different partitions if you like of cpu and memory within the system now normally you don't have to worry about that but what what that means is if a process gets swapped from one pneuma node to another node it's it's essentially transparent to you but it comes at a pretty severe performance cost when we're running virtual machines on a on a system that supports it what we can do is pin those vms to specific cpu cores and that helps prevent that performance loss if the process gets swapped across to a different newman node again you're not likely to see this in the cloud but you might see it if you're running on more specialized hardware such as the ibm power platform and next slide please never thank you uh disks are the other thing that can be slightly different in the virtual environment if you're running on vmware then typically you'll have the option of pre-allocating the the virtual disk space um so if you create a new virtual machine even if you just do it on your your laptop in vmware fusion or vmware workstation you by default the disks there will be they will expand as you use up the space so it will create a virtual distance very very small and then automatically expand it as you as you need to what that effectively means though is that that virtual disk can become fragmented on the system that it's running on when you're building a database system where you're looking for performance make sure you pre-allocate the disk so if you if you allocate an 80 gig drive for your database for example what that pre-allocation means is it will use 80 gigs of space from day one so it will pre-allocate all that space on the physical drives that can then avoid fragmentation it may not avoid it all because it might still have to fragment when it creates it um but it's say it it can minimize it and save on any performance issues if it has to allocate more space during operation now i've never mentioned raid earlier on we've looked at using raid on on amazon so one of the common scenarios that people have suggested to us in the past is that you you mount multiple block devices on your virtual machine and then use linux md raid to strike the data across those block devices we've done some fairly extensive performance testing and actually found that that doesn't make any noticeable performance difference um we believe it's because you're you're using remote block devices so you've even if you allocate three or four of them you've still got the same bandwidth to talk to to the system on which those block devices live that said in other virtual environments there might be benefits from uh from having multiple uh block devices and then and then using strike to mirrored rate or rate 10 over the top of them it really depends on how your your virtual machines in your network are architected if using two devices means you can make use of multiple interface cards rather than just one for example or if those devices get allocated on different sans or whatever it it really does depend on how your system is set up so it's worth looking into and trying to figure out if you can gain benefits from splitting that data and striking across multiple devices the other thing to watch out for in the cloud is when you provision storage by default typically there's no guaranteed performance level you can pay more money and have dedicated or provisioned iops and what that means is that you're then guaranteed a certain number of operations per second on those storage devices it can get pretty expensive so you know figure out what you need and then set that up don't just choose the highest available option because that will end up costing you a lot of money next slide please deborah so thank you dave for the great information uh about virtual machines and postgres so um so we tuned the hardware now we are going to in operating system as a part of the tuning process i'm going to start with tuned daemon which is the which is called additive system tuning daemon dynamic actually adaptive system tuning daemon it is the reddit's default unique mechanism it's also optional for debian and ubuntu so you can use uh tuned on every major uh on every major operating system so um usually actually land uses quite often anaconda that installer picks up a good default so if you're running a virtual machine it's gonna be virtual machines if you are using um a regular one it's gonna be the regular stuff and then however the default even though the default is great uh for the general purpose uh we need to do some manual configuration for for postgres so i'm going to give you show you a quick demo about how to um do this so actually in the document that we will give you at the end we will show you how to do this but by if we just create a directory say called adb postgres you can give any name in here and we create a tuned conf file so basically this is what we suggest you to do of course you can always have more but these are the these are the sections and in here and you just have between the server for postgres so um by default when i say tuned adm list you can see all the profiles and actually you can also see the actor profile which is which is virtual guess in here so um we've we always want to disable transparent huge pages for for uh for postgres and also for other major databases as well and we also want the performance governor because we don't want to see we don't want to save power which is going to slow down the cpus this is quite important actually because when you say when you select power save it means um it's going to slow down the cpus and when you want to use some more cpu power the operating system has to speed up the cpu first so we are going to lose lose time while performing declare analyze running the queries and then um so this is how we actually should tune the cpu uh for postgres same thing applies for some operating system parameters if you are familiar with linux in the past this we used to do this via cctl which is still available but now tuned is the actual way of doing this and finally we always disable the transparent huge pages so in order to in order to in order to enable this new profile we say tune the adm profile edb postgres okay so now then you say of course you can do more but now the actual profile is edb postgres this is how we change and then of course of course we will need to disable huge pages in grub in the kernel and we will have to write it to grub and we have to reboot the machine which i'm not going to do right now so this eventually requires a reboot for the all of the settings to change effects but this is actually how you make make some changes um for for tweening postgres um uh tuning i'm sorry tuning operating system for postgres so we want to have the kernel also do some some cleaning as well swept less we don't want swipe to run so stuff like this and the second thing after tuned is huge pages huge pages are quite important for for applications like databases because huge pages actually allow us to allocate much larger blocks of memory by default the operating system page is four kilobytes so uh at the end you may end up with some fragmentation in memory which probably you wouldn't want for for to get a better performance from here so as the data graph grows more postgres is going to catch more gigabytes of data in ram so with the with the page size uh it may be impossible not immense and maybe not impossible it will be hard to keep up with the performance so instead of doing this we should use huge pages the only thing is huge pages are disabled disabled by default and requires a restart of postgres postmaster in this case so i want to show you how to find the how to find the how to find the requirement for huge pages and eventually how to set it in here so uh let's go back to the um our mission again this is running advanced server 13.4 which is the latest version as of now first we find the process id of postgres i mean postmaster the um the um the main demon so now let's find frog and status in here and we find the number in here so let's let's keep it for now so i would like to change shared buffers on this machine first just to make sure that we have some better number so let's find the new process id okay now as you can see the number is increased slightly in here so now we have to make some calculations this is basically echo divided by 2 kilobytes by pc so here we have this number in here awesome so um now let's go back to the tuned daemon sets vm in our huge pages to this number it is just fun i actually suggest you to use it slightly a bit more like this right like this and oops sorry of course i need to do it root so i'm now applying those changes for us okay now we can again um connect to the database server and alter system set huge pages to on and as i told you we need to restart the database server first so if it fails it means there is a problem with the number that we wrote in this between file i mean maybe some calculation errors and et cetera no worries about that one just just makes setting a change again now postgres can use huge pages that's all and now we can we can postgres can benefit from this huge pages feature as well this was easy we just find found the memory requirements made some calculations and changes and the the others the other step about the about postgres performance from the operating system point of view is optimizing file system um in order to get more from the file system please uh set please use no access time for the while mounting the file system because postgresql doesn't rely on file access time it's going to shave a few cpu cycles it's not a big win but every everything actually is going to give you some minor performance improvements eventually you will get the total great improvement from the from the operating system finally from the operating system point of view we have the file system type selection so there are several options available uh in the file systems and xfs is the most popular and also default on major operating systems and of course you can use always use exe4 if you are using the older versions of the data or of the operating systems but nowadays xfs is a more is a more popular than one and it performs actually great for postgres the only thing is please do not turn off journaling because it's gonna it's gonna probably make help you lose data help you lose the data so some people ask about buttrefs it's not quite there yet it's promising but probably it needs some more a few years to be able to keep up with the performance of xfs dave okay so let's um let's talk a little bit about um tuning um [Music] parameters in postgres um there we go there's the right slide um so out of the box postgres is designed to run anywhere um raspberry pi is tos or point of sales machines our build farm in fact at one point even had a playstation 2 running the regression tests um now that's great because it means that everybody gets a reasonably good experience when they first start postgres because it usually doesn't fail to start but it does mean that you need to do some tuning to get the optimum performance out of whatever hardware you have there are some parameters that should always be changed um and what we're going to do over the next few slides is just go through the parameters you should go look at now i mentioned earlier the document will share the link to the the end which goes into this a lot more detail it goes through a lot of these settings uh and tells you how to actually go about calculating default values um we don't have time to go through all of that for for all of these right now so that's a an after-class homework assignment i guess um similarly the the demos that everyone just gave i'm using tunde for example they're all documented in the paper as well um so you don't need to remember everything that we've said and shown on the slides right here um so i'll go through the various uh settings um sorry the room you can go to the next one you may hear us call these gucks so these are postgres settings that are stored in the postgresql.conf file or a file that's included in it they're also what tends to be um what gets set if you do an alter system command in postgres so that that's basically writing one of these conflict values um as i say they're they're often called ducks which actually stands for grand unified config it's a sort of post-crest development term but if you ever hear that term then it's one of these settings that's being spoken about first one to think about is max connections when you when postgres first starts max connections defines obviously the the number of connections that uh that could be made to the database at any one time but it's also used to size various internal data structures in particular one called the proc array now the procreate there are various things that happen when postgres is running that require the procreate to be examined that as you can imagine can take a lot of cpu cycles it can cause lightweight blocks and so on to be um required so minimize the number of rook mac or the value you set for max connections if you're only ever going to have 10 people connected to the database don't set it to 100. now in an ideal world max connections would match the number of cpu calls you have unless you've got a really big server that's probably not going to be the case in reality um so we'll often see sort of two to one or four to one ratios or whatever but if you can it's nice to match it to the number of cpu cores now one thing that's definitely worth remembering is if you have lots of connections like hundreds or more or even thousands use a polar don't try and get that many connections to be handled by postgres directly um it's not going to play out well from a performance perspective if you if you have max connections set to 5000 or something like that for example you will almost certainly benefit from using a puller such as pg paw or pg bouncer pg bouncer is the lighter one if you're if you're just doing very simple pooling next slide please okay shared buffers is probably the second one that everybody changes so this is the the main cache if you like the postgres uses so data when it's read from disk will be stored in the shared buffers updates will be made directly into the stored buffers and then get written out to disk at a later time typically this depends on ram common to see sort of 30 to 50 of the available ram you really want to get don't want to go any higher than 50 work mem is a the size of a block of memory that will be allocated for use for sort and hash table operations when queries are executing so this might be set to 16 megabytes for example it's worth remembering that there are multiple sort operations or hash table operations and similar similar things going on in any kind of complex query so work mem might actually be allocated multiple times so be really careful it's kind of tempting just to say i'll make that a gigabyte and that that'll make sure that my salt's never spilled to disk but if you've got a query that's going to do eight different sorts because you're joining eight tables and and so on you know you might have just accidentally allocated eight gigs around um so be careful with that um you can there are logging options that will show later on that will help you figure out if you need to increase workman uh but just be careful not to overdo it maintenance work name is used for maintenance operations like vacuum indexing and so on re-indexing is dependent on the available ram but typically users will set so one to four gigabytes or so auto vacuum workmen is similar um we've if we lost the slides debra and you're you also seem to be muted we're having a minor technical problem okay is it fixed now uh i can't see the slides anymore can you know yep uh just go into present mode oh i did but i'm i'm seeing them in edit mode on slide 26. um so there was a failure here um can i now see it yes okay sorry slide 20 please yeah there we go sorry about that um yeah so auto vacuum work man is very similar to maintenance work men but it's it's respected by the auto vacuum demon whereas maintenance workmen is for interactive commands such as an explicit vacuum you set that one to minus one it will use the same value as maintenance workmen effective i o concurrency was originally added back in the days of spinning disks or spinning rust as we sometimes call it um if you had a raid array which was striped across four disks for example the intention was you'd set this to four which would tell postpress that it could uh effectively perform it for io operations at once that could be expected to be executed in parallel nowadays with modern ssds and devran's mentioned ssds a few times but you know what we're talking about here also applies to nvme drives um we would normally actually set this to a few hundred and that's because ssds and nvmes can handle lots of operations in parallel next like this there are various settings to look at regarding wow that is the right ahead log or transaction log that postgres maintains during normal operation um while compression will almost always set to on this helps us to reduce the amount of io by compressing the wow before it's written to disk it costs a little bit of cpu but normally the the cost of the cpu is well worth sacrificing to reduce the amount of disk writing that we're doing um wow log hints so a little bit complex but in postgres within the data pages that it stores on disk has the what we call hint bits which help give hints to postgres about the visibility of um different tuples within the database or different rows within the database um logging modes to or in the wow is actually very useful for utilities like pg rewind so that one whilst it's not so much a performance issue it's it's certainly useful when you're doing um backups and restores and so on um well buffers so this is the the amount of shared memory that's allocated for unwritten well data before it gets written down to the disk wow is written in 16 megabyte files we would typically recommend 64 megs of wow buffers so enough for four wow files and uh the last one on this slide checkpoint completion target so postgres is going to write it's going to checkpoint the data periodically it's going to do that either when a certain amount of data has been written or it's going to do it on a schedule assuming that it's running on a schedule we want to spread out how long that checkpoint takes to complete so that the we minimize the spike of activity um so what we actually aim to do as of v14 this is the default is set checkpoint completion target to 0.9 so we're effectively using 90 of the time in between checkpoints to write each checkpoint so we're spreading that load right across the the the period between the checkpoints um as i say that's now the default as of v14 but 0.9 is a good value to use it on older versions of postgres as well checkpoint timeout is the the maximum amount of time between checkpoints longer timeouts as it says maybe may end up with long recovery times lower values may end up with more io so this is very very dependent upon your workload [Music] now it says maximum time between checkpoints because maxwell size the next parameter if we exceed that then the checkpoint may happen regardless of whether we've hit the timeout so what this is saying is create a checkpoint every time x megabyte of while has been written typically what we want to try and do is just let the checkpoint timeout handle but you you may have other requirements where for whatever reason you you don't want to do that so you might want to adjust this but not what we would normally say is set the value here high enough so postgresql typically and almost always checkpoint because of the timeout rather than the amount of wow that it's written i know that this is a soft limit so it might not cause a checkpoint to happen exactly on that uh that amount of data but somewhere around it slide please query tuning um so a lot of the parameters here certainly the first three parameters here are are effectively arbitrary numbers but they're they're relative to each other um so seek page cost is the the cost of reading a page sequentially from the desk so this matters if you think about it when uh when you're using a spinning disc if you're reading sequential blocks or pages from from that disc with a spinning disc it's a lot cheaper to read sequentially than it is to read random random page costs the next setting is is exactly what it sounds like it's the cost of reading a page randomly from the disk rather than just reading the next one each time so that might actually involve moving the the read head across the platter to get to that random page or waiting for the drive to spin round to the right point to read the page um now as you can imagine with ssd and nvme type drives sequential page cost and random page costs are almost identical because there's no physical moving parts you don't have to move a drive head you don't have to wait for the the platter to spin around and so on so there's a lot of variation in the way you can set these depending on what type of drives you're using and so on cpu total cost is also related to these it's the the relative cost of processing one row in a query and what these hints are doing in postgres is that they're they're helping it figure out when it should do things like choose a sequential scan over an index scan um so it'll look at statistics and we'll look at various different ways that the query can be executed and help calculate costs in order to choose the most appropriate query plan based on these parameters effective cache size is a as it says is a hint to the query plan so that unlike shed buffers this is not space that's going to be reserved um it's a hint to tell the query planner how much of the data we might expect to actually be in round whether that's in shared buffers or whether it's in the kernel's cache or elsewhere the planner needs to know this because if it knows that for example 90 of the data is likely to be in a cache somewhere then it doesn't have to worry about the relative cost of sequential page scans versus random page scans this is all going to come from memory anywhere next slide please uh right so client connection defaults um these are almost not performance related settings as you might think of them normally but they can help with performance in one way or another so idling session transaction timeout is used to terminate sessions that remain idle in transaction for too long so in other words um a transaction is begun by a client and then nothing happens it just sits there for five ten sixty minutes or so on the problem there is that the postgres has to maintain its view of the database for that transaction um so postgres uses what we call multiversion concurrency control where each transaction will see a specific snapshot of the database um the idling uh idle connections caused postgres to have to maintain those snapshots for for much longer and that can cause all sorts of issues with locks and maintenance and so on so using the timeout here allows you to cancel any sessions that users might have just left mid transaction for whatever reason usually not a problem if you're just running a very specific application uh more likely to be a problem if you've got users using psql pd admin and so on where they can specifically start transactions or explicitly start transactions and just leave them open accidentally or otherwise um shared preload libraries this is a general setting for pre-loading libraries when connections start when postgres um we would always in recommend including pg stats statement in there this is a very very useful plugin for monitoring and tuning um [Music] i would recommend just include that in there when you configure your postgres session then when you need to use it you can do auto vacuum so auto vacuum is it's a process along with the manual version vacuum that reclaims the previously used space in the in the database so basically clears out dead tuples that are no longer required auto vacuum and even duration specifies the the minimum duration of the that we log so if we set that to zero it's going to log all the auto vacuum activity um otherwise if you set it to a higher value if auto vacuum takes longer than expected then it's gonna only log an auto vacuum that that exceeds that amount of time auto vacuum max workers uh five is a very good starting point for that um this is basically the number of uh worker processes that can be auto vacuuming at once and the auto vacuum cost limit um very good for for throttling auto vacuum for stopping it trying to do work when actually there's probably no need to um 3000 is a good starting point for that next slide please there we go so other things to think about for reporting and logging log temp files very very useful for seeing if the workman parameter is set too low um when a sort happens when a query is executed if that spills to disk it's going to create temp files um and this helps you see when that is happening and to understand when you need to increase that workman parameter log checkpoints can also be very useful and is worth setting it to on um so you can see when checkpoints are happening or taking and so on and if you're running um edb postgres advanced server you might want to turn on the time statistics option which enables the dynamic runtime instrumentation tools architecture functionality it's a bit of a mouthful so we tend to just call it drita so this this allows you to log all sorts of information [Music] about the system as it's executing queries so i think that's the the last of these postgres settings that we're going to go through as i said there's a lot of them we can't really go through them in in real great detail but hopefully the slides here will give you an idea of what to go and look at and learn more about debra thank you dave uh for all of this great information um so i'm going to continue with fine tuning based on the workload and workbook analysis so um of course we have to find the slow queries first but of course the first question that you need to ask is which is slow and which quarters are slow um so because in some environments two milliseconds may be too slow if you are running a json company and etc or maybe it could be 100 milliseconds all up to you it all depends on the requirements but anyway we still have the log minimum duration statement the by default is minus one which means it is disabled uh so we don't actually log this uh like say slow queries so uh start with a number that that suits you it may be a second it may be a 100 milliseconds or it may be 50 milliseconds depending on the application right now just start logging the queries and start fixing them and once you fix them you can decrease the log minimum duration statement until you reach to a point that logs will be tuned i think we might have lost everyone there um hopefully he'll uh he'll get back in a moment um [Music] so what he was saying on that slide was um we can set a log minimum duration statement to a value over which the uh um slow queries will be locked so if a query takes longer than a specific amount of time um then it will be logged i'm just gonna share the slides from my hand so hopefully that can be seen um so that will help us figure out which queries are taking more time than we want um so we'll have those in the logs we can then um start to investigate exactly why they are they're running slowly um pg stats statements as i mentioned earlier is is great plug-in for um analyzing queries and seeing what's going on um so it is definitely worth looking into that uh and another tool another open source tool that's well worth looking out for is pg badger so this will help you analyze the logs that come out of postgres um so that you can see very clearly any any bottlenecks and so on that it's it's detecting whether they're due to specific queries or checkpoint timings and so on and sorry for breaking up thanks dave um so now we are going to talk about rewriting some queries um there are some other patterns to review and fix um i will yeah sorry sorry i accidentally skipped a slide that's fine uh if you can okay so um uh first let's go with the rewriting query so expressions can prevent use of indexes so if instead of using the query in the fir in the first one i mean when the when there's an expression uh just use a naked column which is which which is the column on the uh bold one on the on the second query so it's called naked columns as dave is just showing us where t dot eight a timestamp is less than the expression so when you use this uh your the postgres is likely going to use index for the specific query so make sure that you you have naked columns instead of using expressions uh i mean of expression on the on the correct side uh to make sure that postgres is using indexes and the next slide please um there are some other patterns to review and fix um the first two ones are don't do ones do not use select where x not in select and also uh do not uh use the second one as well but also but on the other hand you should of course if you want to use the ctes which is window functions uh do not use cts prior to postgres12 which the the postgres developers actually fix a few issues over there and of course you can uh group by the least complex type before the more complex types for efficiency so when you are using group by uh you should start with the least complex data types and then you can all then of course if you need you can use the more complex data types this for all efficiency of the query next slide please and um explain analyze explain and explain analyze years ago years and years ago when the postgres mailing list said they had some had some tips in the footers by default this was one of the tips explained analyzes your is your best friend it may not be our best friend he's one of the best friends of the postgres dba so use it the only thing that i should we should tell you uh please run this inside the transaction block because value explain analyze the query not i'm not talking about talking about explaining here explain analyze actually runs the query on the database and gives you how the queries uh executed so you are going to see the um planning phase you are going to the execution modes and et cetera so uh if you run explain it will just gives you the um the planning phase of the query not the execution phase but if you say explain analyze it's going to the query is actually going to run so it's going to use to give you the executor slots and then it's going to give you details about which parts of the query are slow which path it uses and etc means you are going you are going to be able to fix the problems with the query so when you go to next level we see in the next slides uh in the next four slices i think we are going to show you uh the great pga admin tool and the fiji nu4 and also of course the postgres enterprise manager pam um they have they both have a nice graphical user interface for explaining lies so as you can see in here uh on on on in the first column which is the statistics part uh there are some statistics per node say hash inner join h left join so in this specific query this output shows you v where postgres uh spent most of the time running the query which is great important for us in the next slide now we are also going to see another uh yeah thank you and now we are going to see the analyzers of the query so the darker color it gets it means post case is using more uh more time in here so as you can see on the on the top the the the last the last two items i mean the output is actually from the bottom to the top uh when you see the explain analyze output the the first node executed is shown on the under on the bottom and it is progress continues you are going to see the final one on the top so as you can see in here the costs i'm sorry the sorts and the unique parts uh caused the cause postgres to slow down uh to run this query so now we may have to analyze if there's a missing index or not is the say is the verb is enough or not as they mentioned we have a parameter called workman so you can get reasons and if you can fix it we can fix it the only thing that you should remember in here is when it gets darker it may not be also it may not be the problematic part it's dark in terms of just in specific query so the query may end up with just two milliseconds which is awesome but if it loses 80 percent of the time in a specific node the the all the graphical user interface is going to show you the darker color because it's the slowest part on the on the query so just make sure that if there's something that can be fixed fix it otherwise it's too fast you can just ignore the colors and in the next slide uh you can see the graphical the graphical analyzers of the of the explanation last query so this is also another way of post to see how postgres works how the query works etc next slide please and of course you will have to avoid or at least try to eliminate uh a few things uh with with postgres while running a query the beta estimates are usually caused by the lack of statistic lack of current statistics which are usually collected by analyze or auto analyze as they mentioned uh in the previous uh in previous uh slides so a bit meta estimates means postgres doesn't know uh the histogram of the auto data in on on on specific table or tables so it let's say if things there's there's just a million rows but if you have a 10 million rows everything is going to be different so just make sure that we have the right analysis of the data right statistics for the data external sorts are done on the disks so make sure that the work name is enough for all in general or you can you can set the workman per per query so you may want to get rid of external sorts to speed up the queries likewise hash patches fetches and lose bitmap scans and wrong plane shapes are the things that you should find reason and of course fix and finally we are going to talk about partitioning in postgres i'm going to show you quick demo as well so partitioning is um next slide uh please yeah uh partitioning um v so it's partitioning uh most of people know about purchasing but why and then do we need partitioning we need for maintenance purposes because easier to do data aging as i told you in the first few slides while talking about table spaces uh maintenance that pattern you can run some parallel queries across multiple tables it's going to speed up the query so this is these are the times that we need partitioning um there are three types of partitioning in postgres uh in postgres after version 12 we had range partitioning and this partitioning and i think as of postgres 12 we have the we have his partitioning as well so we have all three in enterprise db postgres advanced server we have the automatic partitioning which i'm going to show you a quick demo about how to do this so i'll just uh swap the screen chair back oh yeah it wasn't the original plan of course yeah sorry about that again i hope i assume you can see my screen now uh yeah can you increase the font size please i will i will everything was set up but then the connection was broken sorry about that um so um i just prepared a quick demo for you um we are going to create a new table called users and generated column uh oh i mean all the we use the serial in the past we have the name and the country uh and then we are going to start partitioning in here so unlike the others other syntax syntaxes and postgres and community postgres as well epa says automatic keyword in here and just this keyword is going to help us a lot so we first create two two tables first two partitions first so for the uh like partition uses england for the values england and partition for the england country and partition users india for the uh for for the guys in india so let's run the query i'm going to first create a table and then select and then select the table name okay let's keep it here drop table if exists users and then i'm just going to create table and then i'm going to get the um get the partition names so in the users table we have uses england and use india as we specified while creating the um while creating the table now i'm going to insert a value for claire who lives in england our virtual user in here select star from users is here so when i run the same query again see english in here now i'm going to insert another row which is arpan turkey now let's see what happens since we use the automatic keyword possibly has created the partition for us with with with this name since it the if the turkey value doesn't belong to the um previously created partitions postgres actually created a new partition for us automatically created new partition for us the same thing is going to happen for uh for these two for these two um inserts as well so i inserted two more uh two more in here and then for germany and andreas and magnus for sweden it created two new partitions for us so this whole automatic partitioning works in postgres but again this is only for this partitioning this is not this doesn't work for the for the range partitioning and then i'm presenting dave for you thank you back to normal with the slides um so what can we conclude from this um next slide please um the hardware the operating system and postgres itself are really the three main things that you need to be able to tune to to effectively run a production postgres system that you need to get the best from but you know this is this is an ongoing process yes we do this once when we're setting everything up um but particularly postgres itself we want to keep monitoring things make use of tools like pg badger impairment and pg admin and so on to keep monitoring the system and you know it's kind of like your car you've got to keep tuning it keep servicing it to make sure it's in tip-top condition all the time new major versions of postgres add new features new parameters and so on um for example at the moment we've got the the automatic partitioning with everyone just showed you in in epass as part of our oracle compatibility uh i would fully expect that a new version of postgres in the future will will include similar functionality although i doubt it will be necessarily modeled after the way that oracle does it um updating to new versions though is is troublesome uh from the sense that you you will often need to recertify your application make sure there are no behavioral changes to the fracture and so on um so you know that that's something you should review and periodically do but it's not something you're going to do necessarily every year what you should be doing at least once a quarter when we uh when we release them is keeping up to date with minor versions of postgres every new minor version of postgres will fix bugs it doesn't change behavior beyond obviously fixing the bugs um certainly doesn't add features or change features or anything like that so it's it's always safe to upgrade to a new version of postgres and we would a new minor version of postgres and we would always recommend that you do that over not doing so bug fixes help make sure your data is secure in the sense that you're not going to lose it or corrupt it and other bug fixes that cover security well the reasons why you might want those uh i hope fairly obvious um so do keep up to date with your your your minor postgres releases but now as i mentioned uh probably two or three times at least by now um this webinar is just over an hour as it turns out um i i think we've got 25 open questions in the q a i think we've answered a handful of them already so you can see this is a huge topic um this could easily be a two day training course the the amount we've we've covered here which is why we've gone over it at an extremely high level and just tried to give you tips and hints on the things to look at and to go learn about the the link there at the top uh postgres performance tuning and optimizations um is a paper that was written by vic fearing uh with help from devram and i uh a few months ago now and and covers the same basic journey from from hardware right through to tuning queries and so on in postgres but does it in much more depth so i i'd strongly recommend that you you take a look at that paper and use that as the the next step on your your learning journey um aside from that you know tuning postgres tuning operating systems you know it's one of those things it changes all the time as new hardware designs come out new versions of operating systems and postgres have different characteristics and different needs so this is not something you're going to learn once it's something you're going to learn and you're going to build on and evolve your skills over years to come i hope you find that enjoyable um as a career choice i i know i do and hopefully you've all learned something today and we'll be able to go away and put some of what we've said to some good practical use yes thank you so much dave endeavour that was an excellent webinar um as you mentioned we are over time so we won't be able to go over all the 28 questions plus we've gotten so we'll just be following up offline and as we've mentioned through the webinar we will be sharing the recording and the slides after via email and with that said thank you dave thank you devrin and thank you all for joining the webinar have a wonderful day thank you thank you \n",
            "let's switch the video there we go okay morning everyone how's everyone how's everyone doing their own awake barely so my name is Graham McAlister I'm a senior principal engineer with Amazon Web Services I work in our database group and most specifically I work on our relational database service Amazon RDS and I spend most of my time in that group working on the Postgres engines which we have RDS Postgres in our new engine Aurora Postgres so but that talks not this talks not really about that today what the talk today is about is tuning Postgres for high right workloads and so we're gonna walk through a bunch of stuff on that so what do I mean by high right workloads well for some people this can mean I just have a single threaded app that does inserts and updates and in that case you know the client talks to the database the database talks to the storage to do basically a commit and then it's got to come back from storage and that's your commit latency that's really gonna affect how fast you go as well as the return of the client and the client latency so this is really hard to make go very fast unless you're gonna tune those two things so some of the ways around that or people start you know using both methods you do copies or you do multi insert post cases you know one of the engines that allows you basically on a single insert to insert a lot of data so now you've got a much sort of chunkier set of data going to the database you know you've got bigger log flushes going to storage but commit latency still matters and then you have the return to the client and so client latency still matters so the way around this for most things is to go and have many clients working and whether you're doing small things or bigger things you basically have a huge amount of clients and this is like if you're running let's say PG bench to do a benchmark you might be running with 500 clients to get maximum throughput so then this problem becomes less about commit latency although it's still important and less about client latency sorry this is about throughput so and I'll show you why that matters later on in the talk so to test this to walk you through this what I did was I built a table gonna walk you through that real quickly I start with the PK being a UUID and so that's essentially a random value i next added like a typical you know ID which is a right-leaning sequence this is what you normally would have as a PK for a lot of tables I've a varchar are 100 that's gonna be all random characters of a varchar' 50 that's a small set of words an integer which is a very large integer all random a smaller integer which is a smaller set of randoms a boolean which is random but there's only two values so that's not really that and then a boolean where I leaned it to one side so it's a 75% true and a 25% false and then a timestamp like you know basically like the now function in Postgres which is also right-leaning and then I did the really evil thing and I indexed every one of these and you say well who would do that well no one would normally have 9 indexes on a table if it's only got 9 columns but instead of having building the 250 column table that many of you probably have in your databases with 9 columns indexed I thought this was just an easier way to see it so let's run this workload on Postgres here's 96 this was run on already ass single AZ so no extra we're not replicating to another availability so I try to keep it simple and it's the default parameters and so what you get here is you see on that far left we've got the vertical axis is transactions per second right and you can see that we're doing like 25,000 at the beginning and then it drops off very quickly right and it's minutes along the bottom so this is a 5 hour run so you know you get a good like 10 minutes of good performance before it all goes horrible and really what we want is we want something like that black line right we would like something doing 26,000 TPS on these inserts and not not having a problem so I also wanted to show you what updates look like so the same table and what we're doing is an update on the boolean and then the second update is on the timestamp so this is you know kind of a typical thing where you might be like hey I updated the thing and I also wanted to have a last updated column so this is you know sort of typical so again the base workload doing this update there's two updates per transaction and what we see is we get around you know thirty seven hundred rights per second or updates transactions so about 7000 total rights done what we really want is something more like 10,000 right and again what could we do to get there well that's what we'll walk through so we'll start with parameter tuning so one of the things that I noticed right off the bat with my workload was it was checkpointing about every five seconds so that's not necessarily something that you want to have happen so let's start thinking about how we can change that so I'm gonna walk you through sort of how some of the concurrency works in most databases and it specifically Postgres so let's say you have a whole bunch of clients and they all have a whole bunch of queue to work and that's what I'm showing in that yellow then you have the log buffer that the database has and you have the storage so essentially when you go and you queue that work up it goes into the log buffer when you say commit now that's got a flush to storage but in the meantime a whole bunch of the other sessions have queued up work that they would like to now commit but they can't yet because they're blocked because the log buffer is in the process of flushing and so until that log those items get through storage and get you know done you can't re-enter the log buffer so this causes a very stop start method and this is where the throughput of getting through the log buffer really matters and the larger the log buffer is the longer it takes to write which is the longer you're blocked from re-entering the log buffer the other thing that's interesting with Postgres is the thing called full page writes how many people know what full page writes is okay good whew good so let's say we got this block in memory and we start doing updates and changes to it that's gonna end up in our wall but if this is the first time we've touched that block since we last check pointed we're also gonna get a full block or a full page right into the wall stream so if we touch it again you know since we haven't checked pointed it's now a tiny little change right we have to checkpoint that block and we have to archive that okay so why do we do full-page writes well because Postgres by default has an 8k page size but let's say on Linux you're actually an operating system has 4k pages so you end up when you checkpoint that block out you end up actually doing like two rights and one of those may fail let's say if the Box crashes in between those now this is not a likely occurrence but if it does happen basically you have a split page or a torn page and you have corruption so if you didn't have the full page rights happening you'd kind of sooner or later you'd find out you know a month down the line that oh that data really isn't usable so what Postgres does is during crash recovery basically uses these full page rights to replace those rights those kind of incomplete checkpoint rights so that you don't have corruption so this is a really important thing from a preventing corruption but it does come at a cost and we'll we'll go through that in detail so here's a cut down view of a dump of a wall file hopefully everyone can read that I'm trying to make it as big as possible you know that looks kind of okay there's no full page rights and then later on in my run where you're starting to see the very slow performance what we see is a ton of full page rights and if you could count that you'll see that we basically have eight full page rights happening right and what that works out to being is that left side was doing 1k per insert right the right side is doing 48 K so I'm inserting one row and it's taking forty-eight K to go through the wall stream right so that's very painful and that's why we see the performance slowdown so what could we do well we could maybe on a wall compression right so what that does is it helps with the size so let's walk through again you know what we've done and you get the full block in the log well if you turn on wall compression that that block in the wall strings gonna be smaller but it might be that big it might be a little larger it all depends on how random the data is so if you have lots of random values you're gonna get pretty poor compression right so I reran my test and now the Green Line is with wall compression turned on doesn't look like that helps much - what about on updates well on updates it helped a little bit we got you know an extra couple hundred transactions per second which is not bad and this is actually not typical and the reason why was my data again is almost entirely random it's not what your normal data would look like right so if I go look at the size of the average full-page right in my wall stream it was 5.7 Kay compressed was only 5.2 so we didn't say the lot did we and the reason is again because essentially I have a fake data set right in the real world what we see is we actually see wall compression actually making a significant reduction in the size of the full page writes and it does help with performance so again random data does not compress well so the next thing to do typically is what you want to do is you want to have longer checkpoints because again you full-page right after a checkpoint so I increase the max wall size from two gig up to sixteen we have the same chart here now that's the blue line of running with the larger wall size so it looks much better at the beginning we managed to you know for almost twenty minutes get really good performance but then we slowly slide down to almost the exact same place we were with the earlier runs and the reason is we get more blocks more random inserts more full-page writes so what do I mean by that if you assume that we're doing 10,000 you know updates per second and we're checkpointing every 60 seconds then essentially we're touching 600,000 random blocks between each checkpoint right if you have a one gig table that has 130,000 blocks in it then most likely you're gonna touch the same block multiple times which means no full-page right except for on the first time right but if you have a hundred gig table you're gonna have 13 million blocks and essentially the chance that you touch the same page twice is actually starting to get very low so almost all of your run will be full-page writes so this is really important when you have random data so here's what we see on the update workload we actually did get you know a reasonable benefit going up by about 30% so you know it is a good improvement now someone would say well why not run async why you know you're just you're doing this high right workload maybe you could run async wouldn't that help and as you can see it actually doesn't it actually made things slightly worse and here's the reason why when we go back to this log buffer example right when we have that queued work and it gets into the log buffer and we have new queued work show up so even if we're doing async what happens is that's storage essentially now we don't have those guys that are the eight little yellow blocks there they don't actually have to wait right but notice the folks up top still have to wait cuz they can't enter the log buffer so it's less about the commit latency now than it is about how long it takes the log buffer to empty right it's again it's about throughput so when we added the extra you know wall space for to make our checkpoints longer that helped our performance but there is a downside so I went from a min wall size of 256 Meg up to 2 gig and I went from or sorry from 200 T 6 and 2 gigs to to gig and 16 gig but you'll see the recovery time is drastically different right it goes from 3 seconds to 91 you can imagine if my you know checkpoint was now running at like five minutes then my recovery times gonna be probably much closer to that so one of the things that you have to trade-off here is in your application deciding how much downtime can I take when I do crash the database versus how much write throughput do I want right so we can continue sort of working on making our checkpoint more effective and larger but I kind of want to leave it there so that we can see some of the other ways that you can actually get performance improvements without doing parameter changes so let's take a typical bee tree and this is not you know absolutely correct but it just kind of to show you kind of an idea here if we insert a sequence into a bee tree right like 201 what we're gonna do is we're gonna touch all those blocks right now if those blocks weren't in memory we would have to load 4 blocks right now since it's a sequence the next time we go to insert like 202 we basically don't have to load any more blocks right it's essentially free and from a full page rights perspective if you assume that those changes came after a checkpoint we would only possibly have to do one full page right now let's talk about what happens if we do that to a random tree or a random insert into that tree so if we insert one 24 what happens is we see again those typical 4 blocks that need to be loaded but now when we do something like 99 what happens is we starting touching more blocks again right so three blocks this time and 161 now again a couple more blocks and you can see if this was a really an actual real representation of a really large tree right this would continue on pretty much with every time you're touching something you're touching a bunch of new blocks so this is actually important for multiple reasons one is if you're doing this kind of randomness you need a lot more memory to hold your Vitry in because if those blocks aren't in memory you're gonna have to load them every and the other thing that you're seeing is even if we just look at the modifications to the leaf nodes at the bottom we had to touch three full-page rights as opposed to in the previous example where it's one right so that's a large amount of full page rights that we're doing so to see that in action what I did is remember you know the table with this nine indexes let's cut it down to six so we remove this index on a random string we remove this index on this random integer and we removed this index on this boolean now remember this is one of the columns that we were updating so this allows us now to have a hot update how many people know what hot updates are good so for those that don't I'm gonna just run quickly through this so assume that you have indexes a B and F and you have a table with a through F as columns and so you can see the indexes they basically point to the heap right and if we went and modified for example column F which is an indexed column then we essentially what we have to do in Postgres is you know we create a new tuple in the heap and then we mark the old one as old and then we have to repoint the index to it but that means we've got to make all those changes to the index pages as well as the heap page right now this occurs if you're updating any column that has an index it also occurs if there's not enough room in the heap for a new tuple so in this case even though I in the second example I'm updating C which is not an indexed column because there wasn't enough space we also don't get a hot update so in the hot example if I go and do that same update to see what happens is instead of us doing a whole bunch of changes to indexes all you end up is is linkage in the heap tuple or between the two poles in the heap and so the indexes don't have to change and so this is a lot less work if you can do hot updates and to show you that in detail here's again a slice of a wall output where I do my hot update so I my heap and my X log and you'll see one full page right because but when I did mine on hot update 1 what I see is five full-page writes so this is a dramatic difference on the cost of those two update statements right and again it's three you know basically three and a half K versus almost 17 K right so it's a dramatic difference if you can do hot updates so this is really important in Postgres when you're thinking about what things to index you really want to look at what columns you're changing with update statements and really be very you know kind of diligent about do I need to have a you know an index on that because it comes at a very large price if you're doing high writes so how can you look at this we can look at PG stat all tables and they have two columns which is basically the number of tuples updated and the number of hot tuples hot updated so if I go and do my update and here I'm updating column E which is the one that doesn't have the index now what we see is that this is a hot update because the number of tuples updated is one and the number of hot is one so we know that's all good now here's a different update where I'm updating my last updated which does have an index and what we see is that we did two updates but we only did one hot so essentially this was not a hot update and that was the one that was very expensive and this is the result of those changes so now we see in yellow what the reduced index buys us we get a much more stable run so we have less randomness right and I ran this actually for much longer and it really just sits right around 20,000 TPR inserts per second for you know days almost so this is a much more effective thing by just removing a couple indexes you know we got much better results than all the parameter tuning and all the other work we did and of course on an update side because we got rid of that one non hot update we see a doubling of our performance on the index or on the Envoy room moving that hot update or removing the non hot update so let's go back to our table again and let's look at some other pieces so remember that first column that I showed you which was the primary key which is a UUID and that's completely random well what could we do about that even if you wanted to use uuu IDs because you like them this is gonna cause you problems because it is fully random so what we really want to do is we don't really want fully random we want something that's constrained we want something that's working over on the right side of our tree to keep the number of blocks touched smaller so we really want something that behaves like this where if you've in certain 172 it's sort of on that right side if you insert you know 199 again you're only touching a couple of blocks right 161 168 none of those are causing a lot more activity which means you have a lot less F full page rights than you did if you were just doing the random thing so how do you go about doing this one technique that we came up with a long time ago was you can prefix your u ID with something like a date because the date is always going to move to the right on a on an index and and you can choose how much precision to put on the date for how far you want it to be at the right and how many blocks you would basically want to be touching so if you have more precision you basically are gonna be less blocks but you might have contention on those far right leaf nodes of the B tree so you know that's definitely could cause a problem but you can dial that in basically to your rate so what I did was I changed you know this original we removed those indexes we remove another index on this varchar' moved another random index on that one and then we made the UUID non random and we made it right leaning by adding the date to it and this is what we get as a result so you can see in the black there a sense we're now getting a super consistent run right that is essentially never going to have a dip because it's not doing much random work anymore it's essentially all right leaning which means we have very few full-page writes we'll only you know we you know massive reduction in those numbers now we see a slight increase on the number of on the update side and there's probably some tuning I could have done here to get that to be a better number but you know mostly it it's about stability on this workload that I wanted to kind of show now one of the other things that's really important with Postgres is vacuuming now everybody knows that you know you need to vacuum because you want to you know not run out of space on your system and you need to vacuum because you don't want to have transaction ID wrap around but you also need to vacuum for performance and here's why let's say we have six tuples sitting in a block right and we go and update them so the gray is sort of the old one now and the blue is the new one right so it's getting up in a new block and so as we go we're sitting there doing that that's all fine right but let's say we fill up this this whole second block with some more rows okay good now if we go vacuum right what's gonna happen is assuming there's no longer running transactions we're gonna remove those first two tuples and make space in the block right and so then when we go and update tuple three we're gonna go back into that same block now remember what I said about hot updates one of the things that we need is to be able to do this kind of thing to get hot updates to work right and so that continues to happen and we've only touched two blocks here on the other hand if we do the exact same thing and we don't run vacuum when we go update tuple three notice we're now touching a new block right and if it's the first time since we checkpoint it we're gonna have another full page right not to mention we're going to have the problem of not having a hot update so this is gonna cause a lot more performance issues and so you know you can just continue to see that without vacuuming performance gets worse so more blocks more cache misses right because you gotta have more memory to hold the these this extra space no hot updates more full-page writes and this is what it looks like in practice so this again is you know another update graph this one's run for much longer so now you can see that this is like 20 hours the red line is sort of just to show you what we'd hope to get for performance the little yellow ones you can see moving up and down and I'm kind of zoomed in on this so it's not actually that bad and the brown line is the moving average so we see over time essentially you know that it does dip down quite substantially now you could imagine if you kept this running for days without vacuuming that you know you'd start to see significant performance changes to your system if you're if you're not vacuuming you know often enough so one of the things I tell everyone turn on you know vacuum logging and then watch as it's vacuuming tables making sure that it's getting around to the table on a regular basis and keeping them clean right and that way you won't have these problems so one of the other things to think about is about how to improve you know specific workloads so let's say you had an insert only workload where essentially you're just adding more and more rows to the heap right and you're just adding new heap pages and then once they fill up you're going to get an another heap page right and if you have either no indexes or indexes that are right-leaning in that again you're not going to be touching random pages and if you have this kind of workload and you don't have long-running transactions one of the things you can think about doing is actually vacuuming in memory so how does that work so let's go back and look at our the same diagram I had before where we have the block in memory and we have our wall so we start doing changes let's say inserts into these in this block right so we're slowly filling this block up and then we're done with the block and then at some point we checkpoint right and we're going to write that block out as a data file and we're gonna archive it so that's all good but guess what that blocks not frozen so at some point you're gonna have to vacuum that right so when the vacuum actually runs it's gonna pull that block back in could be from cash might be from disk right and then it's going to vacuum it and then eject it and when it does that you're gonna get a full page into the wall stream again right so this is gonna again be very expensive and you have to re-check point that page right so you ended up writing the page twice through checkpoint twice through the wall stream right and you had to read it so this is very expensive if we vacuumed in memory on the other hand we end up with the exact same you know filling the block up but then we do that vacuum in memory and if we do that before the checkpoint happens what happens is we only checkpoint the block out once and we end up with no full page ending up in the wall stream so this is essentially at least half the cost if not more than doing it the other way and we end up with a block that's frozen right off the bat so you know if you're doing this big load you don't have to worry about oh well you know the age of my tables keeps going up so how can you do this well one of the things you'd want to do is make sure your checkpoint timeout is set kind of high because you want to be able to do this vacuuming in between the checkpoints right and one of the nice things about Postgres that you know most people know but I think sometimes forget is that you can set vacuum settings per table right so you can go set stuff like the max freeze age and other so you know related parameters on a particular table so that it will vacuum in memory so you can look at how many transaction IDs you're basically generating what the age of that table is and look at how long your checkpointing and then you can set your parameters correctly so that you can actually get it to vacuum in memory so I ran this test just manually if I vacuum in memory and I get that done before I checkpoint so this is you know load a bunch of data you know run the vacuum then essentially the vacuum takes three and a half seconds which is pretty good because this was I think I was doing twenty-five thousand rows per second for ten minutes so that's actually a lot of data to basically vacuum in three and a half seconds now when I vacuumed in memory so after a checkpoint so this time I loaded the data up did a checkpoint then did a vacuum command it takes 84 seconds now that doesn't mean your systems completely busy but that just shows you the difference and the amount of work that has to be done between those two things right and then to you know to show you what happens when it actually has to load from disk I basically cleared the cache and cleared the page cache and did the exact same thing then it takes a hundred and sixty five seconds so I mean if you're running a system which one would you rather have do you know would you rather have three and a half seconds or 165 right you know obviously you'd like the one that is in memory so that's you know this is very specific but it does show you how you can work with your workload to optimize these settings to get something that works much better now the reason why I'm going to talk about our or Postgres is it's not generic Postgres it's our version but a lot of this stuff that I've talked about is things that came out of our work here that we looked at on ways to improve because of the challenges of doing all this tuning and one of the things that's different about Aurora Postgres is it's a log base system so we have no checkpoints we have no full page rights and no log buffer now what does that mean in practice what that means is like if we go back to our you know log buffer example and we have our queued work goes into log on post grass you know so I'll sort of fast-forward through this since we've covered it you know we see this stop and go motion right on Aurora there is no log buffer when work comes in it can immediately be sent to storage and it can be completely out of order and what we do in Aurora is we track if we if we've gotten durability on those items so it's a four of six commit system so we need four acknowledgments so if you can see that example we have a ABCD all these different items so over time they'll be acknowledged as durable so for example a there is now durable in two places C's only durable in one once we get to for durable acknowledgments we can say it's done and now C's at four but it can't be acknowledged as done because we do need strict ordering so we need to wait for B and everybody to get done but you can see the difference here is that there is nothing that blocks us from having high concurrency because there's no single log buffer the other thing that's different because we don't have full page rights is how writing works again in Postgres we have that same thing where full pages are emitted to the log stream after a check point so you know this causes a lot of work checkpoints wall archiving all that on Aurora what we have is that blocking memory essentially gets the updates and we send those as log vectors to Aurora storage and that's it it doesn't matter if it's you know there is no before or after a checkpoint because we don't have checkpoints so that reduces greatly the amount of work that has to be done especially if you have a lot of indexes so what does that look like from a practical perspective so this now the brown line at the top is the Aurora Postgres now remember that this is a three a Z 6 way replicated system versus I'm comparing it currently against things that are not replicated so you could say well it's basically about the same as the black line where we optimized out all kind of the indexes and we got rid of the randomness it's pretty similar right and that's true but if you're actually if you couldn't make those changes to your application and you needed all those indexes what you see is you know a difference between something that's doing you know 7,000 TPS and 26,000 right so we didn't have to go tune the Aurora Postgres workload at all to get it to perform pretty well and on updates it's even more dramatic again in the brown we see that we're getting like 17,000 yes as opposed to the 3000 that we were getting on the base system again you can tune Postgres to do quite well but you have to spend more time in it so you know it's just an interesting thing when you change some of these features you can get some dramatic results so with that I'm going to wrap it up and open it up for any questions sorry can you repeat your question log o if you use unlogged yes so the question is the question is if you use unlogged feature then you get to skip a lot of this as well and that that is correct yes it all depends on again what you're doing from a recovery perspective and you know if you're replicating whether you need those things to be logged as well there yes sure so the question is using the non-random gooood versus a serial yeah they are very similar some some companies dislike sequences right and so they've you know kind of go with goods so I show that example to show that you can have sort of this thing that's fully random from a not needing or requiring a sequence but still get the performance the same as you would get from serial so you would see the same performance if I would have just replaced it with a regular sequenced ID other questions yeah so the question is on or can you do point in time recovery the same way as standard post-crisis yes so rora has the standard role right to you know within a second to you know when you did something so yeah it's it's exactly the same other questions yes if in sorry in just general Postgres or a roar I guess is okay so the question is really about if you have a real us right workload how does that change compared to what I showed um it doesn't actually really change much so both Aurora and regular post grass you know if the blocks not in memory gets read and that's still getting read as a block from both systems right if they're in memory it's exactly the same there's still just blocks in memory so none of that has changed at all it's just really you know changing the right path which is so Aurora is absolutely no faster reading you know out of memory than Postgres is post Chris does a great job at that Aurora does the exact same thing right it's only to get you know to you know to enable higher right workloads did you know some of these changes makes sense yes the question is what is the recovery time in Aurora compared to regular Postgres so this is another great question and I have a slide from another deck I could have shown but what we see with Aurora is that because it's continuously recovering on the storage layer there is no recovery that has to happen on on a crash so when you crash it's about two to three seconds no matter what right workload level you're doing so I actually have a one where I show you know Postgres running and getting about you know 90 seconds recovery versus about three for Aurora so you don't have to make that trade-off of do I want high throughput or do I want quick recovery other questions oh sorry oh there yeah sorry the lights right like yeah the question is what is the storage layer for Aurora it is it is a proprietary system I mean in that we've built a system of storage the the interesting thing is it's not really your typical storage it's got parts of the database in it right it's taking the log vectors and doing recovery to make blocks and the storage layer there's also a lot of other things about having the six-way replication being able to catch up do repair and all that stuff it's like a whole nother talk I'll probably giving it reinvent but yeah it's it is specific to ours which is why you know doesn't make any sense to open-source it because the it only works in you know I mean the the Postgres piece of it cuz it doesn't it doesn't actually work with anything but our storage layer the question is is it is the storage layer share between rora my sequel and or Postgres yeah it's the same storage system obviously they understand different things for each of the engines but yeah it's a shared component for us cool any other questions okay yeah right sure yeah yeah so the question is really about you know is this more applicable to serve OLTP versus data warehousing yes I mostly spend I spend a majority of my time thing about OLTP so that's kind of the focus for the talk but some of the things are applicable for example like the vacuuming in memory is one where it doesn't actually matter how you load the data at some point you need to vacuum freeze those blocks right and if you can do that before it gets down to disk you know you get the savings that I had in the slide right but the things like full-page writes if you're only ever doing you know inserts into a table that you know basically doesn't have a lot of indexes most of those full page write problems probably won't exist in in a data warehouse right but some of the other things I was talking about about throughput really is about trying to get the system so that moving the maximum amount of data through that log buffer as quickly as possible is still sort of applicable in a data warehousing sense but this is I mean I think you know from a talk perspective a lot of the challenges really are around those random indexes in an OLTP situation yes yep yeah you would ask me this wouldn't you so one of the challenges as I was writing this talk and realizing that there are no way to see full-page writes in Postgres at the moment except for by dumping the logs which guess what I run a managed database service that doesn't allow you to do that so that's kind of unfortunate right which is why you know my colleague asking me this question is quite humorous yeah so I is definitely one of those things that I think we'll look at you know maybe adding something to do that because I do think this is actually a major you know kind of performance changer right about how much full-page writes are going on and the fact that you essentially have to do PGX log dump to figure out if you're doing full page writes is is kind of bad right versus some of the other work like I was I was showing like hot updates there's a nice metric that you can easily look at right so I would expect something like that to be you know something Oh yeah there's lots I very nice yeah other questions okay I think with if we have any other questions about the talk RTS in general Aurora will be at our booth for the rest of the day so we have a team here so if you know wanting to ask any other questions feel free to stop by thank you very much [Applause] \n",
            "um and this is gonna be kind of high level and oversimplified uh it's not possible even for just these two topics to really get down into all the nitty gritty in the time that we have so uh if there's anyone on the call or watching this later on youtube who is very familiar with postgresql you're going to be like but you're leaving things out and that is true um so um the two topics that i picked out to talk about today are joint planning and statistics and we're going to start but with joint planning which i think will be a little bit more than half of the talk and then statistics will be the second part which will be a little bit less than half the talk um and so the first question about joint planning is you know why is that interesting why are you talking about that and i have three reasons so the first one is that joint planning has a big impact on query performance to the extent that you know if you get a good plan for the joints that are involved in your query you're probably going to be reasonably happy with the performance of your query uh if you get a bad plan for the joins that are in your query the query is probably going to be so slow that you're going to be complaining that's not always the case sometimes you get a bad joint plan and you still survive sometimes you get a good joint plan and it's still bad uh but it's a pretty good indicator uh it's uh it makes a big difference uh the second reason for talking about this topic is that joint planning is complicated and expensive uh the number of possibilities that need to be considered grows astonishingly quickly as the number of tables being joined increases uh we'll talk about that a little bit more in just a minute um but this is a you know a big consumer of cpu during query planning and the third reason is that joins are really common uh a lot of queries use joins and a lot of queries that use joins use many joins it's not particularly uncommon to see a query with 20 or more joins in it whereas it is pretty uncommon to see a query with you know 20 group by clauses or something like that or 20 limits i mean it can certainly be done and people do but joins it happens a lot more so why does the number of possibilities that need to be considered grow so quickly well um i got a query on the slide here that i think illustrates the point pretty well i've got n tables with very creative names and they all have an id column which has the same meaning in every table so it's sensible to just say that all of the id columns are equal to each other and join all the tables how many ways can we join all of these tables well i think it's pretty easy to see that there have got to be at least n factorial possibilities here because we can pick any one of our n tables and use that as the driving table and then we can join it to any one of the n minus one remaining tables and then doing that to any one of the tables we still have left and so on until we get down to the last one so that's n factorial possible join orders but that's actually an underestimate because it doesn't consider what a comment in the source code calls bushy plans because we could for example join a1 to a2 and then separately join a3 to a4 and then join the results of those joints to each other and that's not considered in any of those n factorial possibilities because in in those n factorial possibilities we're joining the tables in one by one but here they're coming in in a clump two at a time or more than two potentially i looked around on the internet for a formula for what the actual you know correct equation was for the number of join orders that had tables uh and i couldn't find one that i was totally convinced was correct so somebody probably knows what the real uh textbook says the textbook says it's the catalan number it's like four to the n okay could be yeah um but uh all what i know for sure is it grows faster than in factorial so that's already a very fast growing thing and that's just for the number of join orders so then for each possible join order um we have n minus one joins that are going to happen and each one could be a nested loop or a merge joint or a hash join so that's three to the n minus one possibilities so it's exponential uh three is actually kind of an underestimate because there's three basic algorithms sure um but each one of them has multiple variants so actually uh it's not really a constant exponent but if it were uh if you imagine that it were the the base would be larger than three so when you put all that together you've got you know something that grows faster than a factorial multiplied by an exponential it's a really large number of possible ways that you can join and tables together um so faced with that complexity you sort of have two options one is that you could try to give up on some of the possibilities without even thinking about them try to figure out which things are less promising and just don't even consider those and the other thing that you could do is try to explore the entire search space as efficiently as you can given how big it is and postgresql mostly opts for the latter approach there are some corner cases which i think are not interesting enough to spend time on in this presentation where we don't consider certain possibilities but by and large what the planner is trying to do is as an exhaustive search so uh we need to just make sure that we do that as efficiently as you can for a problem of the size and the basic strategy is to avoid recomputation so if you think about two possible join orders say a1 a2 a3 a4 or on the other hand a1 a2 a4 a3 they both start by joining a1 to a2 so we hope that when we study that problem the problem specifically of joining a1 and a2 we hope that when we solved that problem the first time we wouldn't use an algorithm where we would need to then figure out uh whatever we learned a second time when we studied the other joint order because they're closely related so postgresql uses a bottom up sort of dynamic programming strategy here to avoid that kind of recomputation and basically the idea is we're going to consider every subset of the tables that are named in the query so if we've got n tables in the query they're two to the n subsets of those tables the empty set isn't interesting but we're going to consider all of the other subsets and we're going to do so starting with the subsets of lowest cardinality so that basically means we're going to start with the tables right because if we have six tables for example there are 63 subsets of six items six of those have cardinality one and those are the six subsets that contain exactly one table so we're going to first consider each one of those tables then we're going to look at the subsets of size 2 which correspond to the 15 possible two-way joins and then there's going to be 20 possible 3-way joints and 15 possible 4-way joints and so on until finally when we get to considering the one possible six join we're actually six-way join we're actually considering the the original problem that we started out to solve which is how to join all six of those tables together um as we consider uh each of these uh tables first and then each of these joins we're going to make a list of potential strategies for solving that part of the problem and it's important to recognize uh that you know we're not actually going to use all of these joins right like we we have 20 possible three-way joins that we could do but whatever final plan we come up with can involve at most two of those uh and usually only one because it could involve two if we join three tables and then join the other three tables and then join the results of those two together but typically that's that's not going to be what happens so we're only going to use one or two of those three way joints for example but we don't know which one so we're going to consider all 20 to get there so in this example uh we have six tables or what the planner calls base relations and then 57 possible joins what the planner calls join relations for a total of 63 relations and we're gonna make a list of potentially interesting approaches for each one so this is not an efficient algorithm because we have an exponential number of things uh an exponential number of relations and we're doing a very much non-constant amount of work for each one uh we're going to consider lots and lots of possibilities for each one of those relations especially the joins that are higher up in the tree throw away the possibilities that look clearly inferior and then keep the rest so this still does not scale to large join problems because we are searching this fast search space so at some point the algorithm breaks down and there's a parameter called gecko threshold that controls when we switch to an alternative algorithm the default is 12 which i think is kind of low in almost all cases you can go considerably higher than 12 but at some point it's just going to get too slow to do planning the regular way so then we'll switch to something called the genetic query optimizer which has a genetic component but the genetic component doesn't really work basically what it does is it tries a bunch of drawing orders at random and picks the best one it will consider all of the possible joint strategies for each of the join orders that it selects but it's just going to consider a very tiny number of possible join orders so hopefully one of those will be okay some queries that's very likely to happen because in in certain query shapes there's a lot of join orders where they're all kind of the same and and so you're likely to hit one of them if you just choose some at random but in other query shapes uh you know there may be only a few joint orders that are any good and you may happen not to get one and then you may get a very bad plan so you you said that the like in for the genetic algorithm like it sounds like the cross breeding portion doesn't work and therefore it jumps just a random walk yeah i mean it there's code for it uh my understanding from people who have played around with it is that the plans don't really improve when it goes through the genetic phase or they don't improve very much so the quality of the final plan seems to be mostly a function of how good a job the random number generator does picking the initial population interesting okay thanks so we're making a list of strategies for each relation which strategy should we keep uh let's look at this uh query that i've got here on the slide select star from food joint bar on food.x equals x or fu.y equals one and consider this query from the point of view of relation foo how should we get the rows that we need out of fo well i think for most human beings looking at this query your eye goes to the where clause you see that foo dot y is equal to one and you say we should use an index on fu.y which does indeed have a very good chance of being the most promising approach assuming that such an index exists but not necessarily because it could be the case that nearly all of the rows in the relation have y equals one um and if that is the case uh then going through the index is going to be really expensive like not just a little bit worse but like a lot worse doing a full index scan of a table when you could have done just on a sequential scan is really bad um so if if rows with y equals one are uh very prevalent then we want to forget about the index and just sequential scan the table the nice thing about those two strategies which are here listed as a and b is that they are easy to compare uh as long as we know how common rows with y equals one are and we'll talk a little bit more about that in the second part of the talk uh we should be fine we can compare the costs of those two strategies and we'll probably get the right answer but there are some other strategies that are very difficult to compare basically impossible to compare at this stage and i've listed those here as c and d so in strategy c we forget about the index on y we forget about the sequential scan and we say hey maybe there's an index on foot.x that we could scan and we could just filter for rows where y equals one and that's going to be slower than whichever of a and b is faster but it might avoid a sort down the road because we could decide on a merge join between foo and bar and if we do and we don't use this strategy then we're going to have to sort sorting could be expensive because there might be a lot of data so we're going to need to keep strategy c even though it's going to be more expensive and we're also going to need to keep strategy d where we're basically totally unable to make a cost estimate at this time strategy d is again using the index on food.x but now we're not scanning the whole index once now we're repeatedly scanning it each time looking for a particular value so the way to imagine this is suppose we end up doing a nested loop join where bar is the driving table and then we're probing into foo for matches um which could be really really good because it could be that bar is a very small table and foo is a very big table and if that's the case that strategy might be a real winner um but we can't estimate how expensive that is compared to a or b or c because we don't know at this point how big bar is so we don't know how many times we would have to do that repeated index probing and we can't come up with any sort of meaningful estimate so we'll have to hold on to this strategy as well uh because we we can't tell so that's the basic algorithm uh and now i've got two slides here kind of talking about uh what works well and more to the point what doesn't work so well uh the good point is that we can effectively postpone our decision making we can really effectively postpone our decision making if we've got a potential strategy or what the code calls a path and we don't know whether it's better or worse than some other path we just keep them both no problem i mean unless you're concerned about performance then you might have a problem because the more paths you keep for the base relations the more expensive your joint planning is going to be at level two and at level three and at level four and at level five and however far up it goes and uh you know similarly if you have more ways in which one path can be worth keeping then you'll probably not only end up with more paths for your base relations you'll probably also end up with more paths for each join relation so everything that makes you have uh more ways to compare things and say that we don't know which one is better the more expensive the whole things get the whole thing get so when somebody comes along and they say i want to make the planner do something new if they just want to generate paths that will in a particular case either be better or worse than the paths we've already got that's probably okay i mean assuming it's well implemented and they're not going too crazy with how many new paths there are uh it it adds some cost but it's probably not going to be a big deal when people want to add new cost metrics right to say that something is better because it may avoid a sort or something is better because uh you know it may facilitate a nested loop or because it's better because it may let us do parallel query later or you know it's better in some other way other than just being straight up cheaper that kind of multiplies the planning cost through through the whole process so sometimes we can't do all the things that we want to do because it would just make planning too expensive um the other sort of problem that i see here is that we're doing decision making with very limited information one of the big things that we don't know as we're working our way through the individual relations is how many rows are going to get produced at the end so you know if we knew that your query was going to generate a billion rows it would probably be sensible to just give up on the idea of doing parallel query straight off because the bottleneck is going to be how long it takes to send those billion rows to the client in virtually all cases so to expend additional machine resources to generate those rows faster most likely makes no sense at all but we have no idea when we're thinking about the individual relations in the query we we have no knowledge of what the final row count is going to be we don't even know yet what the row counts of the other relations that are involved in the query besides our own is we we only have this very local information about what's going on with a particular relation we have you know very slight pieces of visibility into other parts of the query like you know what join clauses are available so which which tables is this table being joined to and we can you know skip over generating things that aren't interesting because there are no join clauses but that's like practically the only information that we have so i i i don't know exactly how we would make use of more information that if we had it but it does feel like uh you know the postponing decision making thing often forces us into making somewhat uneducated guesses about which strategies are worth pursuing at the early stages and instead of skipping the work of generating them and saving everybody else the trouble we have to generate them and then see what happens so vehicle like it's not so the the the process after is this multi-stage multi-level thing as you call it um and so at this first stage which is generating the paths you don't do any introspection to say i have some histograms or sketches that say what i think the selectivity will be therefore try to guess what what the output of a single operator or a single joint would be no we do know that we do know that but only for our own relation so like if i'm trying to estimate the joint between foo and bar i know how many rows are on foo i know how many rows and bar i know how many of those rows are coming out given the filters that are in the where clause uh i have an estimate of the joint selectivity between foo and bar right but i only know about those two tables if there's a third table baz involved in the query i don't have any idea what he's doing except i know whether he's joined to either of my two tables but i don't know how many rows are in that i don't know whether there are four other tables involved in the query or 15 like i only have facts when i'm planning to join between foo and bar i basically only have facts about those two tables and not anything else that that may be happening this is almost like an engineering thing that like sort of as you have you're doing local optimization rather having a global view what's going on right yeah okay i understand so i wonder uh sorry for a timing but i wonder whether there's an artifact that uh postgres is doing a bottom-up query planning so that you look for a local optimum for like a few number of relations first and then you walk up but on the other hand if you use cascade right if you use a top-down framework then maybe you can know what are the like stats of the entire query plan even though you are only concentrating on subtree for now because you will know what will happen what might happen logically on the other side of the tree does that make sense i wonder uh yeah i i think that's right i i think you know it's been proposed on the mailing list uh that maybe we should have more of a top-down approach which would obviously be a big change in terms of how much code would have to be revised um but i think it could potentially help because then uh you know if nobody ever asks you don't have to generate the data and if somebody does ask then uh you know you generate it and you cache it in case you get asked again true so the guy gertz graphic the guy that invented top down cascades he made a sort of offhand comment at sigmod one year that like you should use top down for all their planning but joint ordering you want to go bottom up and the germans that hyper guys were going to hyper and umbra those guys are very adamant about bottom up is the way to go i mean you're the first person i've heard is like yeah top down might be the right thing to do when they already have a bottom-up implementation well the the interesting thing about it is like if you've got a path like sequential scan the table bottom up is fine right because every time you consider that table you're going to consider that path right like if i if table z okay has a sequential scan path i can use that when i think about how to join a to b when i think about how to join b to z when i think about how to join the joint product of a and b together to z when i think about how to join c to z i'm going to get a lot of use out of that path because it's general right but what about strategy d here repeatedly scanning an index on food.x each time looking for a different value of x that path is specific to when i'm trying to join something with bar in it tufu right now it doesn't have to be just bar i mean in this case it does because there's only two tables in the query but but this kind of path on table foo could be used when joining bar directly to foo or i could join bar to some other tables first and then join the result of that join to table foo afterward and i could still use that path but but this path is less reusable than than these other paths right these other paths are 100 reusable this path has diminished reusability because it's only reusable in certain contexts now i i don't quite see how going top down wins because i feel like i'm still going to have to generate that path sometime and if i've got to do it i might as well do it in the bottom up flow what am i really gaining out of postponing it i it's not clear that i'm getting anything at all but it makes me uncomfortable like when i'm trying to write code to to generate new paths i'm like gosh you know i don't like i don't really know whether this idea is any good right i can't i can't tell right and so i don't have a specific proposal here i just think there there might be something to it i mean another idea that i've thought about is doing one bottom-up pass where we just estimate row counts and don't do anything else and then once we've got row counts for everything go back and do a second pass where we generate paths i feel like maybe there's something to that idea right because then we'd have some information about how things were going to turn out in the end earlier before we actually generated all of the paths and if knowing what the end result was going to be made you able to skip some of the intermediate work that might be cool but i don't exactly see how it would let you skip anything either so like i have a feeling there's something we could be doing that is better than the thing that we are doing but it's probably hard and even worse i don't know what it is yeah that's awesome thank you i have a quest a quick question what does the planner with foreign tables currently in terms of foreign data wrappers yeah so if you're using a foreign data wrapper and you have a foreign table involved in the query then basically the foreign data wrapper has to provide a callback uh that that can do the same function that we would do for a native table um so so it's basically up up to the foreign data wrapper to provide a substitute implementation of the same functionality that we're talking about here yeah so andre is talking about uh something called uh partial paths um a partial path is something that i invented during the development of uh parallel query um and basically a partial path represents something that if you if you executed the partial path within a single process it would produce a subset of the results so you you execute a partial path in every one of your processes and then you have to gather all of those partial results or gather merge all of those partial results uh to to produce the the whole output of the parallel query i don't really want to get into that in this talk because that's one of the things where i mentioned at the beginning like if you're a postgres expert you're going to see that i'm leaving things out here and that's just you know so that we can cover some of the general principles and keep the time to something reasonable so i don't really want to you know dive down into that topic in this presentation but uh you know it is a very interesting question i mean i think what i would say in general is i actually would like to generate a lot more partial paths but i can't because of this problem that we just talked about that when you generate uh more paths it it blows up the the planning cost in a way that we really can't afford to do too much of um so yeah it's a it's a problem for sure um okay so uh moving on i'm gonna go to statistics now if that's okay i mean absolutely good for it yes okay cool so uh statistics uh we have this command called analyze and it gathers various pieces of statistical information and basically what we're hoping to do with that statistical information is get accurate row count estimates we have a background process called auto vacuum it actually does both automatic vacuum and also automatic analyze so it will notice when tables have had a significant number of modifications since the last time they were analyzed and when it notices that they're like oh it's time to run analyze on the table and it will do it for you in the background so most of the time you don't actually need to worry about it [Music] sometimes you do but that again takes us outside the scope of this talk um and you know the world count estimates are important to everything that we just talked about so you know as we said before if you've got something like select star from foo where a equals one you need to know whether uh rows with a equals one are really really common because if they're really really common then you should probably forget about the index uh if they're less common then you probably want to make use of that index in some way here's a list of the statistics that analyze gathers these are all on a per column basis so for every column we're going to estimate the fraction of rows where there's a null in that column we're going to estimate the average width of a non-null value stored in that column we're going to estimate how many different distinct sorry different distinct how many distinct values appear in that column as you as you go through the table this one is a problem because it's often not very accurate it tends to cap out in the tens of thousands no matter how large the table is and that's because we don't want analyze to run for an unreasonably long time so we want it to sample a a fixed size portion of the table i mean you can control exactly how much but it doesn't scale as the table gets bigger so we just don't have enough information to really know for sure how many distinct values there are in the column um then we look at the values that appear in the column most frequently and uh make an estimate of the frequency of each one and then we make a histogram uh to give some idea of the distribution of values that are not mcbs so we take all the and then we divide the remaining ones up into like usually a hundred buckets so the lowest one percent of the sample values were between zero and 17 and the next one percent were between 17 and 51 or whatever it is um then we make an estimate of physical to logical correlation if your table has a lot of updates and deletes this is likely to be zero but if not if you have like an insert only table uh and it's like time stamp data so that the time stamp column is always going up then the correlation might be very close to one if you have a column that's only inserted in descending order and you don't have a lot of updates and deletes then the correlation might be very close to minus one and then if the column has an array type then we'll actually peek inside the arrays and see if we can make an estimate of what elements most commonly occur in your arrays and the estimated frequencies of them there's a big in into other data types as well like trees on similar no i don't think so but maybe you're about to tell me that the real answer is yes uh i don't think so though i wonder uh is there any discussion within postgres that to add the possibility to add any sketches to the stats like contaminate sketch estimate to give you a rough estimation of this activity of a particular value things like that uh i couldn't understand the word you were using so i wonder whether postgres has internal discussion on adding some types of sketches as a type of statistic sketches i i'm not sorry i'm not familiar with the term like the hyper log log uh countmen sketch they're like approximate data structures like approximate accounts problematic data structures yeah no as far as i know that idea has not been proposed um i might have missed it but i don't remember a discussion on that topic um we have used hyperloglog for a couple of other things but not with i don't remember anyone proposing it for this purpose we do keep the histograms as basically one type of sketch but we haven't uh we don't have anything more clever than that yet the the ceo of splice machine came gave a talk at cmu a few years ago and he was raving about how like the accuracy of their of their their cost model improved quite specifically when they switched from using like traditional histograms to like sketches um so let's revisit this topic in a slide or two because i'm about to say some more things which are relevant to this to this topic um so um you know the question is like uh if we're gathering all the statistical information then it looks like a decent amount of statistical information uh you know does it work can we accurately estimate row counts um back about 10 years ago now uh not long after i got started at enterprise db i did a sort of informal survey of email threads on the postgresql performance mailing list i actually gave a talk based on that study at an old con if you want to look it up but i went through like 168 email threads and sort of made my own tentative diagnosis of what i thought had happened in my opinion um and it looked to me like the big winner in terms of why those queries were slow was some kind of problem with the planner i attributed 94 of them to that to planner related causes um i attributed 26 to unreasonable user expectations and or confusion 23 to poor settings choices uh 14 to bugs either in the operating system or in postgresql and 11 to deficiencies in postgres outside of the query planner you can see that 94 is a lot larger than all of those other numbers uh and of those 94 i attributed 48 to row count estimation errors um so the number one cause in this informal survey of queries being slow was the plan of doing something bad and the number one cause of the planner doing something bad was estimating the row count um so this is not a solved problem this is very much not a solved problem do you remember do you remember to do that survey whether you were the the cost model was over estimated or underestimated oh both both right yeah i i mean and and just as a sort of general piece of context you know if you get the row count wrong by a factor of two or three it's not that big of a deal the real problems come in and we'll see some examples of how this can happen in a minute when you're off by orders of magnitude you know and it's often a question of not what multiple were you off by but how many of orders of magnitude was it like it can be six right when your row count estimate is off by six orders of magnitude something bad is probably going to happen to your query plan uh so yeah um they're basically only three cases where we can effectively estimate the row count and uh everything else is a mess i'll talk about some of the messiness more in just a minute um but let's talk about the three cases that are pretty good if you've got a simple equality condition like x equals 10 that's typically going to be well handled if 10 is an mcv then we're going to have a specific estimate for 10 [Music] which is likely to be fairly accurate uh if not then we don't really know how common 10 is but we know it can't be that common and that's usually good enough we do tend to be high more often than we're low because basically we're going to say well you know uh 70 percent of the rows seem to have mcbs in them and that means 30 percent of the rows account for all of the non-mcvs and we think there are 25 000 distinct values in the table so take 30 divided by 25 000 and there's an estimate um and that number may not be particularly correct for any non-mcv but unless the table is really really big it's probably going to be a relatively small number and that's kind of what we need because we want to know things like yeah you should use an index if you've got one um and as long as we get a relatively small number there we don't tend to get ourselves into terribly bad trouble um and then things like x is greater than 10 uh we know which mcvs are greater than 10 and we can use the histogram to refine the estimate for the non-mcds in my experience uh even though this is not perfect the number one problem is not algorithmic but just if the table changes really fast and the new analyze hasn't run yet uh then it might be off since we've got a couple of postgresql hackers on the call i'm going to just mention the fact that there is some code uh that is designed to correct for certain kinds of errors in this area uh on the fly at run time but it does not work perfectly or cover all of the cases so it's not a get out of jail free card and you can't have issues because analyze hasn't run uh frequently enough it's not super common but it happens um and then the third case that we can estimate pretty well is stuff like x is null or x is not null uh that's directly one of the things that analyzes measuring so we're fine pretty much anything beyond that we've got issues so my favorite example of this is the first query on this slide select star from through where the quantity a plus 0 is equal to a the planner does not know what plus does it certainly does not know that zero is the additive identity so all you said is select star from footwear it has no no clue what's going on there so it's like okay default estimate half a percent which you know if there are no nulls then this is always true if there are nulls then this is sometimes false which is an easy to overlook point but if there's no nulls you know you your your estimate is off by a factor of 200. um so that's that's the kind of order of magnitude problem that can set back query planning pretty significantly and if you change it to a plus one equals a then you still get the half a percent estimate because it still doesn't know anything about plus and now you're off by however many orders of magnitude in the other direction because uh the real answer is is zero right so that that case those kinds of cases suck uh what's a lot more common than that in the real world is something like that the second example on the slide here select star from foo where a equals one at b equals two uh in general all we can do is hope that those two conditions are independent of each other but they might not be independent of each other at all uh it can very easily happen that rows where a equals one are super likely to also have b equals two or super unlikely to also have b equals two uh and and that can result in you being way off i mean it's particularly toxic if you know both a equals one and b equals two are relatively common individually let's say they both happen 10 of the time so together we're gonna say okay it's probably gonna happen one percent of the time but the real answer might be ten still ten percent in which case we're off by an order of magnitude in one direction or it could be zero in which case we're off by however many orders of magnitude in the other direction this particular case we now have a nice tool that often helps a lot you can run a create statistics command and say please gather statistics on the joint distribution of those columns and then you rerun analyze it gathers additional statistics on the joint distribution and things get better usually there's probably data sets where the additional statistics that gathered are not fine granted grained enough to resolve all the issues but i think results have been pretty good from what i've heard um unfortunately as soon as you make things a little bit more complicated we're back in trouble so the third example on this slide here involves a join and now we've got two possibly correlated columns but they're in different tables and create statistics isn't yet smart enough to deal with that case so you're out of luck uh you may be way off um so to be clear when we say smart enough meaning simply the you don't support correlated statistics across tables correct okay so commercial systems can do that postgres cannot okay i guess that was yeah that was a statement sorry another question yes you're like i think yeah that's fine like i said i i don't actually know what other systems could do but we can't and that's bad yeah and that actually here's like if uh we'll get to him a second like what is what is the overhead for when when you run analyze if you add correlated sex uh i don't think it's very much although i haven't checked it i think most of the expense is having to read the table pages and what you compute after that i don't think is the big problem okay all right go for it yeah so for your example number three uh which is a very important one because it happens a lot because of the chain of the joints and you have predicates on two ends so one solution over there is actually run a sample to it it doesn't take that long to do it and you will see the correlation uh well i mean it it takes a long time to run the query unless you plan that query using accurate statistics no because what happens over here is that when you sample the query you can actually have other predicates in there such as udfs which you have no idea what the hell they are doing but you will see the correlation you will see the effect of udfs and it doesn't take that long you go server does something simple like this as well where they maintain uh i think a little small sample of the tables and they can you in order to derive this difficulty correlation you run them on the table i should pay also or just to random run the sample on a real life table sure yes efficiently yeah because our sampling there are two ways you know bernoulli but we also have table pages sampling so we skip over pages so if i do one in ten thousand it runs ten thousand times fast i say yeah i mean we have a table sample facility but it's just like a user level facility we don't use it for anything involving gathering ups yeah that that wouldn't cut it because it will also visit every row and then flip a coin you've got to skip pages what we're getting for we we have the ability to skip pages and pick a random subset of pages but it's not it's not wired into the statistics framework yeah but you know sql standards supports that right you know you put from table x table sample and then you specify your rate yes you know either bernoulli or you do system that's we put that in this that's what i'm telling you we have we we have to have okay that's good then use it well that's the the using it is the part that we don't do but what we do don't use this it's just for users to use which uh you're pointing out that that's not great and you're right all right um i want to talk one about one more bad case uh this is easily not not even close this is easily the most annoying optimizer fail in in postgresql the general shape of the query is select star from foo where a equals one order by b limit one and the problem here as uh i think many of you will immediately grasp is that we need to decide whether to scan an index on a or whether to scan an index on b and one of those is likely to be really good and one of those is likely to be really bad if we scan an index on a then we need to visit as many rows as there are that have a equals one and just keep whichever one of those has the lowest value of b so if there's not very many such rows this plan is really good and if there's a ton of those rows this plan is really bad the other possibility is that we could scan an index on b and just stop as soon as we find a row where a equals one in which case things are going to be great if we find one quickly and terrible if we go a long time without finding one so you know to see the problem uh imagine that ten percent of the rows in the table have a equals one the planner may say ah well this is good and let's say that's a big let's say ten percent of the rows in the table is a big number so the planner is going to say ah well if i just scan the index on b i will on average only have to scan about 10 rows before i find what one where a equals one so that looks pretty good comparing to go through and going through ten percent of the table but in the worst case it's actually far worse you end up scanning ninety percent of the table because it can happen that all the rows where a equals one have very large values in column b and so you just scan through the other 90 of the table and then all the rows you actually want are clustered at the end of the index and you don't find one for a long time or maybe there's you know maybe you think there's a lot of rows where a equals one and there actually aren't any for some reason i mean that shouldn't really happen if your statistics are up to date and the example is is this simple but you know sometimes we can get things wrong there too so um yeah this is a hugely annoying problem generally the planner goes wrong by picking the the index on b when it should pick the index on a and the best advice that i've been able to come up with in you know probably pushing 20 years of playing around with postgres is to suggest to people that maybe the index on b should be dropped which you know is a fairly blunt force instrument and often works because people often create more indexes than they really need um but clearly if they've needed that index for something else then that strategy is not gonna help um this comes up a lot it's so i was i was gonna say this question to the end but i think this is clearly the right time for this why no hints in postgres like why what does the design plot be it's clearly like this problem would be solved by hints it's it's nice to have it all be automatic but in this case it would save people a lot of headache yeah i mean reason uh for not having them i i mean my sort of you know advanced server is an adb fork of postgres and it has heads and i don't think i've ever told a customer to use them because they're not they don't solve the problem right like the problem that i i mean they might in this particular case actually but in general the the issue with hints is that typically what a hint does is it basically says use this exact query plan and whether you think that makes sense or not you're kind of forced into specifying the whole plan for the entire query because once you start right you're like i know you think it's a bad idea to use a nested loop to join these two tables but i say do it anyway well you forced that to be a nested loop but all the other decisions the planner is still making in other parts of the query tree are still wrong because though the root of the problem is the row count estimates are wrong and because the row count estimates are wrong everything's wrong so you can't really fix it by by by hinting a little bit of it you've got to hit the entire thing right you have to basically fully specify the the the query plan so if i had uh my way uh the hint would say hey you're wrong about the row count estimate and it would give me a way to to fix that and if i could tell a customer to put that thing into their query to fix the problem or better yet if i could do it declaratively with something like create statistics that'd be awesome right but just having a way to force one particular part of the the query to do a certain thing even though it doesn't seem to make sense is generally not great now this case might be an exception because in this particular case there's only sort of two possibilities and because this is being planned as a sub query it's not going to have the same kind of cascading ramifications on the on the rest of the the query plan so in this particular case yeah hemp might be a hint might be golden um that's right um yeah okay uh so last last real slide um i'd sort of like you to take away three points from this discussion of statistics uh the first one is that the system actually works surprisingly well for how dumb it is i was astonished the first time i discovered that a plus zero equals a could not be estimated remotely correctly and then i realized that i'd been running queries against postgres for years and many most of them have been uh working just fine despite it being that easy to fool the darn thing um and i i think that's a pretty general experience most queries run okay for most users most of the time but despite that people are constantly running into problems and some of them just don't have reasonable workarounds you end up saying to people you know have you thought about redesigning your entire schema and they're like no but i've thought about using a different database product that will work with the schema that i've got or at least some of them say that right which is not not very satisfying um and then the third uh sort of takeaway that i'd like you to have is uh that creates statistics is pretty good stuff um and it does help a lot and i think it serves some of the function that you might hope to get out of a hint system uh but without requiring that you decorate every individual query because that's another problem with hints once you once you start using inquiry hinting as a way of solving problems you're gonna you every query that has the problem has to be hinted whereas if you can somehow make create statistics able to deal with the problem then you do it at the dba level and you fix that once and then all the queries that everybody runs just work properly after that which is which is nice and those queries will probably also adjust to changes in the data which also won't happen if you try to nail down the plan to be a certain particular thing not trying to be too hard on hints i'm actually more receptive to the idea of hints than uh at least one member of the post-cresql community but uh uh um it's not i think it's not a problem-free approach even though it could be good in some cases there's also like other engineering aspects like they're sticky so like you upgrade to a new version and maybe the problem you're trying to overcome with hints is solved but like you're still it's like no one's gonna go back and fix the application yeah i mean that's another pitfall of that approach um so i \n",
            "today we will be talking about how you can tune your slow running sqls just a quick recap of what we did in the first hangout let me start my screen share and then so today we are going to look at how one can tune theirs their slow running sqls while working with postcards sql to do a quick recap in the last session we saw how one can find out those connections or those sessions which have slow running sqls how you can cancel a slow running sqls or we also saw how you can log your slow running sqls we saw what configuration parameters are important for you to do such kind of things what postgres sql views provide you information about your sessions and your sqls and the current execution time of your sqls now let's go a step further and see how you can get execution plan of your sqls so this is what we are going to see today we will be looking at explain explain plan of the sqls then we will see some basic tips of how you can tune your sqls and we'll be giving you some heads up for the next session that we will be holding so let's look at explain plan postfix sql provides you a command called explain you can use explain or explain analyze along with your query to see some important information about your query plan this plan is stepwise execution of your query which postwar sql takes or we can rather say that the uh the way optimizer plans to execute your query we will further see uh how what what kind of you know you can also see what kind of statistics are assumed and what steps are involved in the execution now let's look at an explain plan and try to see how you can read that explain plan now as you can see on my screen i have a very simple query the query is select star from a table and a very simple where clause a very simple filter criteria and this filter criteria has been executed by the optimizer using an index scan so you see this green box this is the operation involved this is a step this is one of the steps involved in the execution you see the orange or brown i don't know i don't know i'm really confused sometimes about the colors i guess this is brown so you look at this brown oval this brown oval tells you the cost of the step so this is how you read an explain plan the very first thing is is a step what is the step of execution the second thing that you see is the cost the cost that is involved in that step then you see a rounded square or a rounded rectangle which tells you about the number of rows which are returned by this particular step so here are two important things one is the cost so this cost is estimated from the cost parameters like uh sequential uh sequential scan cost or the random scan cost these are our tuple scan cost in memory or cpu processing cost all these cost parameters that you define inside postfast sql.conf we also see this rows number of rows these number of rows are actually estimated by optimizer based on the statistics available in postcase sql stats tables so this again tells you a very important information if you think that the cost of a query is too high and because of that it is choosing a wrong plan then you know what to do you know you need to update some configuration parameters same ways if you think that a particular step is assuming to return different number of rows compared to what it should then you know that your statistics are not correct they are not proper can can you guys see the screen okay so i just got a query that they can't see the screen so let me do one thing let me okay so let me let me go through these slides is this resolution proper let me do one thing let me start this and then i go back to and then again initiate my screen share and this time i choose a different stream and can you guys see the screen now okay so roger that so this is what i was uh talking about so uh i guess i guess you all guys uh might have been confused with what i was talking so far so this green box is what i was talking about with respect to the step involved or the operation involved this is what optimizer is executing in this particular step so this is how you will read explain plan of a very simple query this oval in brown color this is uh this is what is the cost of this query the cost is calculated based on various cost parameters that you have defined inside your postgresql.conf so if you see that the optimizer has used a very wrong calculation or optimizer has estimated the cost wrongly then probably you need to go and modify your postgresql.com similarly if you see this blue color rounded rectangle if you see this that the estimate of rows returned by this particular operation is wrong then you know that your optimizer has got wrong statistics that means that you need to analyze your tables you need to run statistics update on your tables so your explained plan already gives you quite a good bunch of information about how optimizer is planning let's look at a variant of explain command which is explain analyze so explain analyze is little different from plain explain explain analyze actually executes the query and gets the actual information as well and not just estimates so as you can see the blue oval that represents the estimated cost and estimated number of rows and the width of your rows whereas when you look at the red oval that is actual execution time number of rows iterations done on that particular data set and below you will also see it will also tell you what is the total run time or execution time of this query so just to confirm can everyone see a screen where we have a blue color oval and two red color ovals so this is what we are talking about and this is how your explain analyze command will help you so you can if if you don't have proper statistics estimations available for your optimizer you will see a vast difference in the estimate and in the actual so you see number of rows inside the blue oval and the number of rows inside red oval if there is a huge difference in them then you know that probably your statistics updation is not happening properly and you need to do something about that now let's let's look at some tips for tuning these are generally incremental steps and i myself prefer implementing them in the same order because the order is very important and and we will see why why is that important we will see that in further slides so the first thing i do is spend some time and tune the query optimize it remove costly clauses remove something which is not required and we will be looking at some of those tips how i do that in general second thing that i look at is creating indexes if there is any index which is missing and i see from the plan that a particular operation can be done more efficiently by creating an index then i try it out before i actually create the index i try it out and then i create the index then second thing i do is optimizing my parameters so we'll also be looking at some of the indexing tips and then what i do is i optimize some parameters sequential page cost random page cost effective cash size etc these parameters are kind of feedback or these are kind of inputs to your optimizer and it is very important to set them correctly we are not going to discuss them today but we will be definitely discussing about them in one of our upcoming sessions then one important thing that you sometimes need to consider is some resource hungry queries no matter how much you optimize no matter how many indexes you have there could be some costly queries some olap queries which are taking up lot of resources or which are running slow because they don't have enough resources for those queries you need to change certain parameters either at user level or session or transaction level so that is another thing that i do sometimes and postgres sql is quite flexible in that respect because it allows you to change these kind of parameters at a user level or at session level or at transaction level and sometimes even at database level parameters like work mem which are used for sorting and if a query is taking a lot of sorting space you can allow just that query to take let's say 1gb of sorting space and still all the other queries will be taking maybe 10 mb or whatever is your global parameter so i find postwar sql quite flexible that way obviously there are other changes that one should consider things like changes in your application architecture maybe you have to cache certain tables or maybe instead of firing a query too often you should cache that data somewhere or change in the way your application is accessing data or consider changing your schema design maybe some denormalization or maybe changes in the data type or in extreme cases you can also consider hardware upgrades but as i said these are all incremental steps first of all try to do all sort of tuning that you can do then only you should go for things like application architecture changes or schema changes or hardware upgrades because these changes are generally more costly compared to tuning your query or compared to indexing now let's look at some tips uh for tuning the query few things that i generally look uh and few things that you can easily observe in a query and go ahead and change one thing that i have seen a lot of people doing is they always do select star everyone always wants to use select star but they don't really need it their application actually needs only certain columns but they will end up doing select star because that is easy isn't it instead of listing four five or six or 10 columns out of 20 available columns i can simply say select star makes my development faster isn't it but that does not make your query faster your query becomes slower sometimes and that happens because of the network io and sometimes the disk io that your postgres sql has to do like you can see here in these two explained plans so i have tried to generate an explain plan for a select star query very same query instead of doing select star i specified two specific columns and you see an immediate difference you see that the total execution time has come down by more than half the and the reason is quite obvious you look at the width of result set which is being returned has been reduced almost 1 10 and actual execution time of my index scan has been reduced so this tells you exactly why you should not do select star and over and above this obviously if your application schema changes if your number of column changes your application code may start breaking so suppose today you have 20 columns and you have said select star and your application is able to handle that tomorrow if you add another column it becomes 21 and now your application has not been modified to handle 21 columns so your application code may break as well so this is another reason why you should avoid doing select and you should prefer to use specific columns that you want to select then another thing which i have seen a lot of people doing is they would just go for selecting everything now when i say everything it is not with respect to number of rows uh sorry not with respect to number of columns but it is with respect to number of rows generally you don't need all the rows and even if you need all the rows your application cannot handle all the rows at the same time mostly in enquiry screens where you need lot of roles your application first thing if it if it is java or if it is dotnet it cannot store all the rows everything in your record set inside memory so it will make your application resource hungry if you are selecting all the roles so it is not just a performance performance consideration on the database side but also a consideration on your application side or on your app server side plus when you show it on screen you your viewer or your user cannot see all the rows at the same time he has to scroll through lot of roles so generally a very good practice is to put up all your filters in your sql query fetch only what you need using where clause limit your record set using limit or offset clauses offset allows you to skip certain number of rows and then limit allows you to select particular rows or n number of rows so this way you can implement a rolling inquiry screen very easily and in that case that enhances your database performance enhances your application server performance plus gives a better visibility and better user experience to your end user isn't that wonderful one tip and that gets you three things done so let's look at an example when we look at this example i have fired the very same query but instead of selecting all the rows i am trying to limit the record set i am trying to limit the record set and make it hundred instead of selecting everything so you see that immediately from almost 11 000 rows my number of rows have come down to 100 and what this means again less number of data less number of network traffic less amount of io and because of this you can see that total execution time has come down drastically it has come down by more than 100 times isn't that wonderful plus your application now has to deal with only 100 rows in memory and when your user sees he looks at only 100 rows and when he wants next set of 100 rows he will click next and your database will perform another select query which will be offset 100 limit 100 so this way you can optimize your queries and user experience as well third thing which i have seen a lot of people doing is unnecessary sorting now when i say unnecessary sorting sometimes you need sorting but sometimes you don't now when is that sometimes you have limited number of data to play with and you want to show it on screen but your application has sorting feature and this sorting can happen on any of the any of the columns let's say you have date and amount fields to be shown in transaction details and user has flexibility to sort it either by date or by the or by the amount in that case the sorting should be left to the screen or it should be a a client based uh client based operation or on the client end it should not be done at the database end because does not matter how you are ordering it the user can choose a different order so avoid order by clauses as much as possible use them only when your application and your end user requires them because whenever you do order by there is some sorting memory required when the sorting memory is required postfastesql tries to allocate that from work man but if work mem is not enough to accommodate all the sorting data or all the data set which needs to be sorted it will spill over to disk now when sorting goes to disk there is io involved which slows down your query you can obviously overcome this by setting your workmen to an adequately high amount of memory but that could be an overkill sometimes so try to avoid order by clauses as much as possible but when your business needs it obviously you cannot avoid them now let's look at an example here now when you look at this example we have said order by account and that query has taken 57 milliseconds compared to a query without an order by clause which has taken around 12 milliseconds so this is around one fifth of the execution time with order by clause so there is an advantage of 500 percent right so try to avoid sorting and in this case we are lucky that the sorting method is quick sort and it has accommodated everything within memory nothing has gone to disk so imagine if same operation would have happened on disk this might have been even higher the run time or execution time could have been a a second or half a sec or easily around half a second since it would have involved another i o operation now lot many times people want to do an update and they want to do an update from another target table and this target table has some data that you either need in the where clause or you need to set your columns to that data now this makes it you know a very complicated query that you can see on the screen in red so you can see i'm trying to set up a target table i'm trying to set values in a target table i am trying to set values for two columns which i have got column to update and column two for update and you can see i am trying to select data from another source table and i have to write a sub query in this case a correlated sub query a correlated select sub query inside my update plus i have to ensure that i am only updating those rows which i need so i need a third sub query now you imagine what would be the plan of such a query this query can be easily translated into a much simpler form because very few people know that postgres sql supports a different form of update statement which is update from and you can use this from you can use this from table which is your target table sorry which is your source table in the from clause this is similar to a join statement you are trying to join two tables and trying to update the target table from the source table and you have pretty much a join condition here which joins your source and target tables and based on this join criteria you will get certain values which you are assigning into your target columns now let's look at the explain plan of the first query that we had for simplicity i have avoided using the exists clause here i'm just trying to update all the rows if they do not exist it will be updated to null and let's look at that so what you see here is there are two sub plans these two subplants belong to two different sub queries and these two sub different sub queries are cumulatively getting executed in almost 22 seconds in almost 22 seconds and there is a lot of index scans even though my columns are indexed even then this is quite a costly plan now let's look at update from version of the same query so you see this query looks more complicated because it is more verbose there are more operations happening but you see there is a hash join like i said there is a join condition there is a join happening between the target and the source table and this join condition is joining our columns our target and source table and there is a join memory or the sorting memory which has been used here and the total execution time has dropped from 22 seconds to one second that is a huge huge benefit plus my update query looks much more simpler now imagine in this first example we had two sub plans because we were updating just two columns imagine what if we were updating three or four columns the increase in execution time or the cost of query would had been exponential it could have been 44 millisecond of 44 seconds whereas over here it would have remained more or less same because there is just one hash join happening there are not multiple sub plans which makes our life easier now this is my favorite tip this is what i do you know with this is what i share with developers all the time sometimes we try to do things we try to make things simpler and we try to implement business logic as it is like we want to check all those people whose salary would become 1200 dollars if i add 200 to their salary so these kind of queries people try to implement the logic as it is another example is i want to see what which are those employees who are hired on a particular date and my higher date is actually a timestamp column so in that case the ideal way would be to truncate the datetime part of that column and compare it with a date field or a date literal now these kind of queries will not actually use an index scan your postgres sql will not be using an index which is created on higher date time or in case of first query an index which is created on salary and if you would apply a little more uh you know aptitude in this if you if you put in a little more no you know if you try to go back in time and try to apply your high school mathematics simple high school mathematics which says any equation is same when you shift your left hand side operands to right hand side and inverse the operation so that is what i have done so you see after the arrow the operations in blue marker or in the blue ink it is same as the previous operation but i have changed it i have shifted the expressions towards literals instead of using them with the column so you see instead of checking salary plus 200 i'm checking little minus 200 same ways instead of checking or truncating the time part from my date i'm trying to do it other way around i'm trying to append timestamp to my date and checking between an interval now my second form will actually be using the index it will do a range scan on my indexes which is great which will reduce my query time and i don't have to create another index which is based on the expression and and this is pretty simple now what happens when all these basic tuning tips does not help what happens when these basic tuning operations do not help you out still your plan remains costly or still you have not been able to optimize it to the level that is required by your slas what do you do in those cases so the very first thing that you can do is do an explain plan and look for usual suspects look read through that plan and try to see which is the most costly operation so like you can see on my screen the highlighted operations are quite costly the cost of that particular operation is 66 it is 66 as per the cost defined in my postphasescale.com and now i know which is the who is the culprit now i can try to fix that i can try to either tune that part of the query or i can go ahead and see if any index will help me so that is what i have done here i have tried to create an index on one of the columns which is involved in this exists clause in this hash join so if you see in the previous screen i have a hash semi join happening so what i do is okay there is a sequential scan happening on pg bench tellers for a criteria which involves branch id so i create an index on that and now i see that sequential scan which was costing me 66 units has been converted into an index can which is almost eight and a half units which is much more faster and my execution time if you look at has been dropped down well the first query i did not actually do and explain analyze so i don't have the execution time as such but you can see the cost has been dropped from 75 to 14 which is almost five times of difference which is great so when you do indexing on your tables when you go ahead and create indexes on your tables you need to remember a few things a few facts how indexes work in postgres sql postgresql has a feature called index only scans so if you have a query which can be executed by scanning all your indexes or all available indexes postgres sql will not even touch your tables it will simply fetch data from available indexes something called index only scans is what you will see in your explain plan another great feature that postgres sql has is intersection between two indexes now previously we saw there is an index scan happening here on branch id and if i had one more index on bank balance or branch balance in that case postage scale would have created an intersection between these two indexes and then it would have fetched data so postgres sql can do intersection between two result sets from index scans so if you create individual indexes that can help your queries poster sql can use filter indexes which is quite a great feature which i don't find in a lot of modern day relational databases now how these filter indexes would help you for example if i created a filter index and i said that create index or index branch ids where bank balance or branch balance is equal to one thousand or ten thousand so in that case this particular query would have been fetched directly from that index okay so this is how your filter indexes can help you then comes pattern searches you should be you should be careful that certain pattern search can use indexes like if you're doing a like search where you're beginning with certain alphabets you your indexes on that particular column will be used by post sql but if you are going to use an inde uh a like clause where the pattern is in the beginning and alphabets are at the end or your characters are at the end those queries will not actually use your indexes so you should be wise you should be careful when you are using or you are doing pattern searches you should know which expression will use index and which will not use index poster sql has a feature for expression or function based indexes you can actually if if you recollect if you recall when we were discussing the simple high school mathematics what we did to simplify our operation or to make it much more efficient we shifted the whole operation from columns towards literals what we could have also done is create an index on this particular expression if you know that your database is all your application will always vary based on this expression then you can actually create an index on the expression itself which makes your life much more easier but one thing that you should remember is indexes are not a replacement for a flawed schema design if you have not designed your schema your tables your normalization is not proper or you have over normalized your schema indexes are not a replacement for that or if you have written a query in a very bad way if you have a query which is which has got a lot of sub queries which has not been optimized properly and you think that by simply creating indexes you can optimize or you can enhance the performance that is wrong indexes are not mean for that that is why i said initially i prefer to execute these tuning steps in this particular uh order first thing that i do is tune my queries and only then i go for looking for any indexes looking for a possibility of new indexes and before even writing your queries you should know what is your business requirement and what kind of queries are you going to expect and accordingly you must design your schema so in the next hangout as i said earlier in in the next hangout we will be discussing some basic configuration and tuning techniques we will look at some of the parameters that we discussed earlier uh how you can tune them and uh what parameters you should tune in at the global level what parameters you should think of tuning at the at the session level or at the user level so with this we we end this uh session let me go back and see if we have got any questions it's still loading the question answers if anyone has any questions please put them across we'll be happy to answer that now or in case you are watching this as a as a recorded video you are watching this on youtube or you are going to watch it on our website or you are going to watch it on or you are going to watch this on vmo we are going to upload this on vmware as well then in that case you can go and comment you can put this as a comment and we would be happy to answer your queries so is the date for next hangout announced it is uh not yet announced but uh we are planning on doing this more frequently we have seen quite a good response on the previous hangout and even this hangout we have seen people are liking this and we are going to do it in another 15 days or or a month time and and stay tuned and please let us know if you are one of the experts from posterior sql community or you are a poster social user and you want to share something that you have learned and something that you have tried in postgres sql you want to share that with other users you can use postgres hangouts as a platform to do that and will make you famous as well so people will be watching you live so please drop us an email if you want to or put that in comments on our website if you want to be one of the speakers in an upcoming hangout yeah so \n",
            "hi everyone I want to welcome you guys to mostly mistaken and ignored PostgreSQL parameters while optimizing a post SQL database my name is Lindsay Hooper and I'm one of the Postgres conference organizers and your moderator for this webinar before I hand off to avi I just want to give you guys a brief intro about what we're what we're gonna be talking about today so in this webinar Ivy's gonna detail a list of important PostgreSQL parameters that could drastically improve the performance graph what properly said discuss mathematical formulae and extensions helpful to expose diagnostic data for tuning parameters enjoy a detailed description of the top mistakes made while tuning Postgres parameters and how to set them correctly I'm here with avi tech lead global services at percona prior to joining percona avi worked for open scg for two years as a database architect and at Dell is a technically for almost ten years his vast experience in technologies like Oracle Postgres my sequel and MongoDB he's an avid Python and go Lion developer and he's co-authored a book on Postgres beginning Postgres sequel on cloud and another one i'm PostgreSQL in progress his areas of expertise are Postgres sequel training consulting and migrations so with that I'm gonna hand it off to AVI and you can take it away enjoy Thank You Lindsey you could see my screen right Lindsey cracked okay just what was good for them alright thank you thank you for the introduction and hello everybody again and thank you for joining today for this webinar and as Lindsey said like I've started my career as an Oracle database administrator and and and at the same time like I've seen even MySQL MongoDB so I've got some experience working with those databases as well and then I've started working on Postgres in parallel and now I'm of course also working on migrating those databases away from you know Oracle the actual database I started my carrier with two Postgres huh so yes so this talk is more about the parameters that you would have to tune in your Postgres database or in case you haven't tuned them or in case you're not aware of the right ways of turning them in this talk hopefully should help you all understand how to do it so without any further break like and I'm also working for percona as Lindsey said and as a tech lead please feel free to follow me on LinkedIn and we could have a nice chat all right so some points to note most of the performance issues most of the performance issues today as we have seen in majority of the client environments are all due to lack of tuning so sometimes most of oracle to Postgres migrations during the initial stages when the code let's say you migrate PL sequel code to PL PG SQL and Postgres or you migrate some tables and their data and start running some queries the performance teams starts complaining that the performance is not that great on Postgres right so what really happens in the backend is that you haven't really tuned the parameters that you would have to tune so just not migrating the data but along with that you should also be tuning some of the important parameters and you should tune them like you tune it for Postgres but not like oracle because there are several architectural changes between oracle and Postgres some of the you know commands or some syntax may appear similar so we definitely could go under the assumption that few things stay the same with Oracle or Postgres but we need to be aware that there are a lot of things that are different between different databases right and likewise the Oracle or any a database you cannot just compare directly with Postgres so you need to know how to tune it like Postgres and so like I said most of the performance issues are due to lack of parameter tuning and if you don't tune them and leave it as the default parameters assuming that the Postgres really works fine with the default parameters that may be a mistake if you have a really minimalistic server configuration these default parameters may be good enough for example your shared buffers your Postgres memory area is actually set to 128 megabytes for the by default and that is that could be really tiny for a huge of production database sometimes I also see a lot of mistakes related to auto like you people assume that disabling auto vacuum improves performance but you may be making a really big mistake there because if you go ahead and disable auto vacuum you need to know what auto vacuum is doing in the back end and you should be able to do all of that yourself but it's really really difficult to tweak everything that's being done by auto vacuum and you know you really need to be an expert in doing that and at the same time you should let Postgres do its job while we trying to tune the auto vacuum settings the way we have to do and sometimes while loading data or for some temporary purposes we may create tables and set it to unlock we're just fine but you may really not want to do that in your production data basis just to increase performance because usually you could go ahead and set your table to unlogged but that would of course avoid wall right so you're right ahead logs do not get generated but you should know that the replication would I mean these tables that are set to analog would not get replicated to your standby database so due to that reason even if you would need to perform a point-in-time recovery using any of the recent backups you don't have the right head logs that contain the changes that have happened to these tables because it's set to unlock so due to that reason a lot of damage could happen and at the same time I also see some mistakes or assumptions that setting the Postgres database to an archive mode by archiving writer head logs would you know fiction performance which may not be right right so you need to archive your writer head logs safely biggests your standby your streaming replica or even your point in time recovery all of that depends on the writer headlocks so never try to go ahead and you know disable the durability that's offered by Postgres by default at the same time also never try to turn off some feature that could help you really during disasters Postgres allows instance level database level user session and transaction level settings so some other parameters which you set globally you know could take effect into everything like for every user for every session every query but you don't have to do that you could you have a lot of parameters that you could set at the database level or a user level session level right so you need to know how you could limit the you know overall impact of that setting to a transaction or a session so now we would actually go ahead and quickly see an architecture diagram of the Postgres so that we then get started with the parameters because all of the parameters that we will be discussing today will be based on the architecture diagram that you see here so in this architecture diagram you see processes right which it does include background processes and you see some of the memory components and you also see the storage so in the processes you actually see that the postmaster of course which is the which can be considered for oracle guys it could be considered as a listener who listens to the connections first and forks a background process for each connection right because you have a per connection back-end process right so if you if your application connects to a post-christian base Postmaster wood for I'll use a process and so another process is created in the backhand so likewise if you have so many processes so many user processes right and you also have some background utility processes similar to the background processes that are running in some of the databases so each of these processes have got a certain you know I mean each of these processes you know have certain purpose so for example chip pointer is a process that that's responsibility is to perform a checkpoint BG writer is a background writer archiver wall right and log right logger order vacuum so there are several such background processes you know but we will not be going into all of these background processes in detail today and you also have work mam maintenance work mam wall buffers share buffers which all come under memory because all of these will be using Ram and you also have data files writer head logs and if you archive these writer head logs you call them the archive walls of course you may be storing them in some remote storage which is more durable or it could be cloud and you also have Global's right or tablespaces maybe or a lot of other files so all of these come under storage so now by considering that architecture diagram we would go ahead and start discussing the mistaken parameters so the first mistake in parameter that we are going to discuss about is shared buffers so what does this parameter mean shared underscore buffers so every time you create a flows cross instance or if you already have some existing Postgres instances you may go ahead and see you said it post-credit you said arms you said a value to shared buffers parameter right and you may go ahead and set like 40% of the total Ram available 25% or you may just go with some random value as well but how to set it the right way so shared buffers is the amount of RAM that is actually allocated to the shared buffers which is a memory area so it contains the pages that are either being modified or read so let's say you are you are running a query like select star from employee where ID equal to 10 so that record or total would be usually stored would be stored in a page which could be of size 8 kilobyte if it's a default block size and now that page if it's not found in memory which is shared buffers it needs to be fetched from disk so that page will be loaded from disk to memory likewise if a write happens right if you update a record or if you delete a record or if you insert it's a right to a page and that's nothing but dirtying a page or modifying a page and even that right happens in shared buffers right so it uses the least recently used algorithm to flush are these pages from the shared buffers area so how to set it correctly we understood that yes I mean like all the databases you know this is the amount of RAM that's allocated to the Postgres specific memory and this is used for modified or read to store modified or reread buffers so the an important point that you need to know is that Postgres does not do direct IO right so what is what does it mean is it double crashing so let me you need to follow my words so that you understand it correctly and I could repeat if required so let's say I mean we will discussed about one of the examples so that when you go ahead and do a select star from employee where ID equal to 10 that record or tuple will be in a page which will either be on the disk right which of course will be on the disk and if that page was previously loaded and it's still in shared buffers right automatically that need not be loaded from the disk again right so consider the scenario 1 the page is already found in shared buffers then the user request will be served immediately but if that page is not found in memory the page that contains the tuple requested by the client is not found in memory which is shared buffers then Postgres would request the operating system to get that page from the disk right so now the operating system would fetch that page and you know store it in the cache and from there it's copied to shared buffers so in databases like Oracle on MySQL it is the database that would automatically go ahead and fetch it from the data files and load it to you know I mean in Maya scale you have in ODB buffer pool in Oracle you have database buffer cache right in as part of SGA so automatically those databases take care of that but Postgres would rather request the operating system to fetch that page from the disk and OS would fetch it to memory and from there and we copy to share buffers right so for this reason you rather give more preference to the OS cashier why because let's say you have a database of size 100 GB and you have a ram of size 64gb and all your hard data when I say hard data or active data set the data that on which your all your application connections or the transactions are running right so that data set is more than the total ram like 64 gb let's say now if you go ahead and set shared by first two let's say you know some 32 GB 50% of 64 GB which is the available trolled Ram so you have a ram of size 64 gb and you set your shared refers to 50% nothing but 32 gb when you set it to 50% so that 32 gb can accommodate some of the pages but the likelihood of the same pages being in the operating system as well is pretty high so you're not allowing the database server to cache more rather because of this double caching if you go ahead and reduce the amount of shared buffer size right is I mean rather you reduce the amount of shared buffer size to somewhere around like let's say 25 percent of 64 64 GB right which is nothing but 16 GB then you're giving more room to the operating system to store more right so cache more so that way you are letting the database server cache more and that's actually an advantage right so you are letting more number of pages we cached so for that reason you know earlier we used to see a magic number like 8 gigs or you know sometimes people say 25% of total ram needs to be assigned to shared but first which is good right that's the place to start however you also need to make sure that you do your benchmarking well for your application load and even your performance testing because you know sometimes even 80 percent or 75 percent of the total RAM could also do good because if your active data set can fit all that area right let's say you said you have a ram of size let's say 64gb again and you are able to set your shared refers to 48 gigs considering the activity you know that's happening in your database no sort memory required and all that is great right you may be able to get a great performance with that as well right because your active data set is able to fit in that shared buffer area so you may really get a great performance but you need to do that testing so do not just go with the assumptions that 25 percent of RAM or a GB magic number is the only value to start with you may do your benchmarking and give up to a bigger number in order to see what's in your shared buffers you could use the extension PG buffer cache and that's available as part of the contrib module so that would tell you what amount I mean how much our amount of a table is actually stored in shared buffers you also have an extension called PG stat statements that shows how many blocks have been fetched from memory which is blocks hid and how many blocks have been read from disk or OS right so that way you know what is the percentage of the cache a hit right so you also have PG stat IO views which actually gives you more information so you know for that reason like you could you could of course I mean first good and use PG stat statements which is of course some one of the default extensions observed in most of the production environments because that exposes a lot of information but that does expose great information about what is that SQL that's taking more time to complete and at the same time more amount of pages are fetched from disk rather than memory so you may need to tune that or you may need to set session level some settings in order to see if there is something that you could tune or yeah so yeah these are the PG start io views that I mentioned so you could also use these views to see what are those tables that are mostly fetched mostly cast or indexed or serve mostly you know to the cache you could always run explain analyze buffers and the query right and when you run that that actually tells you how much amount of you know that actually gives you block level information and even the cost at each node so that would also help you determine the shed buffers in some cases so the mistaken parameter to work man so you may we may have heard about word name a lot but if it's new for you I would of course explain it so this is the amount of memory which is allocated for each sought operation like when you do simple I mean in simple terms when you do an order by or a distinct it has to perform a shot and even more joints and when hash phrased processing is you know required so it defaults to four megabytes a query could do more than one sort right there could be parallel sort of operation so each sort get so much of RAM allocated right so if it's a complex query it may require parallel sort but how to set it correctly is for and be fine because in some of the environments you may increase your workmen and you see that the query is performing more faster why because you need to understand whether you give preference to more temp files or memory because anything that's written to disk is very very less in performance when compared to you know the sorting that's happening in memory right so for that reason if you rather allocate more memory you would be able to go ahead and get a great performance however can you go ahead and set a default I mean can you set a global workman value 200 megabytes or 500 megabytes can you do that when you do that each sort by each query in so many parallel connections right and if it's doing parallel sorts understand how much of memory it could you know code and utilize because it is the default memory that's assigned automatically to every sort whatever value you set if you set it to hundred and B each sort is assign hundred and B so for that reason you could go ahead and set session level as well so only for that session and in order to determine whether a session level parameter for that query could be helpful you could run again explain analyze buffers query and you see if there is some rights that are happening to the disk for sorting and you could increase your workmen accordingly so session level workmen allocates so much RAM only for that session so basically this would avoid out of memory like situations right so one other hint that I would say is if you have a if you know how many active concurrent connections could be established during a peak so number of active concurrent connections during peak times average sort operations for each connection right because I said a query could also do parallel sorts right times workmen whatever value you set it should be much much lesser than total Ram - the shared buffers that you have assigned and maintenance work mem times or the vacuum max workers so I'll explain maintenance workmen are no other vacuum max workers in a bit but you know when you when you go through all those parameters as well this formula will be very very clear for you effective caches size so this is the parameter that's only used for estimation purposes by the and it's an assumption on the available cash for each query so the default is for gay right but when you set it to truly such a low value optimizer would automatically prefer sequence scans over index scans so that could be one of the reasons why you see you know queries automatically showing like in the execution plan it shows a sequence plan and it could rather use an index code and see if your effective cash your size is set to a bigger value right if you have faster disks and indexes that are always cast you could set it to 70% or even more right and do remember that it does not reserve any memory you're just telling the optimizer that you have so much of RAM available for caching and that's only for estimate that's only used for estimation purposes so there is no harm in increasing this to a bigger percentage right so 4gb then you have more ram in your database server may be very very less auto vacuum again as I discussed in the in the first slide this is the parameter imeem this me this should never be set to OFF it's default it defaults to on so this is the process that's responsible for spawning worker processes which take care of either vacuum or analyze okay so vacuum could also I mean if a vacuum has got a lot of responsibilities it's not just cleaning up the dead tuples right it's not just cleaning up deductibles it's also like it responsible for free using the exciting of tuples right updating the free space map and you know it even updates statistics of table that are used by planner the analyze right and how to set it correctly just never set it to off unless you know why you're doing that right unless you know the complications just select the default on B you know over there so it requires you to write your own manual vacuum logic which is of course difficult to hack unless you can write try manual vacuuming instead of setting it to off track counts again it defaults to on right so we discussed sometime back that auto vacuum is responsible for running a vacuum or analyze on tables however in order to determine what table should be the next in the queue for a vacuum or an analyze so that a worker process can be assigned to it auto Hakeem depends on the statistics such as how many inserts updates and deletes have happened and that's only track when track counts is set to on so never said this to off right it disables some of the functionalities of auto vacuum or a vacuum vacuum scale factor and vacuum threshold now we are getting into the internals of vacuum and I hope you would really understand some of the important concepts here so auto vacuum vacuum scale factor order vacuum vacuum threshold so when does a vacuum an auto vacuum vacuum start on a table it starts on a table when the following formula is satisfied you know in that case the table falls into the queue so like we discussed when track counts is set to on right updates and deletes and insert subtract so the number of updates and deletes if that is equal to if the number of updates and deletes on a table is equal to or the vacuum vacuum scale factor times total number of tuples plus all the vacuum vacuum threshold if this is successful in that case right you know the table falls into the queue so vacuum scale factor is usually set to zero point two by default which means 20 percent of the table records right so it's nothing but the fraction of the table records that will be added to the formula and likewise vacuum threshold it defaults to 50 the minimum number of obsolete records needed to trigger an order vacuum so for example if your vacuum scale factor is set to 0.2 and if the total number of tuples is 1000 and vacuum threshold is set to 50 then 0.2 times 1000 plus 50 which is 250 for every 250 updates and deletes this table will become a candidate for or a vacuum vacuum right any table with you know which has got 1000 triples if it's got 250 updates and deletes it will be a candidate for an order vacuum vacuum likewise autum acumen analyze scale factor and analyze threshold so what do these parameter means again an auto vacuum analyze which is required to update the statistics of a table because these statistics are used by the optimizer in order to prepare the best execution plan which is very low in cost right so for that purpose it has to use the statistics and table statistics should be update updated very often and when does that happen when does a table fall into the order vacuum queue for an analyze and that happens when the total number of inserts plus updates plus deletes on a table is equal to analyze scale factor times total number of tuples plus analyze threshold so what is the scale factor again similar to the vacuum scale factor but it's set to 0.1 by default you could change it to zero point zero one zero point I mean I see it you know people changing it making modifications we'll see that in a while but ten percent of table records by default and threshold again is set to 50 like auto vacuum vacuum threshold even outer vacuum analyze threshold is set to 50 so if auto vacuum analyze scale factor is 0.1 and total number of tuples is 1000 and trash hold is 50 for every 150 inserts plus updates plus deletes we see the table as a candidate for an automatic analyze so this is this is ow a table falls into the queue for vacuum or analyze now let's actually get into much more important settings so we see that these parameters are actually global settings so are these global settings good enough for us right how to set it correctly so global settings alone may not be appropriate because it ignores the size of the table right even if in case our table is very tiny or very huge the same settings are applicable so consider two tables with ten records and a million records the frequency at which a vacuum or an analyze happens would be much more for 10 records right because 0.2 times 10 plus 50 when compared with 0.2 times million plus 50 it takes a while for the table with million records to fall into the queue but the table with 10 records on that table a vacuum keep on happening right so in in that case we need to go ahead and set table level or a vacuum settings for select the tables right you could also go and say yeah I rather do not worry about the scale factor just for that table I rather set a vacuum threshold so more than 1000 more than 100 or more than 10,000 updates and deletes let a vacuum run on this table right so you need to rely on table level on a vacuum settings in that case and now we start we spoke about or like you vacuum and analyze tables falling into the queue but how many water IQs can happen and can run at the same time it depends on the parameter or to vacuum acts workers it's set to 3 by default so they cannot be more than these many automatic Yume's that are running by default right I mean automatically about Postgres so now let's talk about Auto right Kim IO because and also talk about Ottawa q-max workers much more in detail right so auto vacuum IO parameters so what about the cost of our vacuum so consider that we are talking about the default block size so Auto Accu means 8 kilobytes pages by default right because if the block size is set to 8 KB from a table you know and it modifies or rights to those pages those contains the data bolts right so basically Auto IQ also has to perform a right on a page which could be of size 8 KB which is the default so it involves both read as well as write i/o right so now let's look at some of the auto racking io parameters right auto vacuum vacuum cost limit this is the total cost limit the auto vacuum could reach combined by all order vacuum jobs so we discussed that there are three auto vacuum workers by default so combined by all the auto rack you what is the unit right what is the cost that it could actually reach so that limit is set by this parameter likewise if the cost limit is reached how much time should the auto vacuum sleep and that is decided by vacuum cost delay and we discussed about auto vacuum performing both read and write i/o so what is the cost of Ottawa queue reading a page that's already in share buffers it is vacuum cost page hit and the cost of fetching a pace that's not in shared buffers is vacuum cost page miss and the cost of writing to a page when that tuples are found it is a vacuum cost page dirty so now look at the default values for the auto Akamai o parameters so the default auto vacuum vacuum cost limit is minus 1 which means it defaults to vacuum cost limit which is 200 200 units okay and the cost delay when this 200 cautiously reached Ottawa Hume sleeps for 20 minutes milliseconds and wakes up and the cost page hit so the cost of reading the page from share buffers is 1 when not in shared buffers the cost of reading is 10 and the cost of modifying the page when difficult are found by order vacuum is 20 so what can happen in one second one second is equal to 1,000 milliseconds so how much time can order like you sleep considering the best disc right 20 milliseconds so it's nothing but when there is a really no latency 50 times 150 times 20 milliseconds 50 times 20 is nothing but 1,000 so an auto IQ mccann wake up and go for sleep like 50 times right so it's nothing but 50 times auto vacuum vacuum cost delay now talk about the read i/o and write are your limitations if all the pages with the triples are found in shared buffers in every wake up to hundred pages can be read right so in one second 50 times 200 by vacuum cost page hit so if all the blocks or the pages are found in shared buffers the amount of i/o redial autorad comb could do could be around seventy eight point one three megabytes per second and if the pages are not found in shared buffers its 200 divided by vacuum cost page miss which is 10 so Auto rakion could read seven point eight one megabytes per second and in order to write considering vacuum cost which dirty set to 20 at the most auto rack you can write or dirty three point nine megabytes per second so all these three default automatic um workers in parallel together could read seventy eight point one three megabytes per second or seven point eight one megabytes per second or could write to three point megabytes per second so now is just increasing Auto a q-max workers help us because most of the times we go and increase that to five or six or eight so that more Auto vacuums could run but that could damage a lot of things because all that cost is shared right so now you will also need to go ahead and increase the cost limit and consider you need to consider that you also have you need to have a faster disk and you are able to allocate some portion of the disk just for auto vacuum and you need to choose a disk that has got that could do more i/o right and likewise you could also go ahead and look at the total number of databases that you have and also increase the vacuum cost limit considering all that we discussed and also when you go ahead and increase the order like you max workers it requires increasing the cost limit like you know in the same proportion and so if you increase the cost limit or mm it means 781 megabytes of reads per second approximately from shared buffers right so you need to consider these approximations at least in order to go ahead and see what are those order like um vacuum settings that you could really put in order to find you in your database and all these I ops that you see overall are all shared across all the order vacuuming processes but it's not per order rakia and finally maintenance workmen so this is the amount of memory which is used by maintenance operations that just create index out to table at foreign key especially when you are dumping a I mean you've taken a logical database dump using PG dump and you wanted to restore it using PG restore now it also has to create indexes right so that restore time can be improved when you increase your maintenance work man right so likewise even auto IQ when auto vacuum max I mean work mem is set to minus one it defaults to maintenance work ma'am amount of memory for auto vacuum however it doesn't mean that that you could use all of that right there is a hard limit within the source I mean sometime back I I mean while I would just what I was discussing with a community member is like like heard it has 1gb but I definitely need to go and check it but consider that you know this is the limit or this is the amount of memory that you're also signing to order vacuum when you're Auto racking workman is left 2 minus 1 so you rather set Auto vacuum work mem to fight 12 megabytes let's say as an example and you're limiting the memory of auto rank you work a process to so much right else when it's set to minus 1 it automatically I mean order vacuum work my automatic Lea defaults to maintenance work so you need to set both appropriately and also like in the beginning we discussed number of concurrent maintenance operations during peak times maintenance work now right so it could be indexed like maintenance operations could be reindex or you know similar operations which actually use maintenance workmen and that should be much lesser than the total ram - shared buffers and the work ma'am x max concurrent connections times n which is the parallel right like n is the average parallel sort operations for each connection so you could go over these slides again this will be shared so it because it makes sense when you actually digest it so we'll just discuss very few ignored parameters as well random page cost what does this parameter mean it's a frequently ignored setting it sets the cost of an on sequentially fetched disk page especially indexes are considered to be non sequentially fetch disk page so the default cost is set to 4 while sequence page cost is set to 1 so it affects the planner decision because your query planner the optimizer could assume that the cost of using an index could be more right and recently I've seen even one of the situations were just sitting random page cost to you know almost equal to the sequence page cost has actually solved a lot of issues because almost all the queries we're getting into so many infinite loops and doing a sequence scan instead of index scan and index is also cached in memory right so because it's it's more tiny when compared to the whole table so an index may be less frequently preferred by the planner when this value is higher this could be you know set to 4 or such a huge value when you have a very very slow disk especially the you know like the olden server the old servers like hardest though but not the latest solid-state disk or flash tests that you see in general in the database service today so make sure that you do not ignore this and go ahead and see the value that it's set to and also check PG strata use the tables and see if there are more sequence scans over index scans for all the tables and see if that's a if that's you know if that really makes sense or no I mean in that case like you need to check this parameter this could be doing all that damage because nowadays SSDs are default right so it could help if it's set to same as the sequence space cost wall compression while doing benchmark right when we set it to on the wall compression to on it wrote half amount of wall segments to the disk especially when you are storing the wall segments on the same disk as the data directory especially maybe on AWS right when you deploy your servers or any cloud for that matter you may be you if you use the same disk for storing the writer head logs as well as the data you're writing a lot more and if your eye Ops is more right I options more you need to go ahead and see all compression can be set to on at the cost of CPU so it compresses a full-page image written to a wall right so we'll talk about full page in the next parameter but so yeah when a checkpoint happens where a checkpoint happens after the checkpoint every first modification to that page will be a full page right to the right a headlock right and every subsequent page will be just good you could simply consider it as a change vector or a world record but it will not be a full page right so these full pages this these full page rights will be compressed right so how to set it correctly so storing the full page in which guarantees that the page can be correctly restored so you should not be disabling the full page rights unless you know why you're doing that and you're confident about it so for that reason you could set it to Han so that it reduces the i/o at an experience of CPU and you also need to see if your replica is catching up your streaming applica the standby is catching up because it has to uncompress and apply the wall segment so in one of the benchmark we did 3000 I mean 3.3 terabyte gigs of walls for 10 hours decreased to 1.9 terabyte right when it's set to on it could help checkpoint timeout right so it defaults to 5 minutes so checkpoint timeouts defaults to 5 minutes a checkpoint is forced every checkpoint timeout amount of time right and frequent checkpoint thing we might have heard about this lot of time seen at several places that are on blogs or everywhere and discussions that frequent checkpointing could be a problem why because after the checkpoint every first change to the page will be a full page right right so so many full page rights could happen if you decrease the checkpoint time out to such a very less you know time like 5 minutes for that reason let's say you said it thirty minutes instead now if multiple changes have happened to the same page it will not be so many full page lights now to the writer headlock right you are decreasing the number of full page writes so that way you are also reducing the number of writes so even more volume of data is written to the Warlocks right when checkpoint intervals are less so how to set it correctly you could set it to 15 minutes or 30 minutes or one hour it depends on your workload you need to test it but when you set your checkpoint timeout to a bigger value it could increase the trash recovery time especially when your database server is crashed because now it has to apply all the committed transactions for a larger interval in order to bring the database to a consistent state right and shut down may take more time of course because now it has to make sure that all the I mean so many walled files need to be synced which are for an amount of 15 minutes or 30 minutes or one hour you could run a manual checkpoint to write and you should have shut down which could be more faster checkpoint completion target it's set to 0.5 so you are spreading the checkpoint across checkpoint completion target times check pound you know checkpoint timeout amount of time so if it's set to 0.5 and let's say your checkpoint timeout is set to 30 so when you are when your checkpoint completion target is set to 0.5 and checkpoint timeout is set to 30 minutes then checkpoint has got 15 minutes to apply you know all those work or finish all the work that it has to do you could rather increase that and let the checkpoint spread could increase it to 0.8 0.9 so you rather let that happen over 27 minutes so it'll be kind of a lazy write a work in the backend and it would be less IO in cancer when you give it a bigger window right and finally max Walt sighs so it is I mean it's not just the checkpoint time out of 15 minutes or 30 minutes or one hour or whatever time a checkpoint can also be issued when Max wall-sized amount of walls is about to reach right which defaults to one gig so you know whichever comes first checkpoint timeout or Maxwell sighs so how to set it correctly again so you could go ahead and set checkpoint warning right you could enable checkpoint warning to a much lesser value than checkpoint time or let's say your checkpoint time I'm out of set at 30 minutes you could set checkpoint warning of maybe 25 minutes and if a checkpoint is happening like you know 20 min every 25 minutes right automatically it'll be a warning information will be locked to the log file and if that's happening often you know that your Maxwell size is actually not helping you you need to increase that and or you also need to monitor how much of walls are really happening to the disk right and also though this is not a requirement for you to set max wall size to a bigger value like distributing the wall segments you need to also make sure that always try I mean it's not like making sure but you always try to distribute your wall segments to a different disc so that is to distribute the AI ops right wherever you have an opportunity to do that so that you're not writing wall segments is saying disk as the data directory right and frequent checkpoint indicates less Maxwell size so what else I should be looking at you also look at OS metrics like is your PG wall like the writer had log directory busy or lot of logging information is being written which is a key which is actually using a lot of high ops on the same disk the throttle CPU cache it drops swapping so you see all of these OS metrics and at the same time you also need to go ahead and see if you can if you our app is using a connection puller especially when it's going beyond a certain very I mean a few hundreds of transactions per second concurrently and also you need to see if your Kashian active data set could fit in your memory and you know see if shared buffers can be adjusted or if you need to fine-tune or you know if your servers and this I mean if your server is downsized or if you need to upgrade your server and also need to see sequence can versus index scan and see what are those queries that you need to tune also check what are those indexes that are unused or never scanned so that you could tune them I mean you could go ahead and drop those indexes because writing to those unused indexes again a performance bottleneck to because it it took it add some overhead including the amount of data that's who all locked also look at bloated tables and see how you could take that bloat away so that the tables are always you know healthy and your queries are running over a limited amount of a data set and partitioning bigger tables is one of the important aspect you could use the latest declarative partitioning features and see if you could move tables between table spaces which are spanned across different discs so that you could distribute the I ops right so all of these really help you other than just a tuning so now I would definitely go ahead and go through the questions just to make sure that yeah so I see I see some of the questions right so yes said before a preferences opposite from MySQL or even the way Oracle works I mean it's it's a little different I wouldn't say it's totally opposite so how much tuning is needed for Postgres hosted on RDS I think there is a way to choose your already as instance like a standard one or you know or tuned RDS instance so it depends on what RDS instance you tune but RDS does come up with some tune settings but you it's not that those settings will be perfectly well for every workload you may have to again relook into that because nothing can be so much automated right and but maybe it could be good too so you need to go ahead and see that still because you need to know how you have created the RDS instance is it safe to turn on PG stat statements are very high loaded DB like yes so when you start statements does not create so many entries for each query so there is a limited amount of like how many queries and what is the limit of queries that you want to have on PG start statements and it's just a counter like until you do a reset so I mean I've seen we just start statements extension enabled on truly critical production databases which thousands of TPS however you could test it again against performance instance enable that extension it does require a restart but usually it should not cause any damage but you could test that and implement how to check the OS caches that possible in some case yeah so talking about OS cache a there are some you know in the operating system level you could use you know some other third-party exchange tools that you could even download for free or even in the OS level you could see what are those that are cached in memory so something once in order to present and show that to make it easy like I've used something like V and touch I think yeah so in PG 11 or 12 the allocation of work - dynamic or static okay so it is dynamic yeah you could also set a session level just a set word mem - you know you could you could set workmen to 512 megabyte 1gb just for that session and once you disconnect that it's gone so it's it's dynamic right I mean it it can be good for your session so if I have 1 GB set and have hundred parallel sessions yeah so if you have 1 GB of workman set globally that's a damage you may need 100 times once we or 100 times 1gb times n if in case every session is performing parallel sart's right like queries in those sessions are doing parallel sort so you may go I mean your server may go out of memory so 1gb I mean it depends on how much RAM you of God of course you may have got even more than 700 gigs ram if you've gone with an r5 metal server let's say right 768 gb of ram is what you get so you could do that but you need to be very careful when you set a global 1 GB right it rather needs to be a session level and you need to know why you need to set 1 GB right you need to see how you can avoid that how you could limit that or if it's just for something really specific right so yeah okay I think yeah Bruce thank you Bruce you did respond to that and yes share buffers is always allocated in full at a service start so and so it ages has got a question that he faced on issue auto vacuum was running every 30 minutes on a table was restarting every 30 minutes to run a manual vacuum for that to stop is that a setting that should be tweaked to let out of a Q not quit the unfinished table okay so that's okay auto vacuum I mean I'm not sure of the version or the Postgres version that you have got or maybe if there is any such background job that's scheduled in your environment which is identifying tables with certain amount of debt tuples and automatically kicking off a vacuum on them that could be one of the causes but Postgres you know does like you know skip I mean it does skip the pages that it can't lock and it again depends on the post rest version that you're working on so yeah we are on PG 11 we saw that our i/o was consistently high even during low usage okay the query start for that table kept changing every 30 minutes okay it was present the whole day so yeah so looks like we talked about you know certain specific issues that way that you're getting into so that we can keep this conversation just generate for the moment however also look at all the settings that we discussed if you think that the auto IQ is actually causing some i/o just see if in case any of the auto vacuum IO level settings are aggressively set to to high values - I've seen that happening right and about the indexes yes you can use concurrently can the index can the indexes be vacuumed yeah when you go ahead and create index concurrently right I mean that's like recreating the entire intact index you don't need to vacuum that again and maybe some maintenance schedule for that table yeah you could you could schedule recreate I mean you don't have to recreate index s every 30 minutes unless there is a specific reason why you're doing that so auto vacuum is or a vacuum used in a database that has got no deletes updates or inserts I mean yeah I mean it will be it could be inactive I mean it could be on but if there are no transactions that are running in the database of course I mean it has got no work to do unless if there are any transactions being created and transaction IDs counter increasing and maybe if it has to run some freeze address the freezing may be but yeah I don't see a reason yeah so I think all of the questions are answered and at the same time you could always reach out to me on lincoin it's a Lindsay over to you absolutely great job ivy thank you so much for spending the time with us today Twilight attendees thank you so much for spending a piece of your day with us and we hope to see you on future webinars have a great day \n",
            "good evening my name is payal I am database administrator for a company called ami TI we are mainly a web consulting firm and on the database side of things we can solve with open source databases and meetings right now I've been focusing on post guess that's my you know handle feel free to give me any sort of feedback you have on this talk good or bad I promise you try to handle it as best as I can so since the talk is about essential post was tuning I just want to make something clear this is a topic that's often talked about in every Postgres meetup or conference ii would go to and there's usually either a three-yard tutorial talk or a linux tooling tutorial talk so it's hard to condense this tuning thing and talk about everything that involves teaming in a one-hour talk but there's the reason I wanted to do this is because a lot of the people especially newcomers and go Spurs that I talked to they often get lost in in the various parameters that exist in those threads so first was 10 can anybody tell me and said Jonathan because you would know how many parameters to tuned there are wrinkles burst and what's that oh that's that's really high okay yeah no it's lower than that it's 270 so we're getting there I think we added like 13 parameters between 9 6 and 10 so we are growing our configuration at an exponential speed but yeah so the morning the caveat that this talk is this is essential performance tuning there are things like vacuuming and prosperous or logging there there is still a lot of potential for performance tuning like obviously if you're going to log everything your performance might suffer if you're if your vacuum settings are off again you might suffer but I'm not going to cover all of those things I am going to mainly talk about the things that I notice often find myself touching and I'm setting up a client's database or migrating a database or upgrading to a new version so there's two levels to pulse words performance tuning the first one is obviously pulse width and the second one is the Linux kernel now it's not just the Linux kernel it's any operating system that you're running it on but I'm mainly going to talk about Linux because number one that's what I've been working on lately and number two there are a lot of parallels between Linux and other operating systems Windows not included so if you're using BSD chances are you have something that's problem that probably has a different name but is tuned and works in almost us in other way so we are going to look at both these levels but first we're gonna start with Postgres anybody who's used Postgres has probably touched one of the GU C's which is the closest thing to a global variable in Postgres this is an excerpt taken out from a mailing thread describes GU C's perfectly it's basically local variables that you can set either at the cluster level the database level the session level the user level or if you like to call it role and the role level and there are 270 tu C's in Postgres 10 but I'm only going to talk about the top seven or eight that I think are essential to pulses performance that you should be focusing on and the first one is max emissions by itself Maps connection seems like something that probably doesn't have to do with performance all that much if you have the number of resources you set up your connections to whichever value you see fit but it actually matters a lot especially as a supportive variable for other performance variables in the sense that hosts versus process-based it's not thread so the number of connections that are active at any time are going to use of the total resources that your system has so after a point there's a knee in the graph Varian after X connections X active connections at the same time anymore and your performances your true put is going to suffer your latency is going to increase and to determine that amount you basically just have to start low and if you feel like you really need to up the connections go up but do keep in mind so this is there is no such formula for SSDs but for HD DS this is the rough formula that you could use to determine how many max connections you can have do keep in mind that this is not the number of connections you should set it to if it can be lowered please settle or your your database is going to be happy and also keep in mind that these what we're talking about our active connections connections can have various statuses in Postgres so there's something called I don't know which is basically it's just sitting idle there was something called idle and transaction reading for i/o those connections don't count when it comes to how much resources are getting utilized it's only the actively running connections that that count what if I have an application that has distributed application and they're all making lots and lots of connections yes that's a great point so if you feel like after a point you can't really keep your connections low you really ought to have more connections there's pooling and so you can the most popular tool is PG bouncer there's also PG pool you can look them up in the post worse wiki I believe they have their own dedicated pages so if you really feel like the number of connections that you have are not enough you really need more you use this these pooling tools what they essentially do is they queue up the connections so they won't let all those act connections go to the post where straightaway they will make some of the connections queue up they will make them back and contrary to what you might think that if there are a thousand connections and they are all running at the same time in Postgres that will be faster if if your machine can run them as opposed to 500 connections running at the same time and a hundred waiting it so happens people have done benchmarks to suggest that queuing does not delay that entire process any more than running them at the same time bill so it is better for your system it is better for your database to to enable queuing if at all you feel like after a certain point you do have to have more connections yes I remember buying many years ago yeah the only the most common one is PG bouncer the second most popular one is PG pool but PG pool isn't just a pooling tool it is much more like it provides availability for high availability automated fail overs and other stuff around it and so basically if you just want pulling you would go with PG bouncer I think that's closest to it we go in then I can think of yeah so going on to shared buffers probably one of the most important things and one of the main things you're going to set if you are tuning your Postgres cluster it's basically a data cache it's going to cash in all the data pages that you need in Linux in post rest versions 39.4 post Chris didn't actually talk to the colonel and get those values that you see there which is maxima and all so if you if you were running I hope you're not but if you were running an older Postgres you would have to set it as possible you will have to alter one of those values or more now Max and min are basically determining the size of your page while all is basically telling you why would you use all is if you're running multiple clusters on the same machine so that's the only only time that you will have to change all recomendation yes why wouldn't you to change its so exactly I cannot remember off the top of my head but if you're running multiple clusters on the same machine there's it yeah I don't want to give you the what I think it is but I don't - what version are you running yeah this is only for pre 94 oftentimes if you ran prehnite yes there's nine odd words actually make the modifications all four I'm not sure how exactly deals with that but I'm sure it does because otherwise it would also feel like the previsions where if you tried if your mass was too small then the shared buffer spectrum or setting it's going to fail to start up and say hey you need to increase this fitting 94 onwards that doesn't happen so my guess is the same as yours that it has found a way to automatically change those and talk to the colonel how exactly is shared memory it is mainly used as a page cache so all the data that needs to be read in or right now so everything that Postgres needs to touch is is it is coming into your shared buffers and things that are not what what are the processes that are sharing that memory only Posterous yes Amy there are cities more than one processor yes a shared memory region well post-course is processed so every connection that's happening in Postgres is a process of its own oh yes Postgres is not thread based so every connection that you're making in hospice is a process and so all of those Postgres processes are sharing that buffer yes is that going to be true for eleven you just said Postgres is process based process not the red eleven parallelization but there's a problem so there there are better people to talk about that specifically but the essential nature of post press not being currently is going to change so there is so parallel parallel life Asian the way it's implemented 9 4 onwards on a basic level is you can choose to do it but by default you can eat it is off so even if you choose to do it it is going to paralyze one process but but the connections at the core level any new connection isn't going to be a separate thread so any new connection is still even in 11 going to be a process of its own now within that it can spawn threads but yeah the recommendation for shared buffers there's the usual recommendation is set it to 25% of your ram as long as you have around that's greater than an gig or set it to 1/3 or set it at 2/3 so if you if you google this you will find a very varied set of opinions on this yes I the last PG comp in New York somebody had said that the new thinking was that 1/4 is now the maximum recommended that I think that's wrong I'm gonna discuss why am i what is this 1/4 1/4 Oh laughing of the wrap oh so what the value set for Shea mother should be 1/4 of the wrap at that maybe maybe that person said that at maximum it should be that or it should be lower but it should if they said it shouldn't be higher I didn't agree with that because lately as your amount of RAM is increasing your machines are getting better there have been cases in quite a few cases where sitting at half the memory of 40% of the RAM as shared buffers has resulted in performance gain people have benchmarks up on Google you can feel free to look it up but still the conventional wisdom remains that set it to 25% if your if your data base is too busy or if you're too paranoid and you have too much RAM so to one-third yes alright so the ego by inflection points yes but again I I don't think that that's conventional wisdom yes but I don't think that's true and practice anymore yeah I think yeah because I think the car there were improvements they did the kernel of that yes yeah I have systems extremely large data warehouses where I have much more than eight and there's a reason for having it that high however even though it goes yes no you can even though it goes against conventional wisdom to set it high and many people in thoughts conferences are going to tell you never to set it beyond a certain percentage of the RAM or beyond a certain hard limit in spite of the performance that you might gain from a very high shared buffer setting you should take into account how much available total available around you have so even if your getting it too high and cool chris is performing better right but the amount of memory that's left out so you said 40 percent of the RAM to share buffers and 60 percent is left out but turns out shared buffers only caches in the pages the data pages not the operations that happen on your queries right so any any sorting and in hashing any any vacuuming work work memory related stuff it ha it all happens outside of the shared buffer shared buffer is basically your data page cache so anything so if you don't have available Ram aside from your shared buffers if you don't have enough of that for your actual query operations then your database is going to be slow so unless you have a very high amount of RAM stick with conventional wisdom off 25 to 30 percent maximum another reason for sticking middle or limit is if you are running it on a 32-bit system there is a hard limit of 2 to 2.5 gigabit give gigabytes and that has something to do with address space and yeah 32-bit running out of it what's a 32-bit system well sometimes you have it like specialized firmware you can have an extremely large shared buffers where you have extremely large ram but if your i/o throughput is limited then your database is going to be slower than what it would be with a smaller shared buffer and the reason for that would be the larger shared buffers you have the larger is the amount of data that Postgres will have to write to disk at checkpoint time and so obviously if you're writing too much that might cause latency spikes so again another reason to stick with conventional wisdom of 25 to 30 percent but in spite of all this there are cases especially on your machines where you will see a benefit of having a higher than recommended shared buffers value and how can you find that out there's a core extension called PG buffer cache so if you install Posterous with contract you will have this by default all you have to do is create an extension anybody use this before okay so this is a great great tool to determine exactly how many shared buffers you would need basically post follows an algorithm called top sweep algorithm any any usual cache will wear any hit will increase the counter of that page by one and one and one and so you have a popularity ranking of all the pages and so five being the most popular one being the least and so PT buffer cache can actually track which page has which popularity rating so you can actually query PT buffer cache to tell you how many how much of your shared buffer is being occupied by what class of popularity popular pages so all the pages that extremely popular are are taking up 90% of your shared buffers that's great but all any anything that has a popularity of three four or five that is reasonably popular they are only taking up like they're taking up more if they're taking up all of your shared buffers so that's likely that lack likely means that your shared buffers is really low because if it's already consuming all of your shared buffers there might be more data more popular data more popular pages that might fit in and buffers so if you can afford to you might want to up your shared buffers now one thing shared buffers is a hard allocation of memory that is said whenever Postgres is started so of course it requires a restart so you said it once it's not something you can tune on in the air so keep that in mind but yeah PT buffer cache is an excellent tool right one slight note of that is every time you create a PG buffer cache it takes a lock on your shared buffers not a huge deal but just don't run it in production during the highest little time yeah and close to shred buffers and perhaps even more important in certain cases is effective cache size which is the total ram available to Postgres so in most cases you have a dedicated server for post words right now and so you this this value basically will involve your shared buffers plus the free memory so you can just do a top on your system and get an idea of how much RAM you should allot to it half to three folds is the conventional recommendation but you can even if it's a dedicated system and you know that your database is going to be busy its broad and setting it to 90% is not unheard of it is unlike shared buffers which is a hard allocation of memory that Postgres will acquire at start time this is just basically an estimate so you're just telling post words this is how much you have available at your at your yeah at your service and it's possible that Postgres is not actually able to use that but that's not relevant for this setting so this is just to tell the post res planner hey use this index or hey use this slightly better thing that you're going to use as poster doing a conventional or a boring sequential scan or a heap join so yeah it's just an estimation math connections should be taken into account when you're doing this so if you have too many connections remember I mentioned that Mac's connections does affect the other performance parameters so this is one of those that it really affects if you have more connections you have more process processes if you have more Postgres processes depending on how complex each of those processes are it's possible for you to really be short around so I want to take into account what your Mac's connection is set to before you before you set your effective cache size yes for the effective cache I so like how do you calculate how much to put in them if you think Mac's connections push their buffers there's no hard-coded formula for that what I would do I would start with is I would see what total amount of RAM I have and set it to at least 80% and that's assuming I'm running it on a dedicated server weight but 80% but what if you take - what if you put your shared buffers to 2/3 yeah 80% is assuming my shared buffer is following the 25 to 30% limit yes if you're setting your shared buffer is higher than that so if you're setting it to 40% that's a territory where you cannot follow the conventional recommendation then you have to first take into account that aside from that data cache data page cache how complex are your query is going to be how many total connections are you going to have so do you still the 60% of Ram aside from shared buffers is it still enough to handle that load of sorting of performing other operations of maintenance operations is it still enough if it is then you still can go ahead and set it to 80% and also remember that this does include the amount that you set for your shared buffers out so include the shared buffers when you're setting effective cache size and are there any conventions about next connections like yes the recommendation is set it to as low as you need it to be don't set it to like don't start with connections of 300 if you know that at any point your app will only make 100 connections start with 110 150 and then go from there so keep it as low as you can what roughly was to shipping the whole than that one what's that what should be the foreman explanation is um it's uh it's ten oh it's 10 so it's fairly low yeah you're pointing if you're using it in production you're going to have to change it next up is work memory quite tricky people have been bitten by it but I hope by now it's well known that work mam is first of all it's the RAM that is used for sorting and hashing somewhere joints or hash joints and any other operations like those order by yeah the state all of those operations are going to be using your work memory the key thing to remember again max connections has to have has a role to play why because work memory is per session and per user and per query so it can be set on various levels so you can set it in the cluster at start time then you can change it while the cluster is running then you can change it man a particular role is running something you can also change it per session and why should you do that it's because work memory is per query if I so the default forward mem is 4 MB and that means that if my query if one of my query has 10 sorting operations let's say each of those sorting operations can acquire 4 MB of memory so if I have a hundred connections each with 10 sorting operations multiply that and that will be your actual memory getting used at any point in time and that's why people usually see oh right now is the mem that query can use for its operation let me set it to as high as I can what they don't take into account is that it's per operation per query so it can exponentially increase in a scary fashion and and out of memory errors are not unheard of if your workroom is too high one thing to note though is and this is not something that's mentioned mainly because people do tend to say that high if you're running it if you're running a low reporting database you might want to set it a little bit higher if you have a lower number of connections relatedly speaking so if you only have 10 active connections at any time in your reporting database and each of the connections is going to be fairly heavy right it's a it's an analytical database so you want to set up a higher work map depending on how much RAM you have so that your performance is better overall random page cost it's too high they're not changing the default and the reason for that is because HD DS are still around magnetic drives and yeah that's basically it but for any practical purposes even if you if you don't have it as DS you might still want to learn it you definitely want to lower this this is one parameter if you're benchmarking obviously you will be benchmarking before you to your production you will see that changing it from four to two leads to a significant performance difference overall I'm talking about thousands of TPS more than what it was on for and you can set it to 2 even even if you don't have all SSDs because this value is relative to sequential page cost which is one by default so it is basically the reason it is for is so that to tell phosphorus that a random scan like an index fetch is going to be 40 times 410 times more costly than a sequential scan in practice it's going to be costlier in Fitness hdds but Postgres assumes that some of that information some of that data page pages are already in your RAM so it's not actually yes so that's like really says the Fulda for is too damn high for assistance so sure it is still so because of the optimizations that have happened over the years in major prosperous versions index there are a lot of different types of index scans available and so I would certainly say that even if you're using HD DS and you're you have an adequate amount of RAM not like yeah 10 years ago yes basically in practice today either you have a large Ram organ or you have SSDs yes and that's several tablespace you can't change it for a tablespace but basically you can change it at the cluster level as well but you could if you knew you had some slow disk you can set it for that yes umm so good question I have had heated arguments with my colleagues over that because um I just go by the numbers I see in benchmarks and setting it to one maturity which is what I do leads to a performance difference in a positive index happy yes yes and in SSDs you basically there is no additional seat time so it makes sense to set it to one however paranoid people say that it depends on the quality of the SSD where a sequential scan might still be somehow better than than accessing several thousand pages randomly what if you know sort of a hybrid of like SSDs and hdds but you put the index is so useless we'll simply set it to the actual SSDs as long as your index and and the rows that your index pointing to is on SSDs I would say just go ahead with that right but then the random page saw so you should set it yes yes that's that's basically so if you have an HD HD D and an SSD and you have planned it well different tablespaces definitely do that one thing to keep in mind is this is not an integer value only its decimal so even if you are in that paranoid group that OS s DS can still be slower than index scans can still be slower you can always set it to one point two or one point 1 or 1 point 5 you don't exactly have to either do it one or two you can set it in the middle so that's something to keep in mind 32 core es boxes checkpointing is related to a lot of things in Postgres but strictly speaking from an essential performance point of view there's three parameters you need to focus on by default your checkpoint segments is has a value of 3 which basically means by default it's going to have 3 walk files each one file is 16 and B so that's 48 MB what it means is after every 48 MB of Rights of all files worth it's going to write to disk if you have a very heavy production that's going to be a lot of disk access that you probably can do without so you definitely want to increase that checkpoint segment to a higher value how high depends on your individual resources on your disk space as well because obviously if you increase that those more number of files are going to be on your disk another thing to keep in mind is for those of you that extremely paranoid the larger volume of all files that you have that are not written to disk it means the longer it will take cores can recover in case of a crash so that is something to keep in mind as well but still checkpoint segment default value of 3 is way too low your production is going to suffer immensely if you keep it that way so please increase it completion target it's got a complex type definition and the documentation what it is is basically saying once you have a significant amount of segments that is a good amount of wall files to be written to disk right you can span out the rights you don't have to write it all at once you can spread out your rights based on how long it takes for the next checkpoint item to arrive so by default this completion target is 50% which basically means by the time the next checkpoint the next wall file is 50% written complete all of these wall rights to disk so you the maximum you can have is 0.9 and you want to increase it to that assuming that you've also increased the checkpoint segment the only reason the default is still 0.5 is because the sake the checkpoint segments is still 3 and so if you have a little checkpoint segment even if your completion target is 90% of the next checkpoint it's really not that much time so nothing nothing is to be gained from that so I'm assuming that posters if it ever changes the defaults it will change both of these defaults at the same time or neither so you definitely want to increase these two also note that the default checkpoint timeout is 5 minutes so regardless of those 2 values every 5 minutes your database is going to write to this and so you probably want to increase that to yeah don't increase it too much because then obviously you're running into yeah disaster recovery longer disaster recovery times but yeah several several minutes and definitely higher than 5 should should be recommended again something extremely specific in certain cases Posterous does not have query planning hints right and so that makes this all the more important what what is the default stats target it is basically collecting statistics at regular intervals of each the tables so what kind of data does it have how long will it take to do a sequence can how long will it take to do an index scan and stats like those it's a sample of the table data basically if so depends on the sample size of each of those tables usually I think by default this is set to 10 so it bill in every table it's it's going to take in 10 random rules as a sample and then use that to analyze and and make its query plant stats so better plan versus long around a lifetime if you if you have a significantly varied column or our data in your table you want to have a higher stats target because 10 rows are probably not going to represent your 1 million worth of rows yes that's the 10 most common values found I'm not sure I think it's just random so this is different this sampling is different from the table sample option that Postgres provides the user with which has that own option address so yeah table sample is a user accessible sampling method where you have Bernoulli's theorem and you have the random and you have the popularity but this is something that happens internally in polls for us I'm not sure what algorithm but I would bet it is just basic random at random that I'm so sure because oftentimes even in cases where the data is largely varied it's only when it's it seldom would represent the data as a whole so it's it's still going to take take stuff at random and not not represent a pattern or not going to tell you that hey you might want to have a higher so this is different than if I like to look in the data dictionary for the for that index and it gives me like a set of the most common values no that's the same thing yeah yeah that that will be the same thing what it is different is there is a so if you want to manually sample data from a table you will use something called table sample function in close placement this is not that so this doesn't use that function that is user accessible so this happens internally at random okay so it's done internally - what is it used for yes I was coming to that I will have to explain it without the pointer in there it is used for query planning so when you do a select star from users where manager equals x post press planner is the one that decides whether X even if there's an index on X it should use the index or just sequence scan and the way decides that is by this process putting in the sample and the the statistics for that so that it's used for query planning and because post cruz does not allow for explicit query hints this is all the more important so off if you ever run into a situation which you someday will if you haven't already you're going to see that some things should use an index and it's just not using it and what you want to do one of the things watch if you don't want to investigate it further just you can increase the statistic target per table and you can increase it per column so for that particular column just try increasing it to a thousand or a hundred and then analyze reanalyze the table and run your explained plan again and I won't be surprised if it starts using them next time so that happens in cases where that particular problem column is actually very varied and Postgres doesn't know that because it's only getting the default ten rows and it just thinks that it's just the usual closely related to the previous parameters the stats temp directory it's ver it's a file resides in this directory basically that gathers that that stores in all the statistics that the previous parameter makes Postgres collect it contains the latest statistics and it is a good idea by default it lies in your PG data location but it is a good idea to place it on RAM so you all you have to do is change the path and why because if it's in RAM every time query planner is being used it doesn't have to go to this to ask hey what's the stats on this table what should I use so it's much faster that way one thing to note is just because it's in RAM doesn't mean it's definitely going to be volatile across restarts if your post presses properly shut down before it shuts down it's going to write to this right this father this if it is crashing then it's not going to write to this so if you're recovering from a crash it's a good idea to just analyze your database analyze often happens with vacuum but by itself it can be done as well and it's quite fast so it won't take you hours to analyze your database this is one of the parameters that that is actually not so popular in performance tuning and that's what I wanted to cover it because often you would just look at it and the Postgres config and just ignore it what it's used for is for complex functions especially recursion it depends on what the stack depth size of your kernel is which you can find out with the new limit function and the recommendation is keep it a few bits below what the what the actual limit is and if you keep it if you set it to higher than the actual limit which is not allowed in the latest post worst versions but if you have an old version and you said at that high your post verse is going to crash if a recursive function C and blue or angry yes yes that too so be very careful when increasing that but the default is again given the latest machines and most people the way they run and the resources we have the default of 2 MB is is quite conservative so check your stack that size and usually even if you are paranoid and set it to at least 4 so that way it's at least better than the default ioq all currency is basically how many spindles do you have for your i/o activity basically parallel access to your disk for our SSD so the available value is 1 to 1,000 by default it is 1 so you definitely want to increase it if you're running a raid type specification then it depends on what so it depends on a number of factors and you're better off instead of me trying to explain and you're better off reading it off the wiki but but there's a there's good information in the Vicky if you're if you have a raid type specification if you're running it on SSDs you can easily set it to 100 or even more up until a thousand the thing with this though I have noticed is beyond a certain point I haven't seen a performance game now there can be two reasons for that number one personally I haven't so I see a better throughput when I increase it from its default of 1 to 10 but then I go further I go well I said it 200 or or 500 I don't see a performance gain I don't see it decline either but I don't see a game so the one thing is maybe the scripts that I'm using for benchmarking the praise that I'm using are not something that this might be used for Postgres doesn't see it as beneficial the second thing is this is only used for bitmap heap scans right now this might change it might be used for other operations but right now it's only used for the fitness keep skin if you're writing to disk you can either turn full-page writes off and f sync off so that's going to increase your database performance right obviously it's not going to wait until it's sure if your data has been written but at what cost do you want your performance to increase so don't do that unless you believe in the yoga philosophy have it have it set to on F sync is on by default another thing to keep in mind is there is a parameter called walks wall sync method and it's only used if your f sync is by default on so in that case your wall sync method by default it points to F data sing on Linux which is basically a saying I want to say one thing that you might want to go back and check in your prot systems is what Linux kernel your production is on because there was a pretty significant bugs with axing how limits handles and sync and how Postgres thinks when it's handles acting that recently was patched 4.4 onwards so kernel version 4.4 is one I mean so you might want to upgrade that the latest kernel minor versions have that patched in there it was basically a problem with xing returning an error if there was a problem everything would return an error to post response but if there was a problem again AB sync wouldn't do anything it wouldn't go back and tell Postgres hey you're still having trouble and then posters would assume that if F sync hasn't talked back about the failure it's gone through the second time so that was a significant bug that it's not being patched this is to a couple things that are being passed around yes it's been passed in kernel yeah but not in Postgres I think I think some people's are in there's some more coming in the next release and then there's some more charges they're thinking about yeah ok so at least the kernel end you might want to to the latest version Postgres the first thing I want to talk about is out of memory the way minutes handles it is it behaves like a serial killer if it runs out of memory doesn't care what its kidding it will just kill it based on certain restrictions but those aren't set by default so do things you definitely want to do on your dedicated database system is set these to kernel parameters to disable than zero what it's it's fairly obvious what it's telling the kernel to do is basically don't panic if you run out of memory and do not kill any any tasks any process if you run out of memory let me handle it the user family and if you don't want to do those two things you're you believe in the kernel and the ability to kill processes you can exclude your Postgres process by giving post with a priority of - 17 and not your priority is the right one but giving you minus 17 is a number that minutes kernel will if it sees it associated with the process it's going to execute it from its killing spree and that's just real ice yeah what's that so and then scheduler so CFG is the latest one it stands for completely fair queuing what it is is basically a very smart scheduler that handles priorities that handles what should be given preference when you're writing out to this or reading from it it is not that to let it remain as the primary as the scheduler for you - kernel and running post press but the two other older more conservative schedulers are not bad options in fact I would definitely recommend one of the - mainly because the way see a few functions it's quite complex so if you if you run into an issue where you see that the scheduler is the reason that your your performance is suffering you really can't tell what see if he was actually doing a lot of it is the way it's designed it's complex it works well for certain processes not for databases nope is your friend first-in first-out if you don't know what your database is going to be like if you're not sure if you know how to work with schedulers just make sure it's set to new deadlines is basically for read intensive databases so if you have more reads than writes deadlines scheduler is going to prioritize reads over writes so it's going to make sure that every read that's being requested is processed immediately while it's going to give right to lower priority in the sense that it's going to accumulate a certain number of writes before processing it to this so yeah that line is good for you'd only or read heavy assistance huge pages is a controversial thing mainly because some people say it should be there some people say just turn it off as soon as you decide to set something as your database server what you huge pages is is basically instead of having to keep every address point address location and memory it will point to one of those location and and and make a virtual page that's larger than your actual disk page size so what size your kernel has as a huge page is the last two lines of that code you can create mem info for that and then what should you set huge pages to you find out by finding first of all the postmaster the main post press process that's running on the system you query it's it's prop status you see what the VM peak value is which is basically how much memory it is using and you divide that but they'll colonel huge fete size setting so in this case it's going to come to like 3100 something so that's what you can set your NR underscore huge pages to one thing to keep in mind is if you set huge pages to something that's lower than what Postgres needs or Postgres wants to use a post-race will fail to start so you want to set it very carefully but ninety four onwards there's an option in Postgres config parameter that lets you set whether or not you want to use pages use huge pages and the default value for that is tri so that's great because now Postgres would no longer fail even if the value in the kernel that you've said is too low Postgres is spoon and try to use it if it can it's going to go back to not using huge page if you're using bsd there is something very similar to this it's called super pages the concept is almost the same one issue that people complain about and and and they'll tell you to stay clear of huge pages is a defragmentation issue which existed in older Linux kernel versions it's been patched to an extent now and not that I haven't seen issues with it but I cannot guarantee that you're not going to run into what what happens is that due to the way the kernel D fragments your your total data pages it basically breaks down the Postgres buffers buffer allocate buffer blocks in a way that pools misses I'm able to deal with the blocks blocks all the blocks of data that it wants to at one time so post race is going to have its going to stop a lot it's going to have a lot of latency spikes and if you're suffering through that and you're on an older kernel definitely try checking if your huge pages is on or off and set it too often people have seen a huge improvement just by doing that now this is a very interesting section that I want to talk about because I spit in my head now most kernels of your overcoming memory which is a parameter for your virtual memory management in minutes is by default zero zero would mean that you're telling your letting the Linux kernel decide how much memory to overcommit how much let it decide become the on the basis of how much memory it has at any any time how much free memory it has if you're setting it to one it means that always overcome it you don't want Linux to heuristic aliy these decide whether or not it's good to overcome it you just want to tell then it's just give it what what every process asks that's useful then you have sparse arrays and and and for scientific computations where you really don't care about out of memory errors as much as you care about just high performance don't set it to one for databases though for databases appropriate value the recommended values to which disables overcome it all together if you're running your database on a dedicated system you do not want it to over come in because that's the main post process the main process that's running why do you want to chance any other process running in your server and and just taking up or asking for more memory so just disabling that's what you will end up doing but if you're setting this to two and just you're looking to do this these are the two ways spun on the New York kernel versions you can just use the 6 CTL command on the older ones just use echo but if you're setting it to 2 which you should for a production database dedicated server do look at the overcome it ratio and this is what I got bitten by basically if you have your overcoming memory set to 2 it is very likely you're only using half of your RAM your database is only using half of your RAM and that's just as bad as it sounds this is because the default for overcoming ratio is 50 percent of the total ram and that is because the default for over memory is one which is it can overcome it is allowed to overcome it but when you disable overcome it in memory and then the ratio be 50% of the total Ram guess what it's only going to use 50% of the total ram and so I got bitten by it because I was migrating a database server and the new box had lower round than the old one and basically it resulted in out of memory errors I debug then first tried to reduce the work memory because I thought maybe your queries are running in there but eventually it wasn't the work memory at all it was just the fact that it was only using 50% of the RAM and the RAM on the newer machine was much lower than what it was on the older one and so yeah it wasn't enough definite and my my one complaint point from poster of posters documentation is that in its kernel tuning page dock it does mention over commit ratio but in the context that if you are changing overcometh memory you might want to look at the overcometh ratio I don't think that does justice to exactly how important this parameter is especially given that how many places and how many posters people would tell you to disable your overcome in memory but this aspect is really not well known at all and I've talked to people that previous conferences and I was surprised that this is just not not well known so definitely if you if you have disabled over commit memory or you know that your DBA has definitely tell your need you to check this finally there's sloppiness again what - what - keep in ROM want to transfer to disk the values can be one two hundred think of it as a percent I think the default is forty forty percent yeah if you have a larger RAM which these days most people do you want to lower this for your database so because you already have available around you don't want your Colonel to talk too much unnecessarily I think that if you have a good enough round for your production just set it to zero or if you're too paranoid set it to 10 no more than that make sure in your file system which other file system you're using the two file system parameters know a time and no one to you dir a time are set what that does is basically Postgres the way PT data is structurally set up is that each table in your database is a file in the file system and every time you touch a table that file gets accessed what a time does is records the last axis of every table so you can imagine in a busy system the operating system will spend way too much energy trying to update that last accessed for each table so you don't want that to happen no dir a time is basically for directories especially when your only pair using the directories and not actually opening any file in them so you want both these parameters set or rather a time disabled in your file system so yes yes whichever does your PD data resides on make sure that said and that alone leads to a significant performance improvement and a busy system one tip it is often times you want to upgrade your Linux kernel but your Postgres package is also going to get upgraded so there are ways both in sento has sat on a bun tool and this is a bunch of to pin your package or to priests package and send to us basically there's a file you can specify in the file hey this is the pin version and the second line of the file nine four six in this case so the next time you do a apt-get update minutes will update every other package but not touch that because that is pinned so if you are holding out on an important kernel update just because you don't want your database to get restarted there's a way out for that another tidbit and this time it's you know as related is so it unless clyde commands especially commands like sync anybody uses those okay yeah so they are optimized for its SSDs basically and this one time I was using a using sync to transfer database backup from a magnetic drive on ec2 to s3 and it was extremely slow and the network wasn't a bottleneck it turned out to be the sync command because by default the AWS fly tool configuration has a concurrent request of 10 which is basically 10 parallel accesses to the disk at the same time and yeah for a magnetic disk drive that was way too much so make sure you change that if you have a magnetic Drive saying that the bandwidth by default bandwidth is none so usually you shouldn't have a problem with that but if it's set to something else and you're seeing that the network is a bottleneck and it's only a bottleneck for this a tableau is quite command you might want to check this parameter finally the benchmarking is the thing that you want to do to set each of those and not just follow the recommendations lightly PG bench is a great tool not many people might know this but PG bent allows for user specified files for benchmarking so like if by default if you're on PG bench it's going to run its own set off reads and write query right but if you have very specific queries on your database you know which queries are gonna run more frequently you can actually put them in a file put them in a transaction block and feed them to PG bench you can heat PG bench multiple files multiple transactions within multiple files and it goes taken off and you can tell it how dispersed on how exactly to distribute it how random it should make it and the number of concur and request that it's making to the database do it for a long amount of time don't just do it for a minute parameter change because that's not going to be reliable your production is going to run for several months hopefully several years so make sure at least running for a couple of hours to a day or two ideally make sure you only change one parameter at a time if you're changing two or three obviously you're never going to be able to decipher which parameter there's that led to the performance increase or decrease the best way is to automate that so give it a certain set of parameters and certain set of values for each parameters and let it run over over the weekend or a specified amount of time for each after you change finally I wanted to touch this so when to tune your post requestor firstly when you're staying up there new database when you're migrating it to a new box or you're going to a new major version of course rare but new settings are added and then the next most common thing is you notice a slow query or a query that previously performed well not performing that well and then you look at your tuning and you and you try to see what's causing the problem so essentially the first three are kind of static tuning and the second one is reactive you're reacting to a problem what's what what can be next what can be another type of tuning so there can be on-demand tuning city there have been discussions where let's say you're using PD benchmark or any-any background process that runs that that is reading your log files or reading your system top or vmstat and processing those values and it will come to know oh my system is under heavy load I need to change certain parameters and it can change those so that's on demand tuning predictive tuning is when you already have a lot of data about the system about the database you have a monitoring tool and in place so that has collective data worth of years worth and so what you can do is feed it to ml not locking ml and you can use that to kind of predict let's say on a Black Friday event Christmas is approaching and you don't want to let the system make the change itself you want it to let the DBA or the developer know because it can get scary really fast but that's that's eventually if that can be done well that's going to be ideal it's not too hard to if you already have system data that io data it's not too hard to feed it into a neural net get the values get a graph out of it see if you can find a pattern so at the very least you're going to know what to expect a certain time of the year a certain time of the day yeah these two aside from the extensive information available online these two are probably the most used questions and feel free to send me a feedback thank you [Applause] \n",
            "hello and welcome to my talk of how to scale postgres today we'll talk about automation tuning and sharding my name is lucas now how i kind of you know came to think about this topic is i've spent you know a lot of time with postgres over the years i run a product called pg analyze which does postgres performance monitoring um i am a member of the situs data as well as the azure database for postgres teams where we're looking at you know how to operate postgres in the cloud both on a single server and as well as you know kind of scaling out and then i also have just you know a lot of general interest in working with kind of postcards community and making sure that postgres is you know a good database for all kinds of workloads be it you know small and large now when we think about scaling postgres i would say you know we it's kind of a step-by-step process right so oftentimes you kind of start small right kind of the first kind of square there um and then you know it just starts getting bigger right and you start kind of increasing capacity right and so often times a lot of folks i think will see that they have you know a small database and then a slightly larger database you know at some points very large database and still running on a single node i think that's you know very common situation for many of us second of all um i think you know many of us also especially in bigger companies experience the situation where you just have so many services right like it's just like you start with one and then there's another team and then you have like three and then you know there's another team you have nine and then just like start sprawling right and you have so much so many different kind of postcard servers that it's getting very difficult to to get consistency across them and so that's kind of the first thing i want to talk about is how do we actually handle you know hundreds of database servers and so there is i think a lot we could say here i think the really the main point i want to make initially is that consistency is key right so i think we need to make sure that when we have this many databases right when we we look at hundreds of servers that we apply consistently a certain way of how things are configured we make sure that you know uh the the servers are not just ad hoc changed like somebody doesn't just go into a portal and change the value and then you know that opens up a security issue or performance problem like things need to kind of you know be done consistently now the best way to do this and you know this has been popular for many years now but i think you know it still so so important to think about this is that we treat infrastructure as code right so you shouldn't spin up a virtual machine manually you shouldn't you know create a database really what you should be doing is you change a file you commit that file into a repository and then that change gets rolled out the same way we roll out software and so that's why i would really say you know we need to think of postgres as well in the same sense right so we really need to think about postgres infrastructure as code now let's take a look at how that works in practice so terraform is very popular these days right it's a straightforward way to kind of you know configure a cloud resource usually so let's take a look how it works so we'll actually you know as an example we'll try to modify parameters on our azure service let's kind of take a step back here actually um so we have two um kind of ash database for postgres servers here um number one number two um and so we'll kind of you know try to make a parameter modification here right and so usually when i kind of go here i could you know just change the sort of parameter right pretty straightforward change um i could just you know do it directly in a portal and so for this example let's say you want to change the vacuum cost delay so the vacuum cost delay by default is 20 milliseconds and we want to change that to 2 milliseconds because we heard it's a good change to make now i could change it right here but actually what i'm going to do is i'm going to do this for the cli using terraform so as you can see here right i have kind of you know directory locally on my machine i have an ash.tf file which kind of defines the terraform state now in this you know configuration file we have you know the azure provider that we're kind of referencing and then we're using a variable called postgres config to set the auto vacuum cost delay right so you can see here kind of in the middle i have my other vacuum cost delay set to 20 and i had you know applied that earlier now what i want to do is i want to make that change here too right so let me go right here there we are um all right so i'm making that change here and now i'm running terraform apply right so this i'm already authenticated locally and now with terraform apply i'm essentially telling you know terraform to go call the relevant apis and apply this configuration change consistently to both servers and you know terraform now kind of gives me a summary says yes you know these two values need to be replaced will you know kind of apply that and so you know the the key benefit here is that if i you know use like for example i can put this in version control i could you know then do a pull request somebody could review this change somebody could say well actually you know you shouldn't be changing this to two um in our experience you know this doesn't run well on small servers um so you know we should kind of you know maybe split up this configuration make it you know 10 for certain servers to for other servers right and so that kind of conversation that kind of you know change management is really what's enabled by treating infrastructure as code and so you know whilst this is running we can perfect now we can switch back um let's look at the portal again let's reload this here and so now we should actually change see a change applied right so now if we look again at the parameter list we can see that the cost delay is now changed to two right and again that is done consistently if we had a third server we could apply it in the exact same way and there's no risk that somebody does that by accident all right um let's kind of go back here so you know i think this is a really important building block right and so synchronized configuration is one thing now if you find yourself lucky in a situation where you know you use database as a service that runs in the cloud which you may or may not choose to do but let's say you do right i think there's many benefits you can get from that right so access control is taken care of you could also use terraform again to kind of manage that automated failover which i think is a key key point for automation is taken care of with the kind of database as a service providers read replicas are you know usually taken care of or right there built in backups are built in and i think the one thing where a lot of providers are still lacking including azure is kind of that connection pooling is usually not built in right so usually connection pooling is something where you need to install pj pals for yourself and you kind of need to find your own automation for that now what i want to talk about here though is if you run your own vm right so if you don't choose to use database as a service but you actually choose to do a self-managed vm that's where it gets tricky because a lot of these things are not taken care of right and so you quickly end up with a lot of like badly managed kind of virtual machines that are not consistent and so we could be talking about all of these but we don't have time for that um however we will talk about i think one really key project that we've actually initiated uh two years ago um called pg auto failover now pg auto failover solves the very specific problem of having automated fail or between two nodes and postgres so let's take a look at that right so it's really it's focused on being simple i think that's really the key point here um and so let's say you have a primary and a secondary right like typical high availability situation so the primary server is the one you're writing to the secondary is usually just used you know in emergencies like to switch over but it's not actually used for reads um it has a second copy of the full data and then in the pg auto failure setup there's actually a third node called the monitor node now the role of the monitor node is to enable safe automated failover in the case of a failure right so let's say the primary fails now how would you know that the failure is not a network partition right like it's not like let's say you know secondary and primary can't talk to each other anymore the secondary would not be able to choose on its own to become primary because maybe there's just a network blip essentially between the two the two servers right because that's where the monitor comes in because the monitor can actually um make a decision on behalf of kind of the the two servers right you can essentially say i see both of the servers or i don't see i don't see them and so the monitor is essentially the one that kind of you know makes that decision for failing over now let's take a look at how it actually works and so for this i'll um kind of uh switch to my little raspberry pi setup here um so we can kind of see here right we have data01 data02 and the monitor node um and so these are essentially you know like just you know simple linux vms running ubuntu linux i'm right now connected in my cli to the monitor node over here right so this is the one that messes aside into and so let me go here right um and so what we can do here now is well first of all this runs post scripts rights if we just do regular psa ux we can see kind of you know regular pulse which is running um nothing special about this here um and then now i can use the pg auto ctl cli so pg auto ctl is kind of pg auto failover's kind of command line helper and here what we can do first of all let's look at the state um we're right away also running postgres14 i always you know for demos i think it's it's nice to if we can um use the latest postgres version because it helps us also test the latest postcodes version right the raspberry pi also runs an arm so it's kind of a nice way to make sure that you know other architectures work as well um and so right now here we can see that um kind of you know in this setup right now the data01 is the primary node the data02 is the secondary node right and that information is is cached or kept in the coordinator uh sorry in the monitor so the monitor kind of knows which of the two is which so the first thing we can do and this is just you know really simple is we're just gonna do a planned failover so we're gonna um gonna do this right so this is pretty straightforward so we just say we want to fail over right so we want to change essentially like change places um and we just execute that right and that will kind of now happen automatically no no other action necessary um and this is let's take an example right like i want to change share payload libraries i want to enable auto explain or preach that statements um if i you know had to restart my node that usually takes longer than the failover right because you're shutting it down um you know that needs to like write a checkpoint um there's like a lot of kind of you know changes happening there um and you don't really have a guarantee that the you know the secondary actually would come up again right or the node itself will come up again so the benefit here is that with you know using a failover for maintenance operations you can actually ensure that the node you're failing over to already run successfully right is already online is like already has things in shared memory because you're just essentially telling it to to promote um so really you know like something that enables i think a lot of maintenance scenarios to be quicker and more reliable so now here right we can see um this kind of completed end to end um so now the um if we show our state again right we can see data02 is the primary now let's actually connect to these nodes um so in order to do that we can do this command here all right and so the important thing to call out here um and this is you know unique to pg auto failover is pg auto filler uses a mechanism in postgres um called kind of the target session attribute setting um and so that is something that happens on the client side so if you look at this url here which is the one we use for connecting we can do this right now actually you know copy and paste this correctly and so the key thing you will notice is that in the connection url you have both the data01 and the data zero2 specified and so that is helpful because now the client makes the decision which one of the two is actually the primary right now which one of two is available to be connected to um and for example if one of the two was not available we could still connect right we could still connect to the one that's online and that's writable um alternatives to this are often you know dns based right you in the dns record you update the dns record when the when the primary changes to a different ip or maybe using something like an elastic ip but all these have the problem that they require usually more infrastructure components right so it's very hard to implement in a simple environment and in addition things like dns can have caching issues and so on um and so kind of shifting this to the client is something you can do modern postgres versions which is i think the right thing to do now um we have a table here called test um right now it's empty right so let's insert the timestamp into this all right so we got a timestamp inserted here now what i want to illustrate right is what happens when we have an unplanned failover so for that let's check again what our our primary is right now right so our primary right now is the data02 um so one thing we'll do now is there is a events command all right the events command just shows you um what has happened uh kind of on the pg auto failover kind of you know side of things right so like it kind of tells you the important actions that things that have happened right so before it kind of showed us the the activity of you know the planned failover now what we'll do is we'll do a watch command here right so this will just keep refreshing on its own and so now i'll do the unplanned failover which is i'll just disconnect one of these two here all right and so the important thing to note by the way is that this is actually you know also pulling the power right so this is um like using power over ethernet and so when i pull the network cable i also pull the power so it's kind of really like a hard crash right um you will notice that this takes a couple of seconds so now essentially what happens is that the monitor you know tries to confirm if the secondary is or if the primary is truly down right so right now you know we're essentially running in a timeout we're kind of checking if you know um something is like no longer available and we'll give this a few more seconds perfect there we go um so now you can see right after about 10 seconds or so the monitor decided that the data cr2 node was unavailable right it became unhealthy um and so now what we're doing is we're essentially telling the data01 node to become primary um and without having done anything right all i did was plug like the cable out um and everything else happened automatically and so this will probably take another moment or two um and so once it has ensured consistency it will actually bring up data01 again right so if i now repeat what i did earlier which is i tried to connect we can see that i didn't have to change anything in connection string right i still have both hosts specified um and i was able to just connect directly and even you know more so i could do an insert right so this is actually writable now what would happen if i plugged in the older one again right so if i enable my my watch command again here i'll kind of you know plug this in again just fix it here um now obviously this will will take a brief moment right because it actually has to boot up a system um kind of ubuntu in this case again on that on that raspberry pi um and then you know what it will do is it will connect back to the monitor it will check what its state is right it will realize that it is no longer the primary and so then it will uh it will try to define find out you know where where did we kind of depart right where was that divergent point um and then it will potentially do a pg rewind to kind of walk back the timeline a little bit and then um kind of become the secondary right so we can see here after about five to ten seconds it found out that you know like it came back up right it found out that it was demoted it was no longer the primary um and then it's kind of you know switching it to the catching up state to to actually you know um become a secondary again and so now you know if we waited a few more seconds we would see that kind of you know it comes back perfect yeah and now we're actually back to synchronous replication right so this is important is that you know whilst all these things are happening um there's a trade-off here between you know having always consistent rights to two locations or having availability right and so here we choose availability like we're intentionally saying when the secondary is not available or the primary fails right if we switch over then temporarily we only have one copy right because oftentimes that is a better trade-off really what you're trying to do is be as available as possible even though if that means that you know you might like temporarily not have something like synchronous replication all right um so with that let's switch back and let's talk about tuning all right so for tuning i think you know there's a couple of things we could be talking about here um i just want to give you the highlights of what i think is most important to think about when you're tuning because oftentimes we scale out because we haven't tuned right because we just you know thought it's it's easier to just add more hardware um and really what we should be doing is tuning postgres and so there's a couple of key things i want to talk about here the first of all i want to talk about workman so workman is you know setting that by default doesn't have a good value necessary in postgres um it really is a trade-off between running out of memory um versus you know having operations built to disk so the default which is four megabytes in postgres means that when you have a sorting operation or a hash operation or group operation that happens as part of query execution if that exceeds four megabytes and memory use it will spill to disk it will create temporary files in order to you know not use more than the four megabytes of memory and that can be the right choice right if you're very memory constrained that is probably the right choice however it does mean that things might seem slower than they could be if you just made more use of your already available memory um and so you know this is something that you can look at in detail using two kind of statistic views you can look at pg stat database which has a 10 bytes um field and you look at ph.statements to get a per statement breakdown of this and so the other thing you can do is you can turn on logging here you can set the log temp files in postgres um and then you know it will tell you this this is the statement that i run and this is kind of the temporary file to create now the general you know guideline here would be if something sorts and that's bills to disk right if you get like this you know output in your log file for example you should generally be increasing workman maybe you should increase workman for you know on a per query basis or maybe on a per roll basis right like be a bit careful there not to overdo it um because really what you're trading off here is if you go too high you will hit out of memory right so if you have too much of memory errors really what you should be doing is reduce work them um to avoid them now second i want to talk about vacuum briefly so vacuum and post credits right really important key operation cleans up you know data after you're doing an update or delete now uh there are some basic things we can look at with vacuum right we can look at pgc activity to see which vacuums are running right now we can look at pg step progress vacuum which is new since postgres 10 and it's really useful to kind of know which phase this vacuum is in right now but then really i think the key recommendation and this is actually what we applied earlier in this talk is we need to reduce the cost delay in most cases so before postgres 12 um the cost delay in postgres was set to 20 milliseconds which means that the maximum speed that a vacuum can run at is eight megabytes per second that's just kind of you know how the map works out if you calculate it which means that if you have very large tables you will see vacuums running for days and days like i've seen like last week i saw a vacuum run for 19 days let me tell you it's not a good thing if a vacuum runs for 19 days like things you know the tables get bloated um you run at risk of transactions you wrap around like it's just not a good thing and so instead of you know adding more capacity which you know wouldn't actually help in this case because the vacuum would still be slow really what you need to do is change the cost delay setting um and again you know change the postgresql is changing to two megabytes uh two milliseconds i think that is the right change to make i think you know on all systems and except for maybe very small ones you should just make that change and then i think the other thing that has really really helped in my experience with resolving auto vacuum problems or vacuum problems is using table partitioning right like there's a lot of workloads that can benefit from like time series workloads oftentimes where you're just adding data to to a table and you're deleting the old data and so if you don't use partitioning what happens is you're kind of poking holes into the table right and so then you're filling the data in again like at random places in the table um which means that other vacuums is a lot of work um your sequential scans are often you know very expensive because you're like scanning a lot of unnecessary data and the table partitioning like saves a lot of vacuum work saves you know improves performance so if you can you should always use table partitioning especially for anything that's larger than a couple of gigabytes all right um third i want to talk about checkpoints so checkpoints are important because we kind of you know they're very i o intensive events in postgres so when you look at kind of you know your overall structure right you kind of have the pa the buffer cache which contains you know the um the things that are currently in memory that postgres you know um retains there that are either you know cached for future reads or that are you know rights that haven't reached this yet and so uh the checkpoint is really the point where you know the data directory gets synchronized so that you know for sure that you know there's a safe like point if there's a crash um the data directory is fully synced to the disk and so because of that you know syncing to disk it is an expensive operation um it is also the point where if you don't have frequent checkpoints it will mean that if a server restarts it will take longer to get to get back up because there you know there's more time to go back to the previous checkpoint the most important thing to look at here is the checkpoint log events so you can do log checkpoints enable on and then you just see you know all the checkpoints that are happening um and really the main thing to optimize for from performance perspective is not having a checkpoint be a x-log based checkpoint or wall-based checkpoint but a time-based checkpoint now if all this checkpoint happens if your wall files that you don't need to be kept around until the checkpoint happens exceed the max wall size so if they exceed the max wall size then a checkpoint is forced and then it has to happen pretty quickly because postgres is trying to optimize for disk space usage versus if it's a time based checkpoint it actually happens spread out over time because it doesn't have to you know immediately do all the i o and so if you're seeing a lot of i o spikes oftentimes checkpoints can be one of the reasons the other thing we can do in addition to logs is we can look at these statistics so there's pg stat bg writer which is kind of a statistics table in postgres we can use that to also find out you know how long do the checkpoints kind of how long is the time between checkpoints and what is the percentage of time checkpoints um and then again you know generally um the two levers that you have is the max wall size as well as the checkpoint timeout so those are kind of the two things you can do to you know make sure you have as many check time checkpoints as possible um and then you can use checkpoint completion target for the time checkpoints to decide how you want to have the i o done right so for example 0.1 would mean that the checkpoint would try to complete within 10 of the full interval length of the checkpoint um versus 0.9 would try to do it kind of you know uh at uh like almost spread out completely and this again it also depends a bit on if you have a lot of time checkpoints and 0.9 is probably the good thing if you don't have lifetime checkpoints then if one happens you probably want that to happen quickly all right and last but not least you know since i'm excited about postgres 13 so i kind of want to show you a little bit um what postgres14 can do so let's uh kind of go back to what we had earlier and let me switch right here all right um and this by the way you know it's still running on our little raspberry pi cluster here um and so here we can now do um let me connect again to sql right so we're kind of connecting again to our kind of local cluster here um so now i want to show you what we can do for wall monitoring um let's look at stereo all right so peach set statements generally shows you um which statements have run on your system right so historically kind of gives you information about aggregate query statistics um and as of postgres14 it has gotten a few new um kind of nifty columns that i like first of all it tells you about planning time now we won't talk about that today but just you know call it out this is very very cool feature the other thing is it now tells us about wall records being generated and this shows up in a few other places by the way um and explain for example as well you can now see how many wall records get generated by an update statement um so we were doing this insert statement earlier so i wanted to take a look at how does that you know kind of look from a wall perspective right um and so we did 134 calls i did a few before we we just did one movie testing um and so now we can see right so there were 134 calls and there were also 134 wall records generated by that right so each insert essentially caused one wall record to be created we can also see how many bytes the the wall kind of um bites off wall got generated um and this is helpful because if we look at our system across right like everything we could for example now say let's see if this works um so we could now say you know what was the most expensive thing to do on the system for um kind of you know in terms of wall records and we can actually see you know there was a temporary table created earlier um there was a truncate that was running earlier right um so these were all things that you know kind of ran earlier that had a lot of impact on the wall stream right and essentially caused a lot of wall to be generated and this is you know important for us to know because then we can say you know if we're trying to optimize our right workload right we're trying to optimize how many rights we're making we can optimize for the things that are writing a lot of wall because oftentimes that is what's causing issues right like if read replicas fall behind it is because a lot of wall is being generated and so on all right um so that is that in terms of uh looking at a postcards 14 ball monitoring let's switch back to here all right let's talk about sharding so for sharding you know uh sharding is really about scaling beyond the single server so oftentimes you know we we keep adding our data right we follow best practices we you know optimize our vacuum we use partitioning um but there's still too much data right we're still kind of reaching you know terabytes of data um or we're having performance issues that you know we can't we're paying our cloud providers absurd amounts of money for provision iops and you know it's still not working as we like um and so that's really where charging comes in is that allows you instead of you know keep adding on a single server it helps you to scale out to more than one server now there's different ways of doing sharding today we'll just look at one which is we'll look at the cytus extension for sharding so cybus is an open source extension for postgres you could you know on your own system or on a raspberry pi's i could go right now and just you know install it there and use it to scale out postgres so it's you know like very i would say easy to get started right and again it's you know tries to be something that is you know um straightforward to use as well i would say um i would say the key feature here right is that we can chart the data across multiple nodes so that when a query comes in like for example you're selecting something from a particular table um and that comes into what's called the coordinator node the coordinate node is kind of a regular postgres server in the site is terminology that then kind of has the role of forwarding the query to other the the actual data nodes that actually you know run the statements um and so here we can see you know we're selecting from one table but really behind the scenes there could be many tables right it could be 32 or 64 or hundreds of tables that get you know queried in order to fulfill that query and to the application that means the application can keep doing its queries as if it was a single server but you know behind the scenes essentially there's this kind of scale out architecture and so as our last step here let's take a look at how that works right let's take a look at how we could actually shard in practice and how size kind of works under code now for this we're going to take a look at um so first of all hyperscale size is what we usually call the service right like we're kind of offering site is not just as the extension but really as a kind of you know managed solution um and then we're going to look at it in kubernetes um and we're going to utilize what's called azure arc so azure arc is essentially the mechanism where azure data services and other services from azure are brought to environments that are not necessarily in azure right so we could be running this on aws or google or we could be writing this on our own on-premise data center and we kind of use the azure technology outside of the azure environment so now i'll switch over to here and it's the same terminal we used earlier and so now we will connect to rvm here all right perfect so this is a regular ubuntu vm this one actually runs in the cloud has a bit more capacity than the raspberry pi's um and so on this one here we'll now um we see we'll first call az data login um so ac data is kind of the cli that we use for um kind of connecting to azure arc so azure arc you know it's kind of similar to the azure cli um this is kind of you know what we used to communicate with local controller and so this could be you know something that this could be something else you could be running if you were offline as well right so there are scenarios where you find yourself on a cruise trip probably not these days but you could find yourself in a cruise ship right and you still had kind of a kubernetes cluster running hyperscale site is using azure arc and it could be managed locally through this ace data cli now um let's see what we can do here so we can do postgres server list um and so this is just going to list our kind of postgres instances that are managed by this azure arc data controller on this kubernetes environment right and so we have one kind of server group what's called right now so server group is you know multiple servers or multiple pods um you can actually you know look at that for cube ctl um and so actually do this for all um so we can see here right like in our default namespace like there's a few other things running as kind of main management tasks but really the key thing that i want to show here is that you know we have kind of five pods um running kubernetes and so these correspond to kind of you know the the coordinator node and then the individual worker or data nodes and so if i wanted to connect to this i'll specify kind of you know the the name that i want to access and sit down in a moment there we go um so now you know i can connect to this through here and so now we can see here right like um in this postgres um kind of deployment we have our size extension right which gives us the scale of capabilities now actually drop this table for now let's do a new table so i will kind of do an e-commerce example here so i'll do just you know really quick you know i have my let's call it you know products right and so each product is kind of in a store um and then you know that product has data associated and we're kind of using a json b column in postgres so now um we're creating a regular table here right so now i could insert into this table right so i could do insert into products i'll use one two three and then i could you know say something like you know skew and then this could be let's say um apple right um and so now if i did a select star from test from products i can see that you know um i kind of inserted that in there right um now if i'm right now i'm actually not using cytos yet right so right now i'm just using regular postgres and so if i do an explain on it i can actually see that this is doing just a standard sequential scan on this table um now what we're going to do is you know we're kind of going to distribute this data so we'll call create this through the table we'll kind of you know tell the products and so then the other thing we need to do when we're distributing a table we need to choose what to distribute the table by right because we're not actually copying the same data on all kind of the data nodes we're actually you know splitting it up so that you know some of the data is in node one some of the data is a node two and so on and so here we're kind of choosing the store id you know as that kind of shard key or partition key and so now if i rerun my explain i can actually see that you know the query is no longer just using a single node but behind the scenes is actually you know splitting it up into these different nodes but you know if i wanted to i could you know still query as as regular right like no no change necessary so for the application it you know keeps being the same now last but not least i want to show you how this you know kind of works for scaling out um so right now we have four data nodes here um let's see if we can increase this to six and so um you know this is like what's what's nice about this is instead of you know like changing the size of your hardware right so if you went from you know a single server that has you know four cores to a single server that has eight cores that's usually the operation it takes time like it's very expensive sometimes because you always have to double the capacity um and so here the nice thing is we could actually go node by node right you can just like scale out as if you're just like increasing capacity on a slider um and so if i go back to my you know kubernetes pods i can see now right um there are the four parts that have been running for a longer time and then there's the two parts here that just you know kind of came up just now and whilst this was happening right my application was fully online so i could have you know kept running queries here and then you know as it's scaling out these queries keep working um so there's no downtime no impact to the application as you're scaling out all right um and that's it really um you know we um took a look at quite a few things today we looked at automation um and kind of looked at you know how we can use terraform as well as how we can use pg auto failover to uh to kind of automate your postgres setups so that you don't have to you know manually do things um we looked at kind of some you know important tunings in postgres as well as what's new in postgres 13 which i'm you know again very excited about myself and i would encourage everybody to test the beta just came out two weeks ago so this is the right time to help the community test don't run production of hoskus13 and then last but not least we looked at kind of scaling out capabilities using the cyrus extension all right and with that i'm happy to hear any questions all right perfect thanks everybody um so i think this is working if i see this correctly all right um so i'll essentially go through the questions here um if you have any other questions that you know have come up especially in the last section i know we have a bunch of questions on pg auto failure sections initially but if you have any questions on sharding feel free to add them and i'll get to them at the end all right let's see so um to start with um the first question we have here is asking how is pg auto filler different from petroni um if you're using patrony do you see a need for pg auto failover and so i would say if you already have patrony setup you know patron is great you should use it um so really the reason that we started pg auto failover like two years ago was that we felt petroni was too complicated to set up and patrony required you know not just standard postcards components but require additional changes and so what we did with kind of pg auto failover is you know build a standard straightforward kind of postgres you know just a bunch of c code essentially with an extension postgres as well um to kind of make it straightforward right so both petronium and pg autofill solve the same problem and i'm a big fan of what the teams there has done so it really comes down to what you think makes sense in your environment all right next question from mcmahon j in the failover scenario can we maintain one vip to connect the database instance so the app does not have to change connections during the event of a failover so in the event of a failover right like when there's actually a problem you know like when one of the instances down so what i was showing maybe this was you know a bit hard to see in terminal unfortunately um is that the uh the connection string always has both host names right so you essentially don't uh specify one or the other like you don't specify the primary and not secondary you always have both present and the client makes the decision which to connect to which again is a very like i would say somewhat recent post-course features so you need a client that i think is at least postgres 10 but really the benefit of that is that the client can choose which one of the two is working and connect to that one so you don't need any virtual ip you don't need you know kind of to change any connection strings it just keeps working the same string all right next question from patrick rebrook what happens if the monitor node goes down um so in this setup we essentially protect against the failure of any one of the nodes right so we have kind of the primary we have the secondary we have the monitor and so the idea behind the setup is is that if the monitor goes down your database is still online right you're still replicating however automated failover would no longer be working so it's kind of you know a trade-off here where and we could and eventually you know ppg autofill will have an additional kind of monitor nodes as a feature if you need that right but really what we're saying here is it's likely like in most cases a single vm will go down right and we're protecting against that problem and i apologize for the background noise there's a bunch of construction going on uh right outside um from uh next question from remy cora um about using two dbs and a parameter connected pulse p sequel um unfortunately i didn't see the full question there so i'm not sure i suspect isn't the kind of thing so maybe the follow-up was the next question there isn't it the kind of thing to do with a connection pooler um so i would say a connection pooler could also be utilized for that right so you could essentially have a connection puller handle the same kind of uh connecting to either one of the two servers um here we are essentially moving it all the way to the client all right um a question from brian berryhill do you know of a published list of database parameters that cannot be changed when using an azure postgre service versus a vm that's a great question um so today there isn't really a published list outside of what's in the azure portal um so if you want to know you know which parameters can be changed just go into the azure portal um there has been a lot of improvements there recently um so i would encourage you to if you you know had issues in the past with changing parameter i would encourage you to go back um and check again um i know the team is working hard on making as many parameters available as possible it is always a trade-off you know if you provide a managed service it's a trade-off between allowing everything to be changed while you know still providing sla essentially um and so you know rest assured that we're trying to improve on this um but if there's any particular parameters that you see issues with feel free to you know send me a message and i'll get it to the right folks all right um let's see next question from ramesh uh this fanatic create database hangs after creating the cluster using pg auto ctl so that sounds like a very particular question i assume you tried pg auto failover and there was some um like problems there um so if create database hangs it sounds like there was a problem with synchronous replication um which generally shouldn't happen right so like generally what should happen is when the secondary is not available it should switch to kind of async motors to actually switch to you know single mode so that the uh the primary does not wait on secondary now if it hangs it probably means that it does wait for a secondary um i would encourage you to open a github issue again it's an open source project um and i know the team dimitri and particular team here um is you know quite uh interested to to find these bugs and um fix them all right let's see um john lovato is asking so for applications to automatically fail over they need to have both hosts and connection string uh yes that's correct so with pg autofill over the design again is intentionally that the connection itself has both hosts available at all times um and the connection itself when it you know like with the target session attributes what you're saying is i need a connection that's writable and so it will connect to both and if both are online but one of them is only readable right because it's the secondary then it will not use that it will use the one that's writable all right jan lenfer is asking does pg autofill support multiple secondaries and sync replication um so today it does not um it is something the folks are working on there's actually a pull request right now um on github that i believe more on the team is working on um if that's something you know i would encourage you to subscribe to the pull request if you're interested um it's definitely something that's on the roadmap all right next question from nevas asking monitor component and pg auto failover does it prevent split brain situation is it similar to zookeeper and lcd um that's exactly right so the idea behind the kind of the monitor component is that we do not need something like super density right so that's how patroni for example solves it is by having another kind of you know authoritative source that can say for sure that if you have a network partition you know which node is actually primary so you don't have two nodes believing their primary and so with the monitor component mpg autofill we're essentially using a standard postgres instance to serve that same benefit right so you're kind of reducing the number of different types of components you have to run right so you don't have to run a zoo creeper or icd you just run a monitor and that can avoid this blueprint all right a bunch of questions here that's good um let's see next uh next question from remy cora do you think workmen could be set automatically after odd analysis of the hardware so that's a good question so i think i wouldn't so workman i would say workman is more related to your workload than it is necessarily about the hardware right so workman really says how much memory can a query use when when it's running right and if it exceeds the workman then it will spill to disk and so there's a trade-off here and i try to you know allude to that briefly which is either you run out of memory or you have things run slow because they spill to disk right there's a sweet spot in between and so i think ultimately what you need to do to do this automatically which is a bit difficult but ultimately you need to look at the temporary file log events to understand when do things build a disk right in that case you would increase workmen if you run out of memory you're getting close to the memory limit then you would want to reduce your workman so that queries do spill the disk and use less memory so it's kind of you know it requires a bit tweaking um i think in reality what i've seen is unless you run a lot of active connections it's it's usually not as much of an issue right so really you know you don't want to run the default because that spills to this a lot by default being four megabytes it's just too small for most workloads um but you probably shouldn't set it to hundreds of megabytes either all right next question from sean mcbarnerj is there a concept of huge pages as an oracle so i'm not familiar with huge pages in oracle if they are the same concept as huge pages in linux like the standard huge pages then yes postgres can make use of that um there is a separate setting for that you need to turn on um there's a question here if you want to use transparent huge pages or if you want to use kind of pre-provision like pre-created huge pages um i think these days transparent huge pages are somewhat safe to use they're not as performant though as you know creating huge pages but if you search for postgres and huge pages you just find you know a bunch of documentation that talks about this all right next question from remy cora could you give hints about automating table partitioning that's a great question so i think um it depends a bit on you know how you want to go about it um i would you know search like again there's there's many different tools here i know pg parkman has you know been popular for many years now um you could also use uh cytus data also create an extension called pg chrome which you could also use to do some of the automation here um but yeah i think peach apartment is a good starting point for automation all right question from brian berryhill do you have a question a guide posted on getting postgres14 up and running on a raspberry pi yeah it's a good question um i'm still very happy to you know have like have gotten the raspberry pi demo to work um i think it's actually pretty straightforward if you just want to run standard postgres14 um just uh the beta has packages so you can just you know just search for like if you're using that debian ubuntu just search for app packages um or like the app package repository of postgres and there's a separate kind of you know um urls that you use for the beta one um and so just specify that and then you can use it on raspberry pi um assuming that you're running 64 bits which i would recommend but yeah like modern raspberry pi it's just straightforward nothing else to do um if you want to try out pg auto failover um that is not packaged yet there is a pull request that i actually created as part of creating stock um that you know um fixes postgres14 support so if you want to try it out take a look at the github all right um let me see so mike sanders is asking are there any limitations like query complexity and using a sharp menstrual excited versus more old-school manual shard management based on partition keys and individual dbs that's a good question so let me think through that so let's just look here yes i think the so generally i've not seen an issue with query complexity i think the thing to know about right is like societies is actually pretty good these days about handling complicated sql now the thing you want to be sure of is what are you asking the distributed database to do right so if like essentially when you're using something like site is always remember that if the data is actually split up between multiple worker nodes right that means that the full data set does not live in a single location if you're doing certain kinds of things that you know are for example not going on the partition column but they're you know going across different partitions then you might be pulling data into you know either the coordinator not insiders or like between the worker nodes and so that might be something you want to avoid from performance perspective um the good thing is you could just use explain to you know look at that but that's the one thing i would call out i think is it's less i would say it's less like complexity like it's not a support problem necessarily it's more performance consideration like the way you data model your um the way you select a shark key in your data model does affect performance especially in a dispute setup um all right nevas is asking for postgres charting solutions how is app connection routing handled through a lookup table um so that's really i think the key benefit that cytus provides versus something because you could always use this to do sharding in your application right you could just have your application connect to different database servers and so the nice thing about cytos is that it's completely transparent right so if you have the size extension installed it essentially hooks into postgres and so when you run a query um it will like let's say the store example from earlier right i said that i run select star from stores or products where store id equals five and then inside postgres there's a mapping from that store id five to the chart or the hash of that number five that then maps to a particular chart on behind the scenes on a particular node and so that happens without the application needing to do any hookups it just happens automatically all right lauren slayhe is asking can you use pg bouncer with pg auto failover that's a good question so yes you could use it now i think the thing that might get tricky is the exact setup there so if you're doing that i would encourage you to again look at the github i know the folks are doing it i don't know it right off um dimitri on the team i think could share more on how exactly to use pg bouncer especially with the kind of you know multiple host names and kind of having that set up correctly all right andrea and florea is asking any details about how multi-master application geo-distributed works with hyperscale is this even supported yeah that's a good question so i would say multi-master application is it's really a parallel problem to sharding right so sharding helps you in a sense that you you can now scale out and you could run multiple coordinators which gives you a sort of multi like active active kind of setup but it's i would say i would really think of this distinctly from trying to solve something like multi-master so i i would say you know um look at look at cytos right and see if that makes is a good fit i know there's you know bdr from second port and for example right there's there's other solutions to do active active which are you know maybe more suitable if you have a small workload that you need to replicate in a lot of places and you need to write in all places um when you consider any active active kind of solution i would always recommend you to really understand the trade-offs right so there is a reason that this is a hard problem and this really comes down to you know network latencies so if like essentially don't assume that if you for example use pdr right like it will have its trade-off it will have things that don't work as expected because again you know there's physical limitations um but i would say silence itself is more really focused on that scaling out both the reads and the rights all right then cut kosher is asking what happens for an unplanned failure when the setup is more complicated like one primary two replicas the dc one dc2 and witnesses in the third dc does pg autofill handle auto failure and rejoin and how does it handle stone is to prevent split brain yeah that's a good question i think it gets a bit complicated here but ultimately ultimately i would say right so like two replicas to your point so that's not supported today once it is supported you know i think this will maybe handle a bit better um i i think ultimately we should probably have this conversation offline so feel free to either send me an email it's lucas fiddle.com or you know open a github issue on this and then we can discuss this in more detail all right um azilian olang garathan is asking what happens when we join two tables one partition another non-partitioned yeah that's a great question so this you know inside the setup right like coming back to this earlier notion of you know some data might be in a worker node and you know something it might be somewhere else so um in most of these use cases there's a use a special table type called reference table and a reference table inside us gets replicated to all the worker nodes the same way so that's you know useful let's say we have in our stores again and we have you know maybe a like maybe the stores themselves are a reference table right so the products are sharded by the store but we want the full list of stores in all locations or maybe we want you know let's say counties or something right like some kind of data that it needs to be in all um in all kind of uh locations and so um you know this is kind of you know the best thing to use there all right i think we're out of time um i think there were a couple more questions um i'll you know try to get them later um also feel free to send me an email or reach me on twitter cool thanks everybody \n",
            "so good morning everybody hello everybody's leaving Hey usually it starts in the middle you know when I had my first database course at university we started with a group at 110 people and at the end there were six left I was sitting there but I was afraid to get off you know because if they're only six people left they will recognize you you know my first impression was who needs this stuff and this is how I ended up you know so good morning everybody my name is Sir Hans Hans is easier so just forget the rest so this is postcards performance in five minutes so thank everybody for showing up so it's 200 people which is I think more than Silicon Valley Conference so that's quite an achievement so thank you for the organizers and thank you for having me here excellent pay Jumbo's 50 just to give you a comparison so who are we next slight maybe yeah we're working on it nope nope nope no that is bad news but maybe we can work something out so as you know we do Postgres support but we're not especially into thick clicker support does anybody have a nice idea are these words nope I think so what is so this is why all we should have more people in support because one people person might not make it you know yeah that's what we tried yeah and we work it out no problem ah maybe yes no how can we do that you know either why is this a Windows machine yeah that's what's wrong yeah this it's a bug I've seen it ah works works works so this is us we are worldwide postcodes company we've done this kind of stuff for it for 20 years we've got a couple of offices around the world and we hope to add some more also what we do is basically a lot of Postgres data mining business intelligence all this kind of stuff so in short all you have to remember is we're great right that's what it is so we've got a couple of customers around the world which includes Lufthansa BMW some people you might not know hims is actually you might be interested they are selling viagra right so in case you're interested just tell me privately there's no need to raise your hands you know so let me give you some inspiration for this for this talk here and there was an enlightening moment I was in Germany you know this country Germany never heard of it yeah and this was really nice they had a nice warehouse right it's xoxoxo stuff Exadata and they had a nice little table with four hundred three hundred forty billions of rows which was basically the entire history of the company you know every time something broke every time something got fixed all this kind of stuff and this Oracle thing this Exadata hit its limits right and they did a lot of wonderful awesome stuff such as count star were not in 8 billion rows so and they have 22 queries which they identified as a problem and they looked at it and I said why do you keep counting the whole history of the company every day right it doesn't change much stuff from the 60s it's pretty constant right and nobody knew so they were thinking about investing something like 5 million into some exudate the hardware and that 22 Korea reason nobody even had a clue what the thing was doing and why they ran it and this was wonderful inspiration for this presentation here so why does oracle die if you throw a stupid query counting hundreds of billions of rows how could it happen you know so let's let's approach it from from from a more abstract point of view a guideline to failure and this is international right so it has nothing to do with country there's nothing to do with or gene age or anything this is a universal law which you've got to follow if you really want to achieve perfect failure right and a perfect failure would be you take your free hundred billion rows and you join them 14 times no wonder it fails you to know pre aggregation that the most important thing is you must not know what your queries are doing that's really important so really have to run stuff you have to complain about it on the regular basis but you must not have any idea what it means that's the biggest pre requirement to failure run stuff you don't know and stop thinking right I'm telling you as a post Chris consultant we get rich with this kind of stuff so please stay stupid as it can potentially be right awesome you know wonderful so these shoes were paid by one of those missing in X's you know so what we got here is on small data sets and the question is what is small could be five gigabits of data could be a hundred thousand rows so whatever you consider is small hardware has been so great recently that it's gonna bail you out so if you have small data sets you you can do basically what you want this is not advice this is observation okay so on a small data set you can basically do what you want but at some point your data is gonna grow and this is why stupid stuff is gonna kill you right if you are counting people in the room you can do that easily but if you start counting people in China it's gonna take you you know but it's still the same query right and one very important observation and we hear that all the time on the phone can you tune my post quest conference and they already know that they're doing something stupid right there is no such thing as a magic parameter which says performance on if performance on would exist guess the default value it would be off just you know to torture you know there is no such thing as performance on doesn't exist so the solution in most cases it's not in your post curse configurations it's a simple rule sheet in sheet out right so if you stick to this rule you surely kind of fail right so next thing is hallucinations who has hallucinations anybody oh you yeah how many people do you see okay so people when something goes wrong first of all it has to be the file system it has to be the hardware it has to be the configuration and everything is slow it's not I press this button and this thing is slow but they require the observation is usually everything is slow right who is heard that before yeah everything is slow not just this stuff everything right so the first thing you have to do is to pinpoint your problem and what you usually get is we need to pick a server we need more days we need more RAM right or more rains depends on and that's not going to fix anything so the first thing is in order to figure anything out and this is universal this is not just for Postgres this is for everything is the first thing is please measure you can sit on a stone and meditate and it's going to come to you naturally right it's not gonna happen in database stuff what you want is to measure right and there is one thing the answer is not in your load graph the answer is in your queries you can keep plotting graphs to infinity it's like if you have a fever and you put your temperature on the wall it's not gonna change anything right you still feel sick you see dead and in order to fix it you put a second graph on the second wall right it's not gonna change everything right you want to figure out you got an infection you got cancer you got whatever you you might have right but it's not the graph which has the solution it's it's the disease you know and if database work the eternal truth is that query scores load so why does everybody look at the load graph look at the bloody queries and in Postgres there is a thing it's called stat statements and in my judgment it's the gold standard for analysis because it's really gonna tell you what's going on you're looking for the root cause in many cases it's a small thing you know and here is what we got in PG stat statements we can as we can see the query how often it was called and how much time it took and guess what if you order by time you will get the most time consuming stuff on top how cool is that so it's not the load graph anymore it really points down to a couple of queries so you could see this query is gonna run 50 million times this query is gonna do this and that it's slow it's whatever also you see minimum execution time maximum execution time average execution time and then you also see the number of rows if you run a query which returns 50 million rows 10 million times it's a hell amount of rows but if you press a button on your website and the database has to return 10 million rows well you're going to display that right if you got one website what are you going to do if 10 million rows in the result set it makes no sense whatsoever so this rows column is already an indicator of stupidity right so it correlates highly with with stupid stuff you know so it's it's sort of a soft hint to stupidity second thing is cash he traits shared under score whatever it's not the cash hit rate of your server because you're waiting if you click something you're waiting on a query now on a server right you're waiting on this particular query so cash he trades in a global context is pointless it's really relevant in the context of a query because if you have a hundred terabyte database and if one gigabyte of ram and you're running a backup what kind of cache hit rate do you expect it's gonna be zero and that's the way it's supposed to be right you cannot cache a hundred terabytes of data unless you're Facebook Google or something you know so cache hidrate is really something which is in the context of a query right it's not overall what's the best cache he trade hundred percent oh I only got like 99.5 you know so it's really in the context of a query and finally temporary i/o and in the normal a but let's move on to the magic if you only remember one thing this is it so maybe you can quickly memorize this is literally golden if you just have this kind of query forget the substring on top I'm only going to do that so that it fits on one page right so forget the substring it's not what you want but this is literally the most important query you can have if you're inspecting your database stuff so let's wait for the pictures everybody you can have the slides so it's no problem and this is what you're gonna get you get the queries and at the end you're gonna get two percentage points and I pointed this query to a super intent intelligent intelligent food person in Germany and he spotted at the insert statement it said 1 percent and he said I knew it was the insert if you're at this level of expertise give up forget it oh please but what you're gonna see here is that there are two queries which caused all the load and in average they need one millisecond so it's not much room for improvement usually right if something takes one millisecond and if it takes half your load I mean there's not much you can do so in many cases it's not the slow queries which cause the issue so if you look at the copy statement it's 200 milliseconds it's by far the slowest one here but it's not gonna do anything because it's the initial import okay so you really want to see the queries in comparison to all the rest and then it can charge from the the average run time is it worth looking at this query or not because if we are over already like the third update at 0.02 milliseconds that's not much you can do right and now you've pinpointed the problem so in this case if we could eliminate the first update just because we had better logic in our application we would have already doubled our speed no slow query log log no loud graphs nothing right just this bloody query and a little bit of analysis and the answer to the question here is relevance first what you see here is relevance first that's important stuff so now we know what what is happening here we can basically we can basically move forward and move to the most important topic in the world right so I've told you already who paid for my shoes and I'm going to show you who paid for my jeans I think it was 12 euros forgotten indexes if you are stupid like hell right if you're only able to create proper indexes in Postgres it's already enough to do a part-time consultancy job if it's the only thing you know is crate index you can already fix Evan T percent of all problems right and that's reality if somebody calls you and says we got to fix the rate level optimization file system cache fruit boot IO schedule kernel parameter you can already hear one missing index two missing it exists free misses already PI's up or as somebody on our slack channel stated recently real consulting or missing index was the problem and it's usually missing index and what are we looking for if you're reading 10 million rows 10 million times you're reading many rows and guess what reading many rows is expensive how could it happen right so here is the miracle query so that's the second query you might want to remember to fix 70% of all problems what we're looking for is we're looking for the schema the table then we want to see how often did we read a table sequentially how many rows did we find that six table read how often did we use in index and then what we try to figure out is what was the average size of a sequential scan if we get a sequential scan on Provinces I think we got nine provinces right something like that I learned it yesterday from the taxi driver so if you got nine provinces what you're going to get is a sequential scan there's nothing wrong with a sequential scan but there is a lot of stuff wrong if you read a 10 billion row table 10 million times that's going to blow up the system and you're gonna see that in the statistics right and this is and this is also very important everything that shows up on top here is a candidate for a missing index what you're gonna see is if you order by the number of rows you processed and if something 10 billion times 10 million rows it's this number you know it's going to do something to your system and one missing index can blow up the whole thing so imagine you got a thousand queries for one millisecond and then you get one missing index in your curious one second one bad query is expensive as the thousand good ones you have to keep that in mind so one misplaced missing index can blow up everything you know so usually you can see that impeaches that statements and you're gonna see pointless operations right so let's look at solutions right so now I've told you a lot about problems a lot about other people's stupidity because we never forget missing in exists right anybody who ever forgot missing in extinct heads-up oh it's six people out of 200 go so let's take a look at the classical examples where you can really do something I mean not just Mesa gimmicks is that's boring but let's have gender and let's have male and female right two channels and then we create the table with people so we got a serial number we got a chamber and we get some some data right so we've got to change the table with two entries and we get a person table with five million entries alright and what we do here is we insert into person and we just say half of them is male half of them is female so we get to 2.5 million women 2.5 million main we just load populate the data with five million rows and what we want to do is we want to count how often does each gender show up very simple query can we speed it up somehow any ideas the answer is yes we can and I'm not showing you how to change parameters because we all know memory is cool and things like that we really want to understand the problem and what's gonna happen here is that to fix that we got to understand how post crystal segregation and if we do it that way it's not 960 milliseconds anymore it's only 526 it's it must be a miracle so we almost doubled our speed just by rewriting this query a bit so let's take a look instead of joining here we a grenade on the changer ID so we're not joining anymore and then doing the aggregation but we're aggregating on the chain their ID right and then we take this aggregation and then we do the join right did everybody get the concept l elaborate must be miracle of course it's a miracle and the answer is in the optimizer normally one of the core limitations in Postgres which has been around for 20 years and which is super hard to fix basically is we got to join first and then do the aggregation what it means is that we have to look up the gender 5 million times right so we have to really join 5 million times and then to the operation and then to the aggregation on the more expensive data type okay so that's a problem and this is why it takes 900 milliseconds if we go for the with statement so we get millions of lookups and this is kind of caused the deterioration in performance if we go for this kind of query where we do the aggregation on the ID first and the lookup later then we're gonna get this kind of plan so its first going to do the aggregation in the common table expression and then it's gonna join on the gender this is gonna speed up stuff so if you really have a lot of joining an aggregation it can not always but it might make sense to aggregate first and join later right the good news is one of my workmates has been working on a page for Postgres optimizer to fix this kind of stuff for I think one of the half years so it's that hard to fix it in the optimizer basically to make Postgres to that out of the box but it's not so easy because the the data type going into the aggregate function is changing and things like that so it's it's a super hard problem but for now until this goes into Postgres hopefully 14 one way to speed up aggregations is to do aggregation first joy later you have to try it out it's not always the case but it could be very beneficial right any questions anybody Wow I'm lost right no your nose so second thing is let's suppose we have 20 years of data and we're gonna check for a period so this is a classical example of time series data so we are counting the number of entries for each sensor and what we got here is one billion rows right and we might speed up this query and the problem in this case is that we have to select one billion rows out of twenty billion rows forget to search right good to find them and the answer to the problem in this case is the observation is we have to find one billion rows out of twenty billion rows and of course what you just learned before we got to create an index right so the index has to find one billion out of twenty billion and if you are on traditional hardware this could kill you right it's a lot of Petrie operations it's a lot of searching and the next thing is that a Petrie is super cool for searching but it comes with a price tag and the rule of thumb is that a b-tree entry is roughly 25 bytes so forget roughly right depends on fragmentation and stuff like that but it's roughly 25 bytes so if you get twenty five twenty billion rows times 25 bytes this is the price tag for your Petrie so it's gonna need some storage right so what you can do here is you can partition per year one thing you have to keep in mind partitioning per se is not a miracle there is no such thing as a miracle that fits all right if you can walk up a water which has happened historically I think it's a pretty bad idea if you want to go swimming right doesn't work you know and the same is with with Postgres Postgres features in this case petitioning is going to help out with with with finding the right partition so they can avoid searching for the data because they already know it's in this partition right and one of those features here you can use this a block range index and in Postgres we've got several types of indexes and one thing to fix the the time series stuff is this block range index so how does it work we take a megabyte of data here megabyte of data here megabyte of data here and we'll calculate the minimum and the maximum so instead of pointing to everybody like a Petri we just calculate the minimum at the maximum and say okay we got people from 20 to 40 here we got people from 40 to 60 and Hirakud people from 50 to 90 right not looking at anybody in particular so this is what a block range index does so it's sort of index tile partitioning you could say and the target here is that this guy is really small I told you that the Petri is roughly 25 bytes per entry and this guy is only 48 K right very nice but I told you it's gonna get the minimum of a megabyte of data minimum and maximum minimum and maximum minimum and maximum and this only works if your data is sort of sorted right we've got all that data if they test random the minimum at the maximum is not going to buy you anything because you got the global minimum and maximum here same here same here so this really only works on sorted data but it can be very beneficial for data warehousing right and it's usually something like 2,000 times smaller you know for for time series I can show you more in detail what else can we do we can do many things at once say if you want to speed up your database what you can do is you can do many things at once and one a couple of ways to do that is you can go for pre aggregation so if we go back to the example at the beginning with 340 billion rows all the historic state of the company we can pre aggregate because 1960 is not going to change anymore so we can pre calculate it so that we don't have to do it all over again every day the next thing is a sinker on a six can I'm going to show in a minute and the third thing is you can use so called grouping sets and partial aggregates let me give you a number if you got a terabyte of data and suppose you can read a terabyte of data and suppose you can read it at one gigabyte per second it takes 20 minutes to read the data just to read the data right it's a thousand seconds the thousand seconds is roughly 20 minutes right so only reading a terabyte of data at one gigabyte per second is 20 minutes right so if you have to read it 10 times right it's going to be expensive so it makes sense to read the data once and use it more often because in many cases the limitation on large amounts of data is IO you can you can have a sixty core machine but you cannot have an i/o system which is gonna give you 100 gigabytes per second right I mean you can but not easily no okay so it's a it's a it's also doing more than once and let's add one more column here with age and let's run something like this we count everybody group by gender and then we filter the young ones whatever young might be anybody fifteen one here bad news okay so this is the so called partial aggregate so basically if you do this without the filter it would be free queries mean meaning you have to read your terabyte of data three times but we're using more advanced SQL features you can often get a benefit because the only read once but it get many many result sets in the next interesting thing here from my point of view is the roll-up clause what it means is male-female total the roll-up is going to give you the bottom line if you do this with traditional SQL it would be six queries right meaning six terabytes of data instead of reading one terabyte of data so it makes a lot of since to dig into this modern-day scale features because it's in large amounts of data it's gonna help you to read data only once instead of 20 times okay and this is a bit this is something you you cannot fix with configuration you can only fix it with logic the counter-argument is but we use hibernate well then die gracefully the problem yeah and the next thing is the so called synchronous seek scan which was on the list before what does it mean suppose we got 10 terabytes of data and we want to read it all right and suppose the first operation this one is here right so breeding hundred terabytes of data and our first operation is somewhere here 40 40 terabytes a feed rate suppose a second query comes along and what post Chris does is the sequential scan does not start at the beginning it starts where the other one already is so instead of having one query here and one query here the second query hooks in here so that they go through the table at the same time so we read the data once and use it more often and then the second sequential scan is going to do the rest as soon as we're done suppose Chris basically tries to synchronize sequential scans if they are expensive enough right and what this means is that if you're running 10 queries on identical data so let's say 10 billion rows of measurement data start them concurrently not sequentially because you read the data once but you use it 10 times for 10 queries at the same time right so this is a synchronized seek scan which is super useful for for data warehousing so bottom line is to more at the same time and use modern SQL so to wrap it up first thing measure don't rely on something coming to you naturally measure find the top queries because high load is always related to the Pyrrhus it's never related to cache hit rate or whatever that's consequence right so always find the queries first second thing is check for missing indexes third thing is use more advanced SQL use too many things at once right and understand what you're doing right that's the most important thing so I made it in time thank you for your attention I hope you liked the presentation and you can catch me outside so follow us on Twitter like us on Facebook share the news and have a great conference thank you any questions anybody any questions if anybody has yeah yes Oh forever ten years maybe so when have synchronized six cans be introduced it was at least ten years ago so synchronous six cans at least ten years ago I don't remember which one it has been around for forever in the day right any more questions so if anybody is taking pictures are feel free to send it to me so we can Twitter the hell out of it and yeah have a nice conference thank you [Applause] \n",
            "Hello, and welcome to 5mins of Postgres. My name is Lukas, and today we're going\n",
            "to talk about tuning shared_buffers and whether 25% of RAM is a good choice. Now, I want to start with this\n",
            "tweet by Andres Freund that he wrote a couple of years ago. In this tweet, he was pointing out\n",
            "that the often repeated recommendation to not exceed 25% of RAM for\n",
            "shared_buffers in Postgres is wrong. The test that he ran back then was\n",
            "using a pgbench with scale 1500, that's roughly a 22 gigabyte database, and he\n",
            "was doing this on a local machine with a fast SSD drive and with 32 GB of RAM. He was testing with both 1 GB of\n",
            "shared_buffers, 8 GB, aka 25%, and 24 GB, which notably was slightly larger than the\n",
            "amount of data that he was working with. Back then, what he was showing, is\n",
            "that if you pick 25%, that would actually give you the worst performance\n",
            "out of the three, versus if you picked 24 gigabytes, that would\n",
            "actually get you to best performance. The main point here was, if your working\n",
            "set fits into memory, then choosing a shared_buffers that encompasses your\n",
            "working set gets you the best performance. More recently, Shaun from the EDB\n",
            "team was running a comprehensive set of tests to figure out how to\n",
            "set shared_buffers effectively. Earlier this month, he started\n",
            "by running tests with an HP DL360 server with local hard drives. This is also a system\n",
            "that has 64 GB of RAM. First he was doing a pgbench\n",
            "with a database size of 110 GB, so about double the size of RAM. He was testing different values of\n",
            "shared_buffers up to 32 GB, so 50%. For the pgbench test, performance\n",
            "did get better when there was a higher shared_buffers,\n",
            "like for example 50% of RAM. Shaun was also looking at pg_buffercache. What that shows you is how\n",
            "Postgres re-uses it's pages or has to go and fetch new pages. The usage count here is essentially a\n",
            "representation of whether something was reused or something had to be fetched\n",
            "from disk or the operating system cache. A higher usage count is better,\n",
            "and the maximum is five. So usagecount = 5, that means that a\n",
            "good portion of the workload was in shared_buffers, and so it could be\n",
            "reused, whereas usage_count 0 or 1 means it's less likely to be the case. In this pgbench test, some\n",
            "portion of the workload was reusable, but not a lot actually. And really what you can see here is that\n",
            "this represents a large portion of the working set exceeding shared_buffers. And that's why we have a lot of\n",
            "turnaround in shared buffers. He was also running a TPROC-C benchmark. Instead of pgbench, which kind of\n",
            "goes all over the place, TPROC-C has a smaller, active working set. You can see that a much larger\n",
            "shared buffers did give tremendous performance improvements. And the reason for that is that\n",
            "a lot of the data was in shared buffers and could be reused. He was also testing the TPROC-H benchmark. In this particular case a lower\n",
            "shared buffers performed better and gave you better performance. And the reason for that is that shared\n",
            "buffers wasn't really effective at caching the right pages, and so Postgres was\n",
            "essentially taking away space from the operating system cache that could have\n",
            "instead cached things more effectively. Now, earlier this week, Shaun re-ran these\n",
            "tests with 128 GB of RAM, double the RAM from last time and enterprise class SSDs. What's interesting with the pgbench here\n",
            "is that the pgbench performed exactly the same, no matter the shared_buffer setting. This pgbench had a working set\n",
            "of 219 GB, so this means that it was clearly beyond the RAM. But because the cost of fetching a page\n",
            "was so cheap, because these enterprise class SSDs, they're just really fast, it\n",
            "didn't really make a big difference how to tune shared_buffers here, because you\n",
            "didn't actually get that much benefit anyway versus getting it from disk. He also ran a TPROC-C benchmark again,\n",
            "and there you do actually see that a higher shared_buffers did benefit. Even though these SSDs are pretty\n",
            "fast, there is still a cost to going to the Linux page cache,\n",
            "and then actually going to disk. So it's just more CPU cycles\n",
            "that you're spending, versus if something is in the buffer cache,\n",
            "that's still going to be faster. We can see high usage counts, so this\n",
            "type of workload did perform better with a higher shared_buffer cache. Last but not least, he also did again run\n",
            "the TPROC-H benchmarks and there is still a benefit here to a lower shared_buffers\n",
            "for a data warehousing type workload. Now, Shaun makes two conclusions here. First he says that for a warehouse type\n",
            "workload, you should consider using the default of 128 MB of shared_buffers. The second thing he argues is\n",
            "that you should continue to follow the 25% rule for OLTP systems. And I would disagree slightly here. I actually think that Shaun's\n",
            "data shows that that's not necessarily the best choice. I would also argue that the trade-off here\n",
            "is, if you have a certain portion of your working set that can be reused, higher\n",
            "shared_buffers is going to be better. However, a larger shared_buffers makes\n",
            "it more expensive to get new pages, because the clock-sweep algorithm in\n",
            "Postgres has to look at more data. Thank you so much for listening. This was 5mins of Postgres. Subscribe to our YouTube channel\n",
            "to hear about next week's episode and talk to you next week. \n",
            "Hello, and welcome to 5mins of Postgres. My name is Lukas and today I want to talk\n",
            "about the pg_stat_io view and the everyday DBA perspective on why it matters. The pg_stat_io view is a new feature in\n",
            "the upcoming Postgres 16 release that was committed about three weeks ago. This was committed by Andres Freund,\n",
            "and was authored by Melanie Plageman. In this blog post by depesz on\n",
            "his blog, he talks about how to make use of this new view. And in fact, he references a post\n",
            "that I wrote a couple of weeks ago where I described some of\n",
            "the experience of reviewing this patch in previous iterations. I do want to take a look at this\n",
            "post though, because I think what depesz describes here is how\n",
            "he would use this as a DBA when this is released in Postgres 16. pg_stat_io tracks all the I/O\n",
            "activity in the Postgres system. And, really, what's new here is\n",
            "that this represents an all-up view that's very complete. Previously, you would have ways to get\n",
            "information about individual queries or individual tables, but there wasn't\n",
            "really a way to say: here's everything that happened, and how much of that was\n",
            "spent on autovacuum versus individual queries or background workers and such. Now, this looks like a lot of data,\n",
            "but what's nice about this is that it is actually fairly compact, in the\n",
            "sense that you will always get 30 rows. If your system is really busy\n",
            "or if the system is not busy. What really matters is which\n",
            "counter values are going up, how. As depesz describes here, these\n",
            "stats are global, so you can see the stats for the whole database server. He runs a \"pgbench\" to have a\n",
            "test case for the pg_stat_io data. This is a pgbench with scale factor 1000. It's a fairly reasonably sized database. Afterwards, pg_stat_io\n",
            "looks very different. You can sort this view\n",
            "by the number of reads. For example, we can say, give me\n",
            "the subsystem of Postgres that caused the most reads in this\n",
            "timeframe since the last reset. And so we can say that here\n",
            "the automatic worker spent the most time reading information. Next, the background workers, which\n",
            "are like parallel workers, and then regular client backends, so these are the\n",
            "individual connections to the database. Now, we can do the same for\n",
            "writes of course, here you can see the writes were actually mostly\n",
            "issued by the client backend. As depesz notes here, this doesn't\n",
            "actually tell us which tables or which individual backends are to blame, but it\n",
            "does help you pinpoint which subsystem of Postgres to look at, where you can\n",
            "then have other information, like from the \"pg_statio_user_tables\" to really\n",
            "dig into these specific statistics. There are a couple of\n",
            "other statistics here. Extends are something that's a bit\n",
            "special in Postgres, and they were previously not really visible. This happens when you're inserting\n",
            "data into a table and the table needs to grow to accommodate the\n",
            "amount of data you're inserting. Now, in this case, there were actually\n",
            "a lot of extends because this was essentially an empty table before. And this can be helpful to\n",
            "know and was hard to track in previous versions of Postgres. You can combine reads, writes\n",
            "and extends to understand how many bytes of I/O you're doing. Each of those is essentially a\n",
            "counter value of eight kilobyte pages. And so you can multiply that\n",
            "out to get how much was read. For example, when we have 7,000\n",
            "reads, that means it's roughly 55 megabytes of data that was read. It's important to note that this is data\n",
            "as read from the database's perspective. Your Linux operating system cache\n",
            "may actually cache things and Postgres doesn't know about that. There is a current patch in the\n",
            "current Postgres commit fest, which may make it into 16, we don't know\n",
            "yet, which will also track timing. I think to really get a good measurement\n",
            "whether these operations were cached in the operating system page cache\n",
            "or not, we'll need to also have timing information at this level. Sometimes you gotta be a bit\n",
            "careful to really make a final assessment just based on this data. Now, there's two other things that\n",
            "are interesting: Evictions are something that you didn't really\n",
            "have good visibility on before. And this is when something has to be\n",
            "written to disk because shared buffers, which is a fixed size array, essentially\n",
            "of sorts, doesn't have enough space for the new data that should be read in. And so Postgres evicts another buffer\n",
            "to then be able to load some other data. Reuses are a bit of a special case and\n",
            "this is mostly related to the special ring buffers that you can now see\n",
            "here with bulk reads and bulk writes. You can also just reset this view, the\n",
            "pg_stat_reset_shared function can be used to just reset the I/O statistics. For example here, you can see just the\n",
            "statistics for a single pgbench run. As depesz notes \"I wish I had\n",
            "it on my production server\". I wish too. And if you're interested in reading more\n",
            "about pg_stat_io, I wrote about pg_stat_io in a blog post a couple of weeks ago\n",
            "where you can also see more details about the bulk read and bulk write strategies\n",
            "and some other edge cases which are now more clearly visible with pg_stat_io. I'm really excited for Postgres 16. Gotta wait another, probably half\n",
            "a year for it to be available. But then we can finally have\n",
            "better I/O statistics in Postgres. Thank you so much for listening,\n",
            "this was 5mins of Postgres. Subscribe to our YouTube channel\n",
            "to hear about next week's episode and talk to you next week. \n",
            "Hello and welcome to 5mins of Postgres. My name is Lukas and today we're\n",
            "going to talk about optimizing bulk load performance in Postgres and\n",
            "how the COPY ring buffer works. This is a blog post by Laurenz Albe on\n",
            "the Cybertec blog, and he talks about the different ways how you can load\n",
            "data into Postgres most efficiently. He starts by creating a table\n",
            "and he inserts 10 million rows in a couple different ways. First of all, he inserts in a way where\n",
            "there's one transaction for each INSERT. That's the least efficient way to do it. Then, he inserts single INSERTs in\n",
            "a transaction, so that means he's issuing individual statements, but\n",
            "they're all in a single transaction. This will give quite a performance\n",
            "boost, as we can see at the end. Then he does single INSERTs, but with a\n",
            "prepared statement in one transaction. This reduces some of the overhead that\n",
            "Postgres has when coming up with execution plans for subsequent statement executions. Next, he does one that I've\n",
            "certainly seen myself a lot, which is a multi line INSERT. He does INSERT into the table and then he\n",
            "has multiple rows listed in the values. This is fairly efficient and\n",
            "Laurenz then also does a further iteration on this by using a prepared\n",
            "statement in one transaction. Now the last one here is my favorite\n",
            "and if you can use it, it's the best as you probably know, which is COPY. COPY is not part of the SQL standard,\n",
            "its a Postgres specific API. It allows you to load data in a\n",
            "way that's more narrowly focused just on bulk loading of data. And you have a particular way of inputting\n",
            "the data in the protocol on the wire. And so it is something that you will have\n",
            "to support on the application side, but it is going to be the fastest way to do it. At the end, Laurenz has a\n",
            "breakdown of different methods. First of all, the most important\n",
            "thing to realize is that COPY is just a lot, lot faster. Clearly the worst thing you\n",
            "can do is doing single inserts. For these 10 million rows in Laurenz's\n",
            "machine, which is a laptop, the 10,000,000 rows of single inserts took 9,000 seconds. Now COPY on the other\n",
            "hand, took just 14 seconds. And a COPY is also quite\n",
            "faster than just bulk INSERTs. If you have bulk INSERTs, that's roughly\n",
            "50 seconds here, but the COPY is still about four times faster than that. Clearly, COPY is the winner. Nothing is as fast as COPY. If you have to use INSERTs, do try to\n",
            "use multi-row inserts, because they are going to be the most efficient\n",
            "way of doing INSERT statements. Laurenz has a few tips on how\n",
            "you should tune your Postgres settings for bulk loads. This is important because Postgres\n",
            "has a checkpointing process where it has to ultimately flush out all\n",
            "the data that was written to disk. And this will depend\n",
            "a lot on two settings. The \"max_wal_size\" setting and\n",
            "the \"checkpoint_timeout\" setting. Checkpoints can either be caused by\n",
            "the \"max_wal_size\" being reached, which is a number in gigabytes or\n",
            "the checkpoint timeout being reached, which is a time based measurement. Generally what you want is your\n",
            "max_wal_size to be large enough so that you hit the checkpoint timeout timeframe. For example, if you want to have your\n",
            "checkpoints every 5 minutes or every 10 minutes,  then that's better than having\n",
            "checkpoints every 10 gigabytes of data. On many systems what makes sense\n",
            "is to raise max_wal_size so you can have more timed checkpoints and\n",
            "then consider increasing checkpoint timeout as well, in some cases. Now, Laurenz concludes this with saying:\n",
            "for bulk loads, you should do COPY. I do want to point out one more thing,\n",
            "and for this I actually pulled up Postgres 16 in a little benchmark. What I want to show you here is the\n",
            "impact that COPY has on the contents of shared buffers, and ultimately how\n",
            "a COPY is going to be much better for concurrent read behavior on your database. What we're going to do here is we're\n",
            "going to use podman on my local machine, and we're going to do a\n",
            "comparison between pgbench doing COPY and pgbench doing INSERTs. We're going to do that through pgbench's\n",
            "method of doing either client side or server side generation of the data. This is going to generate 128 megabytes of\n",
            "data, which is also the default for shared buffers on a stock Postgres install. I'm going to run this here in my terminal. And we're first going to set\n",
            "up the COPY version of this. I'm going to initialize this here with the\n",
            "client side generation of data in pgbench. This uses COPY. I'm also going to do the same here on\n",
            "the other terminal, and here I'm going to use the server side generation. Note the uppercase G at the end\n",
            "of that pgbench initialization. And so this here is going to use an\n",
            "INSERT statement on the server side. Now, I'm going to compare how the\n",
            "\"pg_stat_io\" data looks like for this. This is on the INSERT version. With \"pg_stat_io\", we can\n",
            "see how the data was written. So you can see that in the normal\n",
            "I/O context, we had about 16,000 extends, which means 16,000 new pages\n",
            "had to be created in our tables. On the other side here, you can\n",
            "see that we have the same number of extends, also about 16,000, but we do\n",
            "have them in the \"bulkwrite\" context. And this shows you the big difference\n",
            "between COPY and INSERT, being that Postgres has a particular way in its I/O\n",
            "subsystem to handle bulk writes using a ring buffer instead of just using  shared\n",
            "buffers in the regular I/O context. And the reason that matters can\n",
            "be easily seen with pgbuffercache. pgbuffercache is an extension\n",
            "that's bundled in Postgres, which you can use to introspect what's\n",
            "in Postgres' shared buffers. Now I can see here when I was running\n",
            "the INSERTs, I have about 16k buffers in shared buffers, which are part\n",
            "of the pgbench accounts table. Essentially that means the whole\n",
            "table is currently in shared buffers. Now, if I run the same thing on the COPY\n",
            "version of the benchmark, you can see that only 2k buffers are in shared buffers. So that means when you're doing an\n",
            "INSERT, you're fully using shared buffers to run through all the inserts,\n",
            "because each insert has to pass through the Postgres shared buffer cache. On COPY, Postgres also has to pass\n",
            "all of them through shared buffers, but it's able to reuse the buffers\n",
            "contents in a more efficient way. That means when you're doing a bulk load,\n",
            "it's much better to use COPY because here there was a lot of other data that\n",
            "could remain in shared buffers, didn't have to be evicted from shared buffers. Versus in the case of INSERTs, there\n",
            "might've been a lot of data that you actually wanted to have in shared buffers\n",
            "for reads, but it actually had to be moved out to be able to perform the INSERT. That's why generally on a busy\n",
            "system, it's also better to use COPY versus INSERTs to\n",
            "improve the caching performance. Thank you so much for listening. This was 5mins of Postgres. Subscribe to our YouTube channel\n",
            "to hear about next week's episode and talk to you next week. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reddit Scraping"
      ],
      "metadata": {
        "id": "DUhLhCsBl8e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subreddit_id = \"postgresql\" #@param {type:\"string\"}\n",
        "topic_classifier = \"new\" #@param ['top', 'hot', 'new'] {allow-input: true}\n",
        "\n",
        "url = \"https://www.reddit.com/r/{}/{}\".format(subreddit_id, topic_classifier)\n",
        "print(\"Requesting information (json file) from {}...\".format(url))\n",
        "\n",
        "headers = {\n",
        "    'User-Agent': 'shogz-bot'\n",
        "}\n",
        "\n",
        "response = requests.get(url + \".json\", headers=headers)\n",
        "if response.ok:\n",
        "  data = response.json()['data']\n",
        "  reddit_title = []\n",
        "  reddit_text = []\n",
        "  reddit_post_classification = []\n",
        "  for post in data['children']:\n",
        "    reddit_title.append(post['data']['title'])\n",
        "    reddit_text.append(post['data']['selftext'])\n",
        "  print(\"Number of scraped posted: {}\".format(len(reddit_title)))\n",
        "  view_number_of_comments = int(input(\"Number of Comments to Test:\"))\n",
        "  for i in range(view_number_of_comments):\n",
        "    print(\"---------- Analysis of Posts {} ----------\".format(i + 1))\n",
        "    # print(\"Post Title: {}\\nContent:{}\\n\\nAnalysis:{}\\n\".format(reddit_title[i], reddit_text[i], predict_user_mental_wellbeing(reddit_text[i])))\n",
        "    print(\"Post Title: {}\\nContent:{}\\n\".format(reddit_title[i], reddit_text[i]))\n",
        "else:\n",
        "  print('Error {}'.format(response.status_code))"
      ],
      "metadata": {
        "id": "zpwZ8i0Jmwam",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "745a87de-cc15-4712-e7d0-4555b715b1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requesting information (json file) from https://www.reddit.com/r/postgresql/new...\n",
            "Number of scraped posted: 25\n",
            "Number of Comments to Test:25\n",
            "---------- Analysis of Comment 1 ----------\n",
            "Post Title: Best strategies to backup a legacy production PostgreSQL\n",
            "Content:Hi guys,\n",
            "\n",
            "&amp;#x200B;\n",
            "\n",
            "I have a old PostgreSQL 9.6 running on a on premise infrastructure. It's size is apx 500Gb  \n",
            "It's live and the only production instance. Due to management issues, there are no current backup  \n",
            "\n",
            "\n",
            "What are the recommend route to backup this DB?  \n",
            "What are the settings I should check? I've never dealt with WAL, how to I check it?  \n",
            "Is there a tool that backups up to AWS S3 or something similar?  \n",
            "\n",
            "\n",
            "Thanks.  \n",
            "\n",
            "\n",
            "---------- Analysis of Comment 2 ----------\n",
            "Post Title: SpiderOak - unbuntu 22.04 run at boot NOT login\n",
            "Content:I have been running SpiderOakOne on a Suse install for many years as root.  But with ubuntu root isn't a user.  I'd like to run SpiderOakOne at boot.  Adding the command SpiderOakOne --headless in a cron only runs when the user is logged on.  I need it to run at boot as a service but can't get the permission right or make it run correctly.  Can someone help - I need this back my postgres files.  Maybe I need to to post to a different community?\n",
            "\n",
            "---------- Analysis of Comment 3 ----------\n",
            "Post Title: How We Scaled PostgreSQL to 350 TB+ (With 10B New Records/Day)\n",
            "Content:\n",
            "\n",
            "---------- Analysis of Comment 4 ----------\n",
            "Post Title: PostgreSQL vs MongoDB\n",
            "Content:Hello to everyone!  \n",
            "\n",
            "\n",
            "I have to develop a web platform that implement an user registration and a real-time chat. For these two reason I have a doubt on what Database I should use. Some collegue suggest me an hybrid solution, Mongo for the real-time chat and PostgreSQL for the historical data of the user registrated.  \n",
            "\n",
            "\n",
            "What do you think?  \n",
            "\n",
            "\n",
            "Thanks in advice for the answers ;)\n",
            "\n",
            "---------- Analysis of Comment 5 ----------\n",
            "Post Title: I build a AI-powered Chrome Extension to generate Queries and Tutorials (based on a given prompt/sentence)\n",
            "Content:Hi Folks,  \n",
            "I've created a chrome extension that uses AI to help non-expert SQL users to save time by generating error-free queries from simple text.\n",
            "\n",
            "* Avoid endless searches on internet and documentation\n",
            "* No need to switch tabs to get the result\n",
            "* Learn with a tutorial \n",
            "* Get results in seconds\n",
            "\n",
            "If you want to try it out (totally free, credit card not required), you can download it here: [Chrome web store: MagicFormula](https://chrome.google.com/webstore/detail/magicformula/dacblbllifgkolpkpocnnnahbgoccpfb)\n",
            "\n",
            "Cheers,  \n",
            "Nicolas R.\n",
            "\n",
            "---------- Analysis of Comment 6 ----------\n",
            "Post Title: Graphs on PostgreSQL\n",
            "Content:Have you seen this extension to PostgreSQL? It allows you to manipulate data with cypher query language - which I'm thinking it's easier than SQL... what do you think? \n",
            "\n",
            "It also has this tool to visualize the database as a graph [https://github.com/apache/age#graph-visualization-tool-for-age](https://github.com/apache/age#graph-visualization-tool-for-age).\n",
            "\n",
            "&amp;#x200B;\n",
            "\n",
            "https://i.redd.it/uufuhxwgnzxb1.gif\n",
            "\n",
            "---------- Analysis of Comment 7 ----------\n",
            "Post Title: Writing a storage engine for Postgres: an in-memory Table Access Method\n",
            "Content:\n",
            "\n",
            "---------- Analysis of Comment 8 ----------\n",
            "Post Title: PgBackrest: differential backup vs WAL archive ?\n",
            "Content:I have full backups scheduled everyday on a two-node cluster (primary and hot standby). In addition I have archive\\_timeout set to 60 seconds and I see pgbackrest is doing the right thing, recovery works.\n",
            "\n",
            "should I consider setting up more frequent differential backups during the day or is that not useful when I have my WAL archive going to remote storage ?\n",
            "\n",
            "I chose to go with self-hosting postgres for a small production SaaS app since managed hosting solutions quickly exceed my budget outside of the \"hobby\" offerings. I want to make sure i've tested all recovery solutions thoroughly. I'm OK with losing 60 seconds of data.\n",
            "\n",
            "---------- Analysis of Comment 9 ----------\n",
            "Post Title: Announcing DoltgreSQL\n",
            "Content:\n",
            "\n",
            "---------- Analysis of Comment 10 ----------\n",
            "Post Title: Was given the following criteria to access a PostgreSQL database as part of a job interview where I then have to answer specific analysis questions, but can't figure out how to access the database.\n",
            "Content:I was given:\n",
            "\n",
            "Host: (long address ending in \".................[10498929-0.b.db.ondigitalocean.com](https://underdog-data-interview-do-user-10498929-0.b.db.ondigitalocean.com)\"\n",
            "\n",
            "User:\n",
            "\n",
            "Port:\n",
            "\n",
            "Password:\n",
            "\n",
            "SSlmode: require\n",
            "\n",
            "&amp;#x200B;\n",
            "\n",
            "I've downloaded PosgreSQL but can't figure out how to access the database. Any help would be appreciated.\n",
            "\n",
            "&amp;#x200B;\n",
            "\n",
            "---------- Analysis of Comment 11 ----------\n",
            "Post Title: Migrating a Terabyte-Scale PostgreSQL Database With (Almost) Zero Downtime\n",
            "Content:\n",
            "\n",
            "---------- Analysis of Comment 12 ----------\n",
            "Post Title: Postgres vs MySQL: the impact of CPU overhead on performance\n",
            "Content:[https://smalldatum.blogspot.com/2023/10/postgres-vs-mysql-impact-of-cpu.html](https://smalldatum.blogspot.com/2023/10/postgres-vs-mysql-impact-of-cpu.html)\n",
            "\n",
            "---------- Analysis of Comment 13 ----------\n",
            "Post Title: I am not sure why I keep having this issue. It is a test server so its fine. I will delete it soon. The password is 12345abcde\n",
            "Content:\n",
            "\n",
            "---------- Analysis of Comment 14 ----------\n",
            "Post Title: Best cache option for postgresql db self hosted\n",
            "Content:Hi you guys recommend me cache for my postgresql? Its selfhost right now. Have almost 10M records. And complex queries that use mostly 10 filters.\n",
            "\n",
            "Someone recommended me Dozer and change my app from direct db connect to api rest or Grpc api. \n",
            "\n",
            "Im open to your opinions.\n",
            "\n",
            "Thank you\n",
            "\n",
            "---------- Analysis of Comment 15 ----------\n",
            "Post Title: Using Postgres Filter\n",
            "Content:\n",
            "\n",
            "---------- Analysis of Comment 16 ----------\n",
            "Post Title: How do I insert data into multiple tables in relation in the easiest way?\n",
            "Content:So here is a table between Author and Books\n",
            "\n",
            "```\n",
            "CREATE TABLE book\n",
            "(\n",
            "    id SERIAL,\n",
            "    name VARCHAR(20) NOT NULL,\n",
            "    PRIMARY KEY(id)\n",
            ");\n",
            "\n",
            "CREATE TABLE author\n",
            "(\n",
            "    id SERIAL,\n",
            "    name VARCHAR(20) NOT NULL,\n",
            "    book_id int REFERENCES book(id) ON DELETE CASCADE,\n",
            "    PRIMARY KEY(id)\n",
            ");\n",
            "```\n",
            "\n",
            "Now say I want to add Author named Jack who wrote a book titled Book1. We also want to add another Author Jane who co-Authored Book1, To do that I should first insert Book1 into the books table as\n",
            "\n",
            "``INSERT INTO book(name) VALUES('Book1');``\n",
            "\n",
            "Now I have to know what id the Book1 was given by the database, to do that I need to\n",
            "\n",
            "``SELECT id from book where id = 'Book1';``\n",
            "\n",
            "I get the id of Book1 now. I now have to insert information about authors. Let suppose there were already many Books in the book table and the id for Book1 came out to be 300. Then,\n",
            "\n",
            "```\n",
            "INSERT INTO author(name, book_id) VALUES('Jack', 300);\n",
            "\n",
            "INSERT INTO author(name, book_id) VALUES('Jane', 300);\n",
            "```\n",
            "\n",
            "Now, is there a better way to insert these values at one go? Rather than me first inserting value into book table and then finding out the id given to each book before I insert values in author table?\n",
            "\n",
            "I am learning on Postgres but would appreciate if it is portable.\n",
            "\n",
            "---------- Analysis of Comment 17 ----------\n",
            "Post Title: I made this tool to use AI to build MongoDB Dashboards. Would anyone using postgres be interested in trying this as well?\n",
            "Content:\n",
            "\n",
            "---------- Analysis of Comment 18 ----------\n",
            "Post Title: Real-time Change Data Capture for Postgres Partitioned Tables\n",
            "Content:Blog - [https://blog.peerdb.io/real-time-change-data-capture-for-postgres-partitioned-tables](https://blog.peerdb.io/real-time-change-data-capture-for-postgres-partitioned-tables)  \n",
            "Demo - [https://www.loom.com/share/bd622150edd046aea4d6ca66d38f0eb9](https://www.loom.com/share/bd622150edd046aea4d6ca66d38f0eb9)  \n",
            "This is one of my favorite features that we shipped recently. Building Change Data Capture (CDC) for Postgres Partitioned Tables involves handling various scenarios that need care and emphasis.\n",
            "\n",
            "We at [PeerDB](https://peerdb.io) are building a specialized data-movement tool for Postgres. So supporting this feature was a given and a self expectation. This feature was one of the top asks from customers. We will keep adding more such native features as we evolve. 🐘 😊\n",
            "\n",
            "Also, don't forget to see the demo - it was a very candid one. 😊 I enjoyed covering different scenarios incl. adding new partitions, adding new columns, dropping partitions etc and showing how PeerDB handles replication for each of these cases.\n",
            "\n",
            "---------- Analysis of Comment 19 ----------\n",
            "Post Title: Query (speed) problems with date format\n",
            "Content:Hey, I have one table which has a simple created_at column as timestampz. I want to get all rows from today/the current day.\n",
            "\n",
            "My first solution was \"created_at = current_date\" (talking about the current_date Postgres function).\n",
            "\n",
            "This seems like it’s wrong, so I thought it’s the type -&gt; \"created_at::date = current_date::date\" works fine and is only returning values from today. Without the type it’s not matching any rows.\n",
            "\n",
            "Then I noticed that the query is getting really slow, probably an index problem, because the index is on the timestampz type(?). My next change  was to use \"created_at = current_date::timestampz\". Sadly this isn’t returning any values, guess it isn’t matching either, even when both have ::timestampz.\n",
            "\n",
            "My questions are, is timestampz even the format which I should use? How do I solve that problem when performance is a problem? I can add explains here, but somehow I guess this is a simple general problem. \n",
            "\n",
            "Only found two posts on stackoverflow but they didn’t really help.\n",
            "\n",
            "---------- Analysis of Comment 20 ----------\n",
            "Post Title: Ensuring Reliability: Listening to Database Signals For Better User Experience\n",
            "Content:\n",
            "\n",
            "---------- Analysis of Comment 21 ----------\n",
            "Post Title: Versioning data in Postgres? Testing a git like approach\n",
            "Content:\n",
            "\n",
            "---------- Analysis of Comment 22 ----------\n",
            "Post Title: WHERE Clause Question\n",
            "Content:In the documentation it says page circa 120 \n",
            "\n",
            "\"where search_condition is any value expression (see Section 4.2) that returns a value of type boolean\".\n",
            "\n",
            "Referring back to section 4.2, I'll admit, I haven't understood every single one of these expressions but except for a function call, I can't think of a single one of these expressions that evaluates to boolean.\n",
            "\n",
            "Am I missing something?\n",
            "\n",
            "---------- Analysis of Comment 23 ----------\n",
            "Post Title: Merge two tables (no cartesian product) and repeat rows from one of them\n",
            "Content:Hello everyone,\n",
            "\n",
            "Let's say I have table `Foo` with a single column `foo` :\n",
            "\n",
            "| foo |\n",
            "|-----|\n",
            "| A   |\n",
            "| B   |\n",
            "| C   |\n",
            "| D   |\n",
            "| E   |\n",
            "| F   |\n",
            "| G   |\n",
            "\n",
            "And table `Bar` with single column `bar` :\n",
            "\n",
            "| bar |\n",
            "|-----|\n",
            "| 1   |\n",
            "| 2   |\n",
            "| 3   |\n",
            "| 4   |\n",
            "\n",
            "I want to merge both of them, not doing a cartesian product via `CROSS JOIN`, but getting one value from each, and repeating the `bar` table values. Like this ;\n",
            "\n",
            "| foo | bar |\n",
            "|-----|-----|\n",
            "| A   | 1   |\n",
            "| B   | 2   |\n",
            "| C   | 3   |\n",
            "| D   | 4   |\n",
            "| E   | 1   |\n",
            "| F   | 2   |\n",
            "| G   | 3   |\n",
            "\n",
            "I believe I have to use `ROW_NUMBER()` to join them, and `MOD()` to repeat values from `bar`, but I can't find how to properly do that with PostgreSQL (13 in my case).\n",
            "\n",
            "Or maybe there's another way..\n",
            "\n",
            "---------- Analysis of Comment 24 ----------\n",
            "Post Title: Safely hiding user account history\n",
            "Content:Creating a product that allows users to see their full account history, including orders placed, but also delete that history (to comply with GDPR, but also to do  the right thing).\n",
            "\n",
            "For legal reasons (for example, SOX compliance), we can't simply `DELETE FROM orders WHERE account_id = ?`. What I thought about doing is creating a `current_accounts` table.\n",
            "\n",
            "Each account has an entry in the `current_accounts` table. If the customer elects to delete *everything*, we simply delete all identifying information in the `accounts` table (and related records). However, if they just want their history deleted, we create a new `current_accounts` record, anonymize the previous record (including creation date), and assign their `account_id` to the `current_accounts.account_id` (with an appropriate constraint) and all previous orders point to the anonymized `current_accounts` record. We'd also try to apply this to all of our data backups, too (tricky, but they could be running postgresql instances.)\n",
            "\n",
            "Does that sound sufficient to anonymise things? We _could_ have the old `current_accounts` record point to a global \"anonymous account\", but that lumps all orders into a single account and I'm unsure of the implications of this. Is there more I should be aware of?\n",
            "\n",
            "---------- Analysis of Comment 25 ----------\n",
            "Post Title: How to connect pgsql installed in EC2 to your node app\n",
            "Content:I have installed postgreSQL 14 in an ec2 instance (Ubuntu) which is open to listen on port 5432. \n",
            "Now I have a node application in my local and I have to connect it to that instance database , how can I do that ? \n",
            "\n",
            "Ask me for any clarification or more info if needed. \n",
            "\n",
            "P.S. Thanks in advance\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quora Scraping"
      ],
      "metadata": {
        "id": "x_qZENw4mBfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def run(playwright: Playwright):\n",
        "    chromium = playwright.chromium # or \"firefox\" or \"webkit\".\n",
        "    browser = await chromium.launch()\n",
        "    page = await browser.new_page()\n",
        "    link = 'https://www.quora.com/How-can-you-optimize-the-performance-of-PostgreSQL-queries' # @param {type:\"string\"}\n",
        "    number_of_long_posts = 10 # @param {type:\"number\"}\n",
        "    min_post_length_in_chars = 50 # @param {type:\"number\"}\n",
        "    loading_time = 2 # @param {type:\"number\"}\n",
        "\n",
        "    await page.goto(link)\n",
        "    print(\"Sleeping for {} seconds (page loading)\".format(loading_time))\n",
        "    time.sleep(loading_time)\n",
        "    for i in range(number_of_long_posts):\n",
        "      try:\n",
        "        button = await page.get_by_text(\"Continue Reading\", exact=True).nth(i).click()\n",
        "        print(\"Post #{} has been expanded...\".format(i + 1))\n",
        "      except:\n",
        "        print(\"Not enough post data!! We scraped what was available to us.\")\n",
        "        break\n",
        "      await page.mouse.wheel(0,100)\n",
        "    html = page.inner_html(\"#mainContent\")\n",
        "    parser = BeautifulSoup(await html, \"html.parser\")\n",
        "    posts = parser.find_all(\"div\", {\"class\": \"q-box spacing_log_answer_content puppeteer_test_answer_content\"})\n",
        "    answers = []\n",
        "    for post in posts:\n",
        "      if len(post.text.strip()) > min_post_length_in_chars:\n",
        "        answers.append(post.text.strip())\n",
        "    content = ''\n",
        "    dir_path='drive/MyDrive/training_data'\n",
        "    # Write Transcript to Text File\n",
        "    file_name = 'test1' # @param {type:\"string\"}\n",
        "    file = open(f'{dir_path}/{file_name}-quora-posts.txt', 'w+')\n",
        "    print(\"After pruning, {} posts (text-only) remain.\".format(len(answers)))\n",
        "    for entry in answers:\n",
        "      content += entry + '\\n\\n'\n",
        "    await browser.close()\n",
        "    file.write(content)\n",
        "    print('Content has successfully been saved to the Drive!! :)')\n",
        "    file.close()\n",
        "\n",
        "async def main():\n",
        "    async with async_playwright() as playwright:\n",
        "        await run(playwright)\n",
        "asyncio.run(main())"
      ],
      "metadata": {
        "id": "pv3mczAPl-5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96024899-51eb-46c9-86e3-e218a83d7ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sleeping for 2 seconds (page loading)\n",
            "Post #1 has been expanded...\n",
            "Post #2 has been expanded...\n",
            "Post #3 has been expanded...\n",
            "Post #4 has been expanded...\n",
            "Post #5 has been expanded...\n",
            "Post #6 has been expanded...\n",
            "Post #7 has been expanded...\n",
            "Post #8 has been expanded...\n",
            "Not enough post data!! We scraped what was available to us.\n",
            "After pruning, 33 posts (text-only) remain.\n",
            "To optimize the performance of PostgreSQL queries, you can implement the following strategies: Design Efficient Queries: Write efficient queries that retrieve only the necessary data. Avoid using wildcard characters (*) in SELECT statements and retrieve specific columns instead. Use appropriate WHERE clauses to filter data and JOIN operations to minimize the number of rows processed. Indexing: Analyze your queries and identify frequently accessed columns in WHERE clauses or JOIN operations. Create indexes on those columns to speed up data retrieval. However, be cautious not to over-index, as it can impact write performance. Query Planning and Optimization: PostgreSQL has a sophisticated query optimizer that analyzes queries and determines the most efficient execution plan. Ensure that your statistics are up to date by running the ANALYZE command regularly. Use EXPLAIN and EXPLAIN ANALYZE to analyze query plans and identify potential performance bottlenecks. Adjust the configuration parameters related to query planning and optimization, such as work_mem and random_page_cost, based on your workload. Partitioning: If you have large tables, consider partitioning them based on certain criteria (e.g., date or range). Partitioning can improve query performance by allowing the system to scan smaller and more focused subsets of data. Denormalization: In some cases, denormalizing your database schema by duplicating data or adding redundant columns can improve query performance. This approach reduces the need for JOIN operations and simplifies complex queries. Connection Pooling: Implement connection pooling to reuse database connections instead of establishing a new connection for each query. Connection pooling reduces the overhead of establishing and tearing down connections, improving overall query performance. Tune Configuration Parameters: Review and adjust PostgreSQL configuration parameters to optimize performance based on your hardware resources and workload. Parameters like shared_buffers, work_mem, and effective_cache_size can significantly impact query performance. Experiment with different values to find the optimal settings for your environment. Monitor Performance: Utilize PostgreSQL's built-in monitoring tools, such as pg_stat_statements and pg_stat_activity, to monitor query performance and identify slow queries. Use tools like pgBadger or pg_stat_monitor for advanced performance monitoring and analysis. Use Materialized Views and Caching: Consider utilizing materialized views to precompute and store the results of complex queries. This approach can significantly improve query performance for frequently accessed data. Additionally, implement appropriate caching mechanisms, such as using Redis or memcached, to cache query results and reduce the need for repetitive queries. Regular Maintenance: Perform regular maintenance tasks, such as vacuuming and analyzing tables, to optimize database performance. Vacuuming removes dead rows and frees up space, while analyzing updates the statistics used by the query optimizer. Remember to benchmark and measure the impact of each optimization technique to ensure that it improves query performance effectively. Additionally, consider consulting PostgreSQL documentation, tuning guides, and seeking expert advice for more advanced performance optimization. I could also reccomend this guide - https://www.devart.com/dbforge/postgresql/studio/postgresql-performance-tuning-and-optimization\n",
            "\n",
            "Use bound variables instead of string concatenated WHERE conditions. This protects your code not only against SQL Injection, but also saves time to the internal SQL compiler as it avoids repeated compilation of similar statements with just only different where- values. If you need joins try to shrink the result set of each join- branch by specifying particular conditions to avoid the construction of huge (n) x (m) result sets. When building results over joined tables, avoid SQL engine cycling through huge result sets and evaluating detail queries for each of the result records (e g. when using fContinue ReadingUse bound variables instead of string concatenated WHERE conditions. This protects your code not only against SQL Injection, but also saves time to the internal SQL compiler as it avoids repeated compilation of similar statements with just only different where- values. If you need joins try to shrink the result set of each join- branch by specifying particular conditions to avoid the construction of huge (n) x (m) result sets. When building results over joined tables, avoid SQL engine cycling through huge result sets and evaluating detail queries for each of the result records (e g. when using functions which contain data access again). Try to understand the (most time consuming) statements of your developers, and try to understand how database engine will solve them. Then you probably will learn fast how to teach your developers producing more performant queries. Use ANALYZE statements. These principles are common not only to PostgreSQL, they are quite similar to all relational database systems.\n",
            "\n",
            "There are several ways to optimize the performance of PostgreSQL queries. One way is to use indexes to speed up data retrieval. Another way is to avoid unnecessary joins and subqueries that can slow down query execution. You can also use query caching and query optimization techniques to reduce the number of queries executed by the database. Additionally, you can tune PostgreSQL settings to improve performance for your specific use case. Finally, it's important to monitor and analyze query performance to identify and address any bottlenecks.\n",
            "\n",
            "It can have many reasons. Missing indexes and thus forcing the analyser to use full table scans. Badly constructed SQL. What may be for Oracle a good performing SQL, may seem slow on PostgreSQL. Doing a little tweaking may give you enormous performance gain. When you have in your list of fields a field that is retrieved by a select on another table, it could lead to a million selects out of that basic query. Bad for performance. I always advice to split the query or simplify so you get an idea on how big a result set is at a certain point. It certainly is an interesting part of development. There are lots of examples on the internet. Showing you what can be done to improve your query. Some are rather astonishing. Like a query that runs for 10 minutes being brought to a 5sec query. Have fun !\n",
            "\n",
            "There are a few strategies that can be used. Unfortunately none of them simple.1) If you want an approximate count you can use the statistics pg_stat_all_tables will give you an estimate of the number of rows 2) More complex is to keep track of the count using another table and a trigger to increment/decrement the count on insert/delete Short of that very fast disks or in memory database.\n",
            "\n",
            "Never crossed your mind that such generic questions cannot be answered? What are you facing? Under what circumstances? What is your execution environment? What are you expecting from the optimizer? Are you sure that the query optimizer is the appropriate tool to handle your problem? You can subscribe to PostgreSQL and then you will be able to submit bugs or improvements request to the people who develop and maintain PostgreSQL. Proceeding in this way will be far more efficient that crying in the desert like you do right now. In case they consider that you request impossible things they will letContinue ReadingNever crossed your mind that such generic questions cannot be answered? What are you facing? Under what circumstances? What is your execution environment? What are you expecting from the optimizer? Are you sure that the query optimizer is the appropriate tool to handle your problem? You can subscribe to PostgreSQL and then you will be able to submit bugs or improvements request to the people who develop and maintain PostgreSQL. Proceeding in this way will be far more efficient that crying in the desert like you do right now. In case they consider that you request impossible things they will let you know. If they find there is a chance to satisfy your request they will be more than happy doing it.\n",
            "\n",
            "For many years now, PostgreSQL has been the favorite of database aficionados for all the right reasons. It continues to surge in popularity by occupying a coveted place in very large deployments for its extraordinary features and stability. It was touted as the Database Management System (DBMS) of 2020 as enterprises around the world continued to reclaim control of their data with PostgreSQL. Industry experts consider it as the most transformative open source technology that is being used by some of the major players including Apple, Instagram, Skype, Reddit, and Spotify. Enterprises today follow an open source-first policy to curtail software licensing costs and empower development teams with added capabilities to build an application without waiting for permission from a centralized IT team. But the thing about PostgreSQL is that your work doesn’t stop with simply setting up the database correctly. You need to optimize PostgreSQL which is possible only with the right maintenance and updates. With every table, you add and each query you run, you will require ways and means to ensure that the PostgreSQL is at its best working optimally. There are things you need to do following a PostgreSQL setup. So let’s get into the PostgreSQL performance tuning and optimizing tips and tactics to get the most out of this relational database management system. Read on@Optimizing PostgreSQL Databases - MiCORE SolutionsFor many years now, PostgreSQL has been the favorite of database aficionados for all the right reasons. It continues to surge in popularity by occupying a coveted place in very large deployments for its extraordinary features and stability. It was touted as the Database Management System (DBMS) of 2020 as enterprises around the world continued […]https://micoresolutions.com/optimizing-postgresql-databases/\n",
            "\n",
            "Generally, there are several tricks that can improve query performance.Use a cloud server instead of a local serverYour databases may have several tables with over 10 million, or even 100 million rows. If you run the query on your local computer, the performance of the query depends on your computer’s RAM. If your computer doesn’t have a large RAM, try using cloud database management tools, such as Acho Studio, instead. Since these tools are built on cloud, they can utilize cloud computing to speed up the query.Shrink the size of the table that the query returnsTry to limit the size of the tabContinue ReadingGenerally, there are several tricks that can improve query performance.Use a cloud server instead of a local serverYour databases may have several tables with over 10 million, or even 100 million rows. If you run the query on your local computer, the performance of the query depends on your computer’s RAM. If your computer doesn’t have a large RAM, try using cloud database management tools, such as Acho Studio, instead. Since these tools are built on cloud, they can utilize cloud computing to speed up the query.Shrink the size of the table that the query returnsTry to limit the size of the table when you explore the database. For example, specify column names instead of selecting all columns when you query a table.SELECT column1, column2 FROM table1; Alternatively, use LIMIT to narrow the results returned.SELECT * FROM table1 LIMIT 10; Avoid using some inefficient statementsSome statements in SQL require a large amount of computation power and cause the query to run very slowly. For example, Where is more efficient than Having to define filters. Hope this answer helps!\n",
            "\n",
            "“So” inefficient? Could you please elaborate or quantify? The best optimizer is still the developer and the developer’s knowledge of data and data model.\n",
            "\n",
            "The same way you run a query in any other client-server relational database.If you need to run it from a command-line tool, you use that. In PostgreSQL’s case, that tool is called psql. You invoke it with the connection string (hostname, user, pwd, db) you need to connect to the instance you want to query, and type in your queries.If you need to run queries from an application, you’ll need a client connector library. For Python, you may use one of the several libraries discussed in this link: https://pynative.com/python-postgresql-tutorial/. There are PostgreSQL client connectors for dozens of languages.If you’re running the query from a data browser UI, you get a UI that can connect to PostgreSQL, enter your connection information to connect to the instance to query (similar to the command-line example), type in your query, and do whatever you need to do with the results.\n",
            "\n",
            "After using the Explain command as suggested by Steve Lacy i found out that given the current structure of my databse it will always be sequential so im going to have a cache with the most recent count to speed it up a little and execute the query only when necesary\n",
            "\n",
            "There's a nice presentation from Neil Conway  at http://neilconway.org/talks/executor.pdf\n",
            "\n",
            "There could be any of several reasons:Insufficient cache memorySlow storageMissing indexesPoor schema designOther server tuningPoorly designed SQL\n",
            "\n",
            "Why would you want to? The results of such a query will be EXACTLY the same if you have no GROUP BY clause at all! True for every RDBMS that uses SQL, not just PostgreSQL. OK, true only if the columns listed in the projection clause represent a unique set of values. So, if the question refers only to “all columns” listed in the projection clause, and those are not a unique set of values, then the answer is: The only way is to list every single column or its ordinal position in the GROUP BY clause. There is not short cut like asterisk (*) is used in the projection clause.\n",
            "\n",
            "How about this? UPDATE mytable  SET mycolumn = 'newvalue' WHERE myid in (1,2,3,...)  This assumes that you have a set of unique IDs that each child process can own, separate from other child processes. Also it assumes you have a single new value for each of those IDs per child process. You can use a CASE statement if the second assumption doesn't hold: UPDATE mytable  SET mycolumn =      CASE myid      WHEN 1 THEN 'one new value'      WHEN 2 THEN 'a different new value'     WHEN 3 THEN ...     END WHERE myid in (1,2,3,...)\n",
            "\n",
            "I don't actually know. No doubt it's documented somewhere in the manual. This limit will typically evolve with subsequent versions of PostgreSQL, so make sure you pick the correct version of the manual. It should also be relatively easy to find this out for yourself: just write a deeply nested query, and keep doubling the number of levels until you hit an error message. Then use the 'binary search' algorithm for the number of levels until you get at the “sweet spot”. Doubling the number of levels is easy: just copy/paste the previous query as a subquery to its deepest level. A simple query to starContinue ReadingI don't actually know. No doubt it's documented somewhere in the manual. This limit will typically evolve with subsequent versions of PostgreSQL, so make sure you pick the correct version of the manual. It should also be relatively easy to find this out for yourself: just write a deeply nested query, and keep doubling the number of levels until you hit an error message. Then use the 'binary search' algorithm for the number of levels until you get at the “sweet spot”. Doubling the number of levels is easy: just copy/paste the previous query as a subquery to its deepest level. A simple query to start from could be the following one:SELECT 1 WHERE 1=(SELECT 1          WHERE 1=(SELECT 1                   WHERE 1=(SELECT 1                            WHERE 1=1)))\n",
            "\n",
            "Perform explain query, try to get where it is consuming time.Try to add more filters as much as you can.Replace the expensive function with a low-cost function.You can also add indexes which will help for faster retrieval of data.\n",
            "\n",
            "OQ: Why is a PostgreSQL update slow on a relative small/medium table (PostgreSQL, query performance, PostgreSQL 9.4, DBA)? I don’t know for Postgres in particular but I can tell several reasons for any RDBMS.The table is live and there are locked rows you are trying to update. The update is held till these locks are gone.Columns of the table being updated are part of many indexes (maybe function-based ones) and the update requires also these indexes to be updated.In case this slowness is common for many of your tables there may be some general shortage in your resources like caches, cpu, I/O.In Postgres you can search log which nicely includes performance hints like ‘Increase your write-ahead space’ which is called WAL. Seek for these hints, read on them then decide if you want to apply and see what happens. In Postgres again there may be needed to vacuum the database’s files once in a while. If you forgot for some long time, this may be the reason too. PS. Did you check execution plan for this slow update statement?\n",
            "\n",
            "Without seeing your actual query, it's difficult to know what the issue is. General hotspots tend to be a bunch of cascades, or you're trying to update from a subquery. If it's the subquery, then you're probably using “in\" which is going to be slow. You want to rewrite your query to use “exists.” Postgres has low-level locking, so an update will generally not be slow. There is a whole page from the PostgreSQL documentation. The docs are world class and you should refer to them. For what it's worth, 9.4 is long past support, and you should consider updating your database.\n",
            "\n",
            "Establish a connection to one PostgreSQL instance, describe your query and execute it. You can use psql console, PgAdmin 4, phpPgAdmin, applications using the appropriate connector (.Net, jdbc, odbc, PDO, …) or elementary C or C++ code invoking libpq.\n",
            "\n",
            "This question is very unclear. I don't catch what “returning a query” could mean. Like any other RDBMS PostgreSQL follows a client-server model: A client be it a user application (can be a GUI or console program, a dynamic WEB page, a WEB service, code embeded in an Iot device …), pgAdmin, phpMyAdmin or psql issues a query as an argument of a request to the RDBMS server and this one returns THE RESULT OF THE QUERY as a response argument. A program issuing requests to PostgreSQL uses a PostgreSQL connector which recovers the query results thru a well documented interface. This one changes folloContinue ReadingThis question is very unclear. I don't catch what “returning a query” could mean. Like any other RDBMS PostgreSQL follows a client-server model: A client be it a user application (can be a GUI or console program, a dynamic WEB page, a WEB service, code embeded in an Iot device …), pgAdmin, phpMyAdmin or psql issues a query as an argument of a request to the RDBMS server and this one returns THE RESULT OF THE QUERY as a response argument. A program issuing requests to PostgreSQL uses a PostgreSQL connector which recovers the query results thru a well documented interface. This one changes following the programming language being used.\n",
            "\n",
            "If you’re experiencing slow performance when joining a view with PostgreSQL, there are a few things you can do to resolve the issue. One option is to create an index on the view. This will help speed up the query by allowing PostgreSQL to quickly find the data it needs. Another option is to use the greatest-n-per-group feature in PostgreSQL. This will allow you to return only the rows that are needed, which can help improve performance. Finally, you can also optimize your query by using the appropriate data types and avoiding unnecessary JOINs. By doing this, you can help PostgreSQL run your query more efficiently.\n",
            "\n",
            "Hello, and thanks for asking. I have prepared the complex article where different practices were shown. Take a look atdbForge tools for MySQL and Oracle · September 6, 2021How to Optimize a Query on PostgreSQL: Best TechniquesPostgreSQL query optimization is a balancing act between effort and benefit and the measures are subjective. This example will focus on reviewing the details within the dbForge Studio for PostgreSQL tool. Using proper tools we can eliminate a lot of the guesswork. When we think of query or cost optimization we really need to concentrate on two criteria: total query cost and execution time. Total Cost refers to the estimated execution cost, which is the PostgreSQL planner's guess at how long it will take to run the query. Basically it is calculated based on things like the number of disk page fetches, joins and sorts. PostgreSQL has its own native tools to estimate this: EXPLAIN and ANALYZE. Some things to note are: 1. EXPLAIN on its own creates the optimal execution plan and builds strategies based on statistics it keeps about the table but does not actually execute the query. 2. ANALYZE on its own collects and refreshes system statistics and does not execute the query. 3. When used in conjunction EXPLAIN and ANALYZE will execute the query. 4. When running an EXPLAIN ANALYZE against a query that inserts, deletes or updates your tables, be aware that EXPLAIN ANALYZE will make changes that you may need to rollback. When we run EXPLAIN ANALYZE or ANALYZE on a PostgreSQL query we will analyze the query and return an estimated execution time. If there are counts or loops on large tables this creates high to very high ANALYZE overhead— introducing timing discrepancies, which render the estimated execution time largely irrelevant as a reliable metric. The issue is that PostgreSQL uses the Volcano method to execute rows sequentially as soon as the row is output from a stage. This is highly memory and performance efficient. But for ANALYZE, when performing counts, it's problematic. This is because ANALYZE is timing the process. So for each row in a count(*) it must access the system clock twice (once for the start time and again for the end time) for each row. In a count(*) on a table with 1,000,000 rows this overhead will add up and hence the discrepancies between estimated execution times and real execution times. 1. It’s vital to use fresh statistics. Before continuing with optimization, refresh PostgreSQL and flush out any stale data. Use the following command to VACUUM the whole table and perform an ANALYZE function to collect and refresh the statistics. VACUUM(VERBOSE, ANALYZE); We can now go ahead and analyze the way PostgreSQL will execute our queries by prepending the EXPLAIN ANALYZE to the query and obtain the results. The execution time and the cost are the important factors to note here. To get a better visualization of the process, we’ll use dbForge Studio for PostgreSQL by Devart. With this suite of PostgreSQL tools we can break down the costs and visualize each step to take out the guesswork. dbForge Studio for PostgreSQL includes a Query Profiler tool, which can troubleshoot poorly performing queries. Let's consider a simple use case whereby we have been tasked to find each customer’s individual favorite actor and list their preference next to their email address for marketing to contact them when videos of their favorite actor is released. To create this scenario we will use a query against the sample dvdrental database available to download here. The scope of work is that the resultant query should be workable, readable and optimized. Our first query v1 is shown below. SELECT email, actor.first_name, actor.last_name, COUNT(actor.last_name) FROM (SELECT email, actor_id FROM (SELECT email, film_id FROM (SELECT email, inventory_id FROM customer AS cu JOIN rental ON cu.customer_id = rental.customer_id ORDER BY email) AS sq JOIN inventory ON sq.inventory_id = inventory.inventory_id) AS sq2 JOIN film_actor ON sq2.film_id = film_actor.film_id) AS sq3 JOIN actor ON sq3.actor_id = actor.actor_id GROUP BY email, actor.first_NAME, actor.last_name ORDER BY COUNT(actor.last_name) DESC;  Alt-Text: Query v1 returning data output The code however is very unwieldy as it consists of 3 sub-queries and several joins, so it fails on readability but importantly, it works. To optimize the query, we will first use the Query Profiler tool to get insight into how our query is performing. To run a query through the Query Profiler requires that you build an SQL query in the Query Editor and then switch to Query Profile mode with a single click on the Profiler icon on the upper toolbar.  Alt-Text: SQL Query in Query Editor Query Profiler is similar to EXPLAIN since it doesn’t execute the actual query, instead processing the optimal run strategy for total cost and actual time. Note: when you run a query in Query Profiler do not prepend with EXPLAIN or EXPLAIN ANALYZE. Just enter the query as it is. Query v1 SELECT email, actor.last_name, COUNT(actor.last_name) FROM (SELECT email, actor_id FROM (SELECT email, film_id FROM (SELECT email, inventory_id FROM customer AS cu JOIN rental ON cu.customer_id = rental.customer_id ORDER BY email) AS sq JOIN inventory ON sq.inventory_id = inventory.inventory_id) AS sq2 JOIN film_actor ON sq2.film_id = film_actor.film_id) AS sq3 JOIN actor ON sq3.actor_id = actor.actor_id GROUP BY email, actor.last_name ORDER BY COUNT(actor.last_name) DESC; Running this in the Query Profiler, the PostgreSQL engine will run EXPLAIN to plan the best way to perform this task, reporting it as the planning time. Remember, it does not actually run the query. The execution time and the cost are the important things to note here.  Alt-Text: Query Profiler Explain Results Actual Time: 376.486 ms (Estimated) Total Cost: 23360.89 However when we execute the query in real time we get the following result:  Alt-Text: Query v1 benchmark results So can we do better than 214 ms? Lets try changing our query to remove the subqueries as these do not help with code readability, one of the main goals. This gives us a query like this: Query v2: SELECT x.email,x.last_name,x.count FROM( SELECT cu.email, act.last_name, count(act.last_name) ,row_number() over(partition by email ORDER BY COUNT(act.last_name) DESC ) FROM customer as cu JOIN rental as ren ON cu.customer_id = ren.customer_id JOIN inventory as inv ON ren.inventory_id = inv.inventory_id JOIN film_actor as fil ON inv.film_id = fil.film_id JOIN actor as act ON act.actor_id = fil.actor_id GROUP BY cu.email,act.last_name ) as x WHERE row_number = 1 ORDER BY x.count DESC; We get a result of:  Alt-text: query v2 results Actual Time: 556.693 ms (Estimated) Cost: 22307.69 The change has had a detrimental effect by adding delay. When we run the query outside the Query Profiler (i.e., without EXPLAIN ANALYZE) the execution time is much lower than the forecast. Unfortunately, it’s still a lot slower than the first query and our benchmark.  Alt-Text: Query 2 test result analysis Although we have brought down the cost from 24508.60 to 22306.79, the actual run time forecast has shot up from 358.304ms to 617.821 ms or an increase of 57%. However, even the real-world execution times are still too high. This discrepancy is due to ANALYZE accessing the system clock twice for every row, which in our case is 2 million times per query, hence the discrepancy.  The PostgreSQL planner’s job is to devise an optimal execution plan for each query it receives. It is critical to the performance of the system that the planner chooses the optimal plan. The structure of the plan is a tree of plan nodes with each node having a specific task. Nodes at the bottom of the tree closest to startup perform the raw scan of rows from a table. Then higher up there are nodes tasked with joining, aggregating or sorting or other operations on the raw rows. The plan tree has a node at each step and shows the node type its task and the cost estimates applied to it by the planner. The top most node holds the execution time, but this is not the time taken to execute the query but the target time the planner hopes to reduce.  Alt-text: Query Profiler menu The tools we’ll be looking at are the Query Profiler, Plan Tree and the Top Operations. These can all be accessed through the Query Profile menu as seen above. Running both queries through dbForge’s Query Profiler produced the results that we can use to optimize a solution. For query v1 with 3 subqueries:  Alt-Text: Results for query v1 Note that the Execution time is 267.847 ms and the cost is 23360.89. For query v2 without the 3 subqueries and an optimized count method:  Alt-Text: Results for query v2 Note the Execution time is 640.592 ms but with a lower cost 22307.69. These results give us a clue, and we can use that to examine the timing of each query, since the execution cost is lower. If we now look at the Plan Tree for each query we can view the timing at each step to locate the command causing the runtime to soar. Query v1:  Alt-Text: Query v1 Plan Tree Query v2:  Alt-Text: Query v2 Plan Tree The key columns in the Plan Tree are startup cost, total cost, actual startup time, actual total time. When working with a Plan Tree or Plan Diagram, we work from the end backwards, or in this case, from the bottom up. Startup costs are those costs accumulated before even the first row has been accessed let alone output. Typically the startup costs are inclusive of any child costs, but not always. For example a Limit or a Seq Scan will have zero startup costs as neither requires any pre-processing. On the other hand a Sort has large startup costs as it needs to process the entire contents of the table before it can output a single row. By examining our Plan Tree, we can see that query v2 has an extra stage—the WindowsAgg function. This is used to count the number of occurrences of an actor’s name, as a measure of their popularity. But that spawns another child subquery of sequential scan. Now if we look at the Top Operations we can see the top operations for each query. Working from the top down we can see each step's contribution to the total cost and the actual time. Top Operations for Query 2:  Alt-Text: Table for Top Operations for Query 2 WindowAgg function is contributing significantly to the execution time—5.7% + 4.06% subquery scan is approx 10%. So, what is this WindowAgg function for? If we consider the code for Query-2 we can see a new count mechanism was added to obtain the top movie star per customer email address using the row_number() window function and then a subquery where row number = 1 to narrow in the results. //SELECT x.email,x.last_name,x.count FROM ( SELECT cu.email, act.last_name, count(act.last_name) // ,row_number() over(partition by email ORDER BY COUNT(act.last_name) DESC ) FROM customer as cu JOIN rental as ren ON cu.customer_id = ren.customer_id JOIN inventory as inv ON ren.inventory_id = inv.inventory_id JOIN film_actor as fil ON inv.film_id = fil.film_id JOIN actor as act ON act.actor_id = fil.actor_id GROUP BY cu.email,act.last_name //   ) as x //    WHERE row_number = 1 //    ORDER BY x.count DESC; Note the code commented out for the row_number window function and the subquery, where row_number=1. To verify that this join partition and Windows Aggregate is indeed contributing to the added delay, let's refactor the query to run using the same count method as in query v1. Query v3: SELECT cu.email, act.last_name, COUNT(act.last_name) FROM customer AS cu JOIN rental AS ren ON cu.customer_id = ren.customer_id JOIN inventory AS inv ON ren.inventory_id = inv.inventory_id JOIN film_actor AS fil ON inv.film_id = fil.film_id JOIN actor AS act ON act.actor_id = fil.actor_id GROUP BY cu.email, act.last_name ORDER BY COUNT(act.last_name) DESC; When we run the Query Profiler we get the following results:  Alt-Text: Query Test The Execution time is: 192.802 Total Cost: 21995.4 Notice in the diagram the WindowAgg is gone. If we go and execute the new query v3 we get the following output:  Alt-Text: Final Query Run Time So, comparing all three queries, we have the following picture:  This result meets our original requirements with a working query, clean readable code, and an optimized real execution time of 194 ms. Query optimization doesn't always go as planned. However, using these strategies in dbForge Studio for PostgreSQL Query Profiler helps to identify the bottleneck eliminating a lot of the guesswork. Just download a free 30-day trial and see how convenient and easy PostgreSQL development can be. By the way, you can find a lot of video tutorials at PostgreSQL Tutorials | Database Tools for PostgreSQLhttps://qr.ae/pGeLAFHope this example has helped you understand the techniques and practices of query optimization a little better.\n",
            "\n",
            "The question itself is the answer. The query takes long because it is not using the index.\n",
            "\n",
            "The first technique is to enable log_min_duration_statement in potgresql.conf to something more than x ms  and see what are the queries taking time.\n",
            "\n",
            "It really depends on what you're actually doing. Could this be modelled as a series of sequential tasks, that can be dispatched via a message queue? Or is it more like a critical section? In which case you can use locking at the row level to serialise operations.\n",
            "\n",
            "One. Its not using an index. Two. Try doing a this on the main tables being queried.vacuum full analyse myschema.mytable;\n",
            "\n",
            "If there is no index with the column referenced in the MAX() aggregation, then the engine has to retrieve every row in the table that matches other criteria in order to calculate the maximum value. Even if an appropriate index exists, if other columns are selected and query filters are supported by other indexes, the engine may have to retrieve and examine a large number of rows.\n",
            "\n",
            "It’s always possible having huge tables not normalized and running in troubles later. If you don’t like typing queries with multiple joins define a view based on the query with joins so you will write it only once.\n",
            "\n",
            "Yes, you can add multiple conditions in a single WHERE clause in a PostgreSQL query. The conditions are connected using logical operators, such as AND and OR, to specify the conditions under which a row is returned in the result set. Here is an example that demonstrates how to add multiple conditions in a WHERE clause SELECT *FROM employeesWHERE salary >= 50000AND department = 'Sales'; In this example, the query returns all rows from the \"employees\" table where the value in the \"salary\" column is greater than or equal to 50000 and the value in the \"department\" column is 'Sales'. The conditionsContinue ReadingYes, you can add multiple conditions in a single WHERE clause in a PostgreSQL query. The conditions are connected using logical operators, such as AND and OR, to specify the conditions under which a row is returned in the result set. Here is an example that demonstrates how to add multiple conditions in a WHERE clause SELECT *FROM employeesWHERE salary >= 50000AND department = 'Sales'; In this example, the query returns all rows from the \"employees\" table where the value in the \"salary\" column is greater than or equal to 50000 and the value in the \"department\" column is 'Sales'. The conditions are connected by the AND operator, so both conditions must be true for a row to be returned in the result set. You can also use OR to specify that one of the conditions must be true:SELECT * FROM employees WHERE salary >= 50000   OR department = 'Sales'; In this example, the query returns all rows from the \"employees\" table where the value in the \"salary\" column is greater than or equal to 50000 or the value in the \"department\" column is 'Sales'. The conditions are connected by the OR operator, so only one of the conditions must be true for a row to be returned in the result set.\n",
            "\n",
            "Query optimization include various steps. Few are as follows: Optimizing SELECT Statements Queries, in the form of SELECT statements, perform all the lookup operations in the database. Tuning these statements is a top priority, whether to achieve sub-second response times for dynamic web pages, or to chop hours off the time to generate huge overnight reports. Besides SELECT statements, the tuning techniques for queries also apply to constructs such as CREATE TABLE...AS SELECT, INSERT INTO...SELECT, and WHERE clauses in DELETE statements. Those statements have additional performance considerations because they combine write operations with the read-oriented query operations. In MySQL NDB Cluster 7.2 and later, the NDB storage engine supports a join pushdown optimization whereby a qualifying join is sent in its entirety to NDB Cluster data nodes, where it can be distributed among them and executed in parallel. For more information about this optimization, see Conditions for NDB pushdown joins. The main considerations for optimizing queries are:To make a slow SELECT ... WHERE query faster, the first thing to check is whether you can add an index. Set up indexes on columns used in the WHERE clause, to speed up evaluation, filtering, and the final retrieval of results. To avoid wasted disk space, construct a small set of indexes that speed up many related queries used in your application.Indexes are especially important for queries that reference different tables, using features such as joins and foreign keys. You can use the EXPLAIN statement to determine which indexes are used for a SELECT.Isolate and tune any part of the query, such as a function call, that takes excessive time. Depending on how the query is structured, a function could be called once for every row in the result set, or even once for every row in the table, greatly magnifying any inefficiency.Minimize the number of full table scans in your queries, particularly for big tables.Keep table statistics up to date by using the ANALYZE TABLE statement periodically, so the optimizer has the information needed to construct an efficient execution plan.Learn the tuning techniques, indexing techniques, and configuration parameters that are specific to the storage engine for each table. Both InnoDB and MyISAM have sets of guidelines for enabling and sustaining high performance in queries.Avoid transforming the query in ways that make it hard to understand, especially if the optimizer does some of the same transformations automatically.If a performance issue is not easily solved by one of the basic guidelines, investigate the internal details of the specific query by reading the EXPLAIN plan and adjusting your indexes, WHERE clauses, join clauses, and so on. (When you reach a certain level of expertise, reading the EXPLAIN plan might be your first step for every query.)Adjust the size and properties of the memory areas that MySQL uses for caching. With efficient use of the InnoDB buffer pool, MyISAM key cache, and the MySQL query cache, repeated queries run faster because the results are retrieved from memory the second and subsequent times.Even for a query that runs fast using the cache memory areas, you might still optimize further so that they require less cache memory, making your application more scalable. Scalability means that your application can handle more simultaneous users, larger requests, and so on without experiencing a big drop in performance.Deal with locking issues, where the speed of your query might be affected by other sessions accessing the tables at the same time.Subquery Optimization Certain optimizations are applicable to comparisons that use the IN operator to test subquery results (or that use =ANY, which is equivalent). This section discusses these optimizations, particularly with regard to the challenges that NULL values present. The last part of the discussion suggests how you can help the optimizer. Optimizing INFORMATION_SCHEMA Queries Applications that monitor databases may make frequent use of INFORMATION_SCHEMA tables. Certain types of queries for INFORMATION_SCHEMA tables can be optimized to execute more quickly. The goal is to minimize file operations (for example, scanning a directory or opening a table file) to collect the information that makes up these dynamic tables. Try to use constant lookup values for database and table names in the WHERE clause You can take advantage of this principle as follows:To look up databases or tables, use expressions that evaluate to a constant, such as literal values, functions that return a constant, or scalar subqueries.Avoid queries that use a nonconstant database name lookup value (or no lookup value) because they require a scan of the data directory to find matching database directory names.Within a database, avoid queries that use a nonconstant table name lookup value (or no lookup value) because they require a scan of the database directory to find matching table files.Optimizing Data Change Statements This section explains how to speed up data change statements: INSERT, UPDATE, and DELETE. Traditional OLTP applications and modern web applications typically do many small data change operations, where concurrency is vital. Data analysis and reporting applications typically run data change operations that affect many rows at once, where the main considerations is the I/O to write large amounts of data and keep indexes up-to-date. For inserting and updating large volumes of data (known in the industry as ETL, for “extract-transform-load”), sometimes you use other SQL statements or external commands, that mimic the effects of INSERT, UPDATE, and DELETE statements. Optimizing Database Privileges The more complex your privilege setup, the more overhead applies to all SQL statements. Simplifying the privileges established by GRANT statements enables MySQL to reduce permission-checking overhead when clients execute statements. For example, if you do not grant any table-level or column-level privileges, the server need not ever check the contents of the tables_priv and columns_priv tables. Similarly, if you place no resource limits on any accounts, the server does not have to perform resource counting. If you have a very high statement-processing load, consider using a simplified grant structure to reduce permission-checking overhead. Other Optimization Tips This section lists a number of miscellaneous tips for improving query processing speed: If your application makes several database requests to perform related updates, combining the statements into a stored routine can help performance. Similarly, if your application computes a single result based on several column values or large volumes of data, combining the computation into a UDF (user-defined function) can help performance. The resulting fast database operations are then available to be reused by other queries, applications, and even code written in different programming languages.To fix any compression issues that occur with ARCHIVE tables, use OPTIMIZE TABLE.If possible, classify reports as “live” or as “statistical”, where data needed for statistical reports is created only from summary tables that are generated periodically from the live data.If you have data that does not conform well to a rows-and-columns table structure, you can pack and store data into a BLOB column. In this case, you must provide code in your application to pack and unpack information, but this might save I/O operations to read and write the sets of related values.With Web servers, store images and other binary assets as files, with the path name stored in the database rather than the file itself. Most Web servers are better at caching files than database contents, so using files is generally faster. (Although you must handle backups and storage issues yourself in this case.)If you need really high speed, look at the low-level MySQL interfaces. For example, by accessing the MySQL InnoDB or MyISAM storage engine directly, you could get a substantial speed increase compared to using the SQL interface.Replication can provide a performance benefit for some operations. You can distribute client retrievals among replication servers to split up the load. To avoid slowing down the master while making backups, you can make backups using a slave server.\n",
            "\n",
            "They are both quite equal for CRUD queries. Buy a faster disk if you want better performance. Now, I have seen queries that fill many high resolution monitors, when you have such a complicated queries, you might start to wonder. You can tweak and tune as much as you like, and is different for everyone, there is never one true config. When you have to go beyond an ssd based machine with 4 cpu cores, 8 threads and 32–64GB memory, then you have enough money to buy two database servers and learn to study either sharding, load balancing or master slave configurations, and after that you can add as manContinue ReadingThey are both quite equal for CRUD queries. Buy a faster disk if you want better performance. Now, I have seen queries that fill many high resolution monitors, when you have such a complicated queries, you might start to wonder. You can tweak and tune as much as you like, and is different for everyone, there is never one true config. When you have to go beyond an ssd based machine with 4 cpu cores, 8 threads and 32–64GB memory, then you have enough money to buy two database servers and learn to study either sharding, load balancing or master slave configurations, and after that you can add as many slaves as you want, well, 4–5 perhaps. When you are there, you need to probably look at different solutions all together, f.ex. elastic search or hadoop or something like that. But the modern computer has your back. You can get terabytes in memory, raids of PCIe ssd disks, you can easily have 48 core machine and even more threads. The speed difference is quite small. Both Mysql and Posgres have been tweaked and tuned for decades, they are very fast. Just keep in mind that the basic setup, that comes out of the box, is more often than not not production ready when you are making tons of money.\n",
            "\n",
            "A2A, thanks... I don't know the exact MySQL query optimizer internals, but it appears to be a fairly standard statistics-driven cost-based query optimizer.  For a general description of cost-based query optimizers, this wiki page is a decent start: Query optimization, but at the end of the day, what query optimizers want to do is find the route to the query answer that visits the fewest number of pages possible. A related post I wrote a bit ago: How does SQL execute a query? Anyway, to start with you have to define exactly what query optimizers do; they're most analogous to a programming language compiler.  They take your SQL input, after it's been processed by a parser, and build an \"execution plan\" data structure that is used by the execution part of the RDBMS to actually answer the query.   In query optimizing, there are a lot of considerations that an optimizer has to account for in its analysis:How to order any joins?  Join order is the sequence used to walk through the tables in the query as the join is executing.  As a rule, you want the tables that produce the smaller working set visited earliest, especially for the \"anchor table\" in the join, as it (or a subset of it if there's other WHERE-clause predicates on it) will have to be fully traversed by the query.  In MySQL, the STRAIGHT_JOIN hint lets the user dictate left-to-right FROM-clause join order.What indexes to use, or is it cheaper to skip the index and use a tablescan?  Note that indexes aren't always best, particularly if a query ends up visiting a significant chunk of the table's rows.  The first two considerations are informed by table statistics, so it's a good idea to run ANALYZE TABLE to update them, especially if the relative sizes of tables change (ie, a table suddenly becomes large or has a bunch of rows deleted).What types of join algorithms can be considered?  Different RDBMSs have different join algorithm capabilities.  MySQL is limited in that it only supports Nested loop join, although it can use Nested loop over index/PK to do the lookup on the target table.  So, in this criterion, MySQL's query optimizer has it easy :)Whether temporary structures can be used versus always hitting the disk/buffer pool for rows every time we visit the table?Does the storage engine or storage media have any special properties that may make certain routes better than others?  For instance, InnoDB uses a table's primary key to organize base table data, making lookups using the PK particularly fast versus using secondary indexes (and this is why you should _always_ have a defined PK on any nontrivial InnoDB table).  Also, some optimizers, and apparently some versions of MySQL, can account for the fact that a table is stored in SSD versus spinning disk.  This would make a difference if joining two tables stored on different storage media.How to figure out all this stuff without taking a long time simply doing the analysis?  This is a far bigger consideration than one may think, especially for complex joins involving lots of tables as the solution space can get huge very easily.  Query optimizers have heuristics that prune unpromising query paths quickly so they don't have to bother doing a full analysis on them, but like any heuristic of this kind, this occasionally means the best query plan gets thrown out.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT"
      ],
      "metadata": {
        "id": "ORtH7T52mEMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Large Language Model"
      ],
      "metadata": {
        "id": "zCZ2suxC57n3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the baseline model we use to pass a query.\n",
        "# We can use this as the control condition when we compare it to the results obtained by passing the same query\n",
        "# and finetuning the LLM for Postgresql QA related activities and/or usages.\n",
        "# Last Update: 2023-11-05 (9:39 AM)\n",
        "\n",
        "base_generator = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model='gpt2'\n",
        ")\n",
        "\n",
        "query = 'How can I finetune a Postgresql server with parameters?' # @param{type:\"string\"}\n",
        "base_test = base_generator(query,\n",
        "                 max_length=80 # @param{type:\"integer\"}\n",
        "                           ,\n",
        "                 early_stopping=True)\n",
        "print(base_test)"
      ],
      "metadata": {
        "id": "OcV3faR7mHhD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1b2aff1-0f88-49c1-d480-987ebf7aefc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': 'How can I finetune a Postgresql server with parameters?\\n\\nPostgresql requires you to enable --skip-trees to run a pre-defined configuration file. However, the recommended configuration setting for Postgresql is a bit weird, so let\\'s set these settings here:\\n\\n--skip-trees=\"Postgresql://localhost:3000/root/.git/'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset('json', data_files='drive/MyDrive/qa_data.json', split=\"train\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(example):\n",
        "    output_texts = []\n",
        "    for i in range(len(example['Question'])):\n",
        "        text = f\"### Question: {example['Question'][i]}\\n ### Answer: {example['Answer'][i]}\"\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "response_template = \" ### Answer:\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/\",\n",
        "    auto_find_batch_size=True,\n",
        "    num_train_epochs=30,\n",
        "    learning_rate=1E-4,\n",
        "    logging_steps=len(dataset['Question']),\n",
        "    fp16=True # Train Faster OwO\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    max_seq_length=512,\n",
        "    data_collator=collator\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561,
          "referenced_widgets": [
            "6debbb4aacea45119818d1f8fc3f8a31",
            "2abade3efd504de9b028188f8d308a53",
            "d3a9f1fc834e469abfdae39421031520",
            "3a7f876f1d354fe2b7889e912ce9641e",
            "991cb0afe1924a60bb0fac26237bb9cf",
            "0f73799f73864ffe91c303a29b400bfd",
            "108398cdfb6e4d258892f4107370ba0e",
            "0e182cebc092496cae4a42e5b91cef8f",
            "55fd86e9a111462681686de1b117a0fc",
            "d3e1d62e0a674dcd9d95ba6f6ff0d509",
            "3d4d74b2da5243ad93a0e2a379822a32",
            "57a8b33ed00f4e0ebcb0be65e7ddfca0",
            "908284fde66c479b8e05fc3bfb11cc05",
            "65587ab1f6aa4d5bb85a2ac979fff241",
            "54f92e410bcf4981be39abb0ab749c39",
            "18a678b89b7648c98f6c304a95acf1a5",
            "a4644b519e3741e48c4a5486fd28ea24",
            "541ac43e4c1540e7a489491758c84ba8",
            "cc9a7215caf94535804906dc25f1bfa2",
            "4ed8cf8c1e32467892bee21966450f41",
            "6197738ead1e4d6bbcc7f0a95f8c82e0",
            "77411c7b048443dca52d89400a7033cd",
            "7dcfec40520141069773ae0abc5f4b47",
            "4f04ff8f37754cc69a861f72c3f2d2bb",
            "cbd548e06fd4403d913d451c5b796728",
            "ca5d926c84a2430d9491765370872a48",
            "fe25dfce69484e7c8d59ad29ee5ab041",
            "f095ca0ee1744820898b77b13eda17cb",
            "8df199d7d61b43fca7b7ef58940e7992",
            "41d6658555a2424bb5f03654573bc3c7",
            "ef25d501bcd143ddbda0f461aa64f961",
            "9fb6c48486424e1d8fdb94405164985c",
            "7f7637d959944fdaab6663710380c7f8",
            "fed40efe61fd42d8b817601a634525d6",
            "597420bbbeca4ea2b95ca274fc9bf2b5",
            "d371edeb48d941938a44da1ef5f25aea",
            "92b5385689cb46cc9ac3907695b5f237",
            "ac98661c88fa4d9a88ba7e18b0b71fc1",
            "550bad64c378414fb831b202bd6fe0df",
            "e316b6d07a1e470bb37e6df3db947144",
            "2cebc973f9db47879701753976b48616",
            "38d9c5cef31c4ba48c1d608083d0cded",
            "c4b9c5f66dfb4f3087b79f6db772c2dd",
            "95388bcea0bf41c986e10d661e2551dd",
            "be3a28b2e4a1455bb639a6770eaacb81",
            "b12b07d8e5c24ed181f7f3cabec23285",
            "8b55cdd5b7a241bc9be11dbd9fda3202",
            "eaef3cbdd75e4785b307525ea56db35b",
            "a70fe5cbcc994078abc54b7eba9efcab",
            "94faa6c6d38a41b88a447163fdef9345",
            "7d658739ed6242a3b7370055415841c3",
            "0a76dfed99d44a23aa065a89ad361014",
            "a383fe8c920e42b7b0a8e96188a5621c",
            "0803d8e230a74226956089d6b957b46c",
            "9786f103e4604cc29b67c6186a224558",
            "dd888022bf0e4caf8e32df48b5b23771",
            "0348c2bcb7ae48bfbdbae235341b8ece",
            "177f790316f142329fbbbca1670a7377",
            "1176f5c93a704f8187ff6100fe0ec472",
            "62614f4f7d2048b38a1d2944ef5868be",
            "beab94bc521f46d89f2b04aa0a73912a",
            "5f9f0c8b2cd241588c9d047ce0992e37",
            "2bdd4f9d7fdf4b658e3db81a060818b1",
            "36d28bff2433435f8158862025110664",
            "793deb07f6a14e759bc2e4cddbf94ab0",
            "32c047e180e3441d9de132a9d6c0f79c",
            "abb3b563336a42f9b5b015a59c30ba4a",
            "5b767ac70b23480e908c9f96ee88de02",
            "67a0b1e7c9e841a799f8eeac5b10fbcb",
            "0be0a83ede9a4b26ae1370edd1106558",
            "99f7693790a1411dad4b64e7f86173a8",
            "6c5f960fff214769b0683d5d335889bc",
            "29d0fa3ed339428e9a2df8ebcb3af8da",
            "4b443f83d4324918b6541253317c9385",
            "44f25f6cd62d485ebac385a7ab2e37dc",
            "f94185a5b516456c9d33565e0ee82add",
            "4ebe190598464c7ab725dad9f62c2d11",
            "876aab1dd6164ff1bf28d46e0087c28a",
            "237e0db6dc104bd197792348b0df55e8",
            "20b68ffce5ef449f8e0d2f5000b67432",
            "5d3befb7a30d4a4fa4b619fb5fcb8a24",
            "238f0d1205854aafaf2fbe12e029bd6c",
            "802dc597fe224c469a94942c175edc6f",
            "a3caa01a433c423697cb8aad6ac753da",
            "26a12064c99f4f8593b7673bd4011e1b",
            "18edd0e16b424d28adc662355d05ef8e",
            "cb0c88b2e51d446cafdc749d610403fb",
            "9e492c61c2034407b61572e5f69fac8e",
            "7267a11584014fe2a701020d32d6ca9b",
            "996b1db325564b01bfdfbd29f6ed4b85",
            "a83f42c72a1c4b70a2f6255102fb9466",
            "34292a4190f54323b024637edf02a4a1",
            "fd814e7c3a3e487c9613f644bd5cea35",
            "bb27052611e84cfcab970d9dd51df093",
            "2b9706e7dfa24700abac8036ad0878d9",
            "4eb528925850454db9ac06a96feeab5d",
            "89474a1af8c541108cb50ca56716e505",
            "07adf6cd556f41d48e64b27614ee8b09",
            "80a5da168acf455297230216b17d579f",
            "cc98937a138444479e97f0b216b704ef",
            "7beb0b054aa64d6badbd9e0ad9ba3d9f",
            "4374359398c04fc2ba687d2fb8b6b82f",
            "de9d72c5e4604c2792709ef5b0003fcb",
            "5757dc9f6b18430e896f60e3e83553bd",
            "214b69019400402caa86472191075267",
            "c93178eceb5b429f918a4e55dae4f5a5",
            "237bd2bdefd8461bab2261b56dbe04b1",
            "c6e54462ff274ae1beaae28c4543ad76",
            "77fcc4fac3b7446e8a19251e4df24be6",
            "db76710e8d834e63892208d395473310"
          ]
        },
        "id": "URFY5Yn06CNP",
        "outputId": "978a5dfd-ef38-44fd-f0d9-a79be84612b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6debbb4aacea45119818d1f8fc3f8a31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57a8b33ed00f4e0ebcb0be65e7ddfca0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7dcfec40520141069773ae0abc5f4b47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fed40efe61fd42d8b817601a634525d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be3a28b2e4a1455bb639a6770eaacb81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd888022bf0e4caf8e32df48b5b23771"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abb3b563336a42f9b5b015a59c30ba4a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "876aab1dd6164ff1bf28d46e0087c28a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7267a11584014fe2a701020d32d6ca9b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/186 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc98937a138444479e97f0b216b704ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [720/720 01:49, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.963400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>0.157800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>558</td>\n",
              "      <td>0.056600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=720, training_loss=0.3114636633131239, metrics={'train_runtime': 112.4457, 'train_samples_per_second': 49.624, 'train_steps_per_second': 6.403, 'total_flos': 267106800384000.0, 'train_loss': 0.3114636633131239, 'epoch': 30.0})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline('text-generation', model=model.to('cpu'), tokenizer=tokenizer)\n",
        "query = \"How can I optimize my PostgreSQL database system for read performance?\" # @param {types:\"string\"}\n",
        "response_length_in_chars = 180 # @param {type:\"integer\"}\n",
        "number_of_responses = 10 # @param {type:\"integer\"}\n",
        "\n",
        "generator(\n",
        "    # Multinomial Sampling\n",
        "    query,\n",
        "    max_length=response_length_in_chars,\n",
        "    num_return_sequences=number_of_responses,\n",
        "    early_stopping=True,\n",
        "    temperature=0.7,\n",
        "    repetition_penalty=1.05,\n",
        "    do_sample=True,\n",
        "    num_beams=1,\n",
        "    top_p=0.90,\n",
        "    remove_invalid_values=True\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "297Y4Jg77jnI",
        "outputId": "d2b71e52-8d51-47cd-e791-e7969fac6557"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:399: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"How can I optimize my PostgreSQL database system for read performance?\\nIn the above example, I used'shared_buffers' to manage cache data efficiently. However, this may increase memory usage and affect data durability. A recommended value is typically 8MB, but setting it excessively high can lead to resource contention and negatively impact system performance. Avoid extreme values, as they may lead to resource contention. Long-term plans are advisable, as they may improve system stability. However, excessive use may negatively affect system performance. Be cautious about long-term plans, as they can lead to resource contention. Use judiciously designed applications, especially on underpowered hardware. Always monitor the impact of an application's usage on your system. Additionally, adjust the'max_connections' and'max_connections' parameters to balance resource utilization and performance.\\n\\nRelated Reading:\"},\n",
              " {'generated_text': \"How can I optimize my PostgreSQL database system for read performance?\\n, by optimizing various parameters and settings. For instance, consider increasing'shared_buffers' to a recommended value of 25% of available RAM. Be cautious not to allocate too much memory, as it can lead to resource contention. A common recommendation is to set'shared_buffers' to 25% of available RAM. However, be cautious not to allocate too much memory, as it can lead to resource contention. A recommended value is around 4MB. Excessive values can lead to resource contention. Longer values may reduce resource usage but may lead to increased memory usage. Avoid using excessive values in your workload. Be cautious when adjusting these settings. Always monitor the impact of a change on your system's performance. Be cautious not to disable custom hardware at all times. Ensure your tables are well-designed for read and\"},\n",
              " {'generated_text': \"How can I optimize my PostgreSQL database system for read performance?\\n, particularly the'shared_buffers' parameter. It's crucial to configure this parameter properly, as it can impact read performance. A recommended value is around 5000MB. Excessive values can lead to high memory usage and potential resource contention. Lower values can reduce these concerns but be cautious as they may lead to increased memory usage and potential resource contention.\\nJust like with SSDs, keep this parameter under control and use it judiciously. Avoid using overly powerful storage technologies like SSDs or HDDs due to potential resource contention. Always monitor the impact of resource usage on your system.\\nDo not disable this parameter based on your specific workload. The combination of high-end storage and high-availability can make this decision a difficult choice. However, be mindful of memory usage and cache eviction when using powerful storage. Be\"},\n",
              " {'generated_text': \"How can I optimize my PostgreSQL database system for read performance? Consider configuring'shared_buffers' to allow asynchronous replication. This will speed up query execution, but be careful as setting a value too high can consume excessive memory and impact other database operations. A recommended value is around 5000MB. Excessive values can lead to resource contention and affect system stability. A common recommendation is to set'shared_buffers' to 25% of your available memory. Be cautious not to allocate too much memory, as it can lead to resource contention. Avoid using too much memory, as it can lead to resource exhaustion. Avoid using too much memory, as it can lead to resource contention. Be cautious about resource contention, as it can lead to resource exhaustion. Avoid using too much memory, as it can lead to resource contention. Monitor and test to determine the impact on your system's performance.\"},\n",
              " {'generated_text': \"How can I optimize my PostgreSQL database system for read performance?\\nJust like with any other query, you can optimize your application's performance. A well-balanced configuration can reduce write-related overhead and improve read performance. However, adjusting the'max_connections' parameter can impact performance. Be cautious not to set it too high, as it may lead to resource contention. A recommended value is around 20 connections, but adjust based on available resources.\\nDon't forget about'max_connections' – it can increase in size and affect data durability. A recommended value is typically between 5-10 connections, but adjust based on available resources.\\nAlways monitor the connection usage and adjust its level of priority if necessary. Avoid using excessive resource utilization. Always use reliable connections for complex queries. Always monitor connection usage and adjust its level above and beyond. Avoid extreme connections, as they can\"},\n",
              " {'generated_text': \"How can I optimize my PostgreSQL database system for read performance?\\nIn this article, I like to use the 'work_mem' parameter to control the memory allocated for sorting and hashing operations. Increasing 'work_mem' can help complex queries, but setting it too high can lead to memory pressure. A recommended value is around 4MB. Excessive settings can lead to high memory usage and potential resource contention. Be cautious when setting a value, as it can impact write performance. Always set a recommended value based on your specific data plan.\"},\n",
              " {'generated_text': 'How can I optimize my PostgreSQL database system for read performance? You may benefit from optimizing the `wal_buffers` parameter, which determines how much memory is used for the Write-Ahead Log (WAL) and for Partitioning. The `wal_buffers` parameter controls whether PostgreSQL waits for write operations to be written into memory. Increasing it to a recommended value, like 32MB, can enhance read performance but may increase disk space usage. Be cautious as setting a value too high can consume excessive memory. Be cautious not to set a value too high on high-end hardware. Be cautious about set-related issues to avoid resource contention.\\neBook reader support When using eBooks, make sure that you monitor the level of compatibility available. A well-balanced database can significantly improve read performance, but improper settings may lead to data loss or performance issues. Always'},\n",
              " {'generated_text': \"How can I optimize my PostgreSQL database system for read performance?\\n\\nIf your data durability is negatively affected by excessive disk space, consider using less storage. A recommended range of 128MB to 1GB is often the most effective value for your data durability. However, setting it too high can lead to resource contention, impacting overall system performance. Consider adjusting based on your specific query plans. Excessive disk space can lead to resource contention. Be cautious about choosing an appropriate storage solution. Always monitor the impact of resource contention on your system's performance. Be cautious when switching between storage systems. Avoid switching at random access cost. Be cautious about long-running backup operations as they can lead to data loss or performance degradation. Always monitor the impact of resource contention on your system's performance.\\neBookmarks can significantly improve write performance if utilized properly. Be cautious about long-running backup operations in situations\"},\n",
              " {'generated_text': \"How can I optimize my PostgreSQL database system for read performance?\\nInclude the 'work_mem' parameter in your'maintenance_work_mem' parameter. A recommended value is around 4MB. Excessive settings can lead to resource contention and negatively affect system performance. Make sure to set 'work_mem' to a lower value (e.g., 128MB) to prevent resource contention and improve system stability. However, be cautious not to allocate too much memory, as it may lead to resource contention. Additionally, be cautious not to allocate excessive memory, as it can lead to resource contention.\\nWhat if I have multiple replication systems on the same disk? The 'work_mem' parameter determines how much memory can be used for index and hash tables before switching to an alternative. Set 'work_mem' to a value between 1-2MB in your primary maintenance\"},\n",
              " {'generated_text': \"How can I optimize my PostgreSQL database system for read performance?\\nIn this post, I'm exploring various strategies to enhance read performance. Identify and optimize slow queries, and adjust the'max_connections' and'min_pool_size' parameters to balance resource utilization. Make sure to monitor connection usage and adjust these settings based on the workload. A recommended value is around 20 connections, but adjust based on the available resources. Excessive connections can lead to resource contention. Be cautious when adjusting these settings, as it can impact overall system performance.\\nJust like with any other query, be cautious about resource contention. Always set'max_connections' to prevent resource exhaustion, and'min_pool_size' to help prevent resource exhaustion. Avoid extreme query plans, as they may result in suboptimal query plans. Be cautious when using overly large datasets. Be cautious\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4oU96DTH74G1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
